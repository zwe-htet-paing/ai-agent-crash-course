{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861342ab",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "\n",
    "- https://docs.google.com/document/d/12wVi866gQDFSw09LZdTixYltqY4_P0g7x04IyYDil5o/edit?tab=t.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed964730",
   "metadata": {},
   "source": [
    "### Why We Need to Prepare Large Documents Before Using Them\n",
    "\n",
    "Large documents create several problems:\n",
    "\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG, this process is referred to as \"chunking.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d5b33",
   "metadata": {},
   "source": [
    "### Loading Data from Day-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121dd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ingest import read_repo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e062c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0975ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Create Plant',\n",
       " 'openapi': 'POST /plants',\n",
       " 'content': '',\n",
       " 'filename': 'docs-main/api-reference/endpoint/create.mdx'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dea7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic documents: 114\n"
     ]
    }
   ],
   "source": [
    "pydanticai_docs = read_repo_data('pydantic', 'pydantic-ai')\n",
    "print(f\"Pydantic documents: {len(pydanticai_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78ecf4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '# CLAUDE.md\\n\\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\\n\\n## Development Commands\\n\\n### Core Development Tasks\\n\\n- **Install dependencies**: `make install` (requires uv, pre-commit, and deno)\\n- **Run all checks**: `pre-commit run --all-files`\\n- **Run tests**: `make test`\\n- **Build docs**: `make docs` or `make docs-serve` (local development)\\n\\n### Single Test Commands\\n\\n- **Run specific test**: `uv run pytest tests/test_agent.py::test_function_name -v`\\n- **Run test file**: `uv run pytest tests/test_agent.py -v`\\n- **Run with debug**: `uv run pytest tests/test_agent.py -v -s`\\n\\n## Project Architecture\\n\\n### Core Components\\n\\n**Agent System (`pydantic_ai_slim/pydantic_ai/agent/`)**\\n- `Agent[AgentDepsT, OutputDataT]`: Main orchestrator class with generic types for dependency injection and output validation\\n- Entry points: `run()`, `run_sync()`, `run_stream()` methods\\n- Handles tool management, system prompts, and model interaction\\n\\n**Model Integration (`pydantic_ai_slim/pydantic_ai/models/`)**\\n- Unified interface across providers: OpenAI, Anthropic, Google, Groq, Cohere, Mistral, Bedrock, HuggingFace\\n- Model strings: `\"openai:gpt-4o\"`, `\"anthropic:claude-3-5-sonnet\"`, `\"google:gemini-1.5-pro\"`\\n- `ModelRequestParameters` for configuration, `StreamedResponse` for streaming\\n\\n**Graph-based Execution (`pydantic_graph/` + `_agent_graph.py`)**\\n- State machine execution through: `UserPromptNode` → `ModelRequestNode` → `CallToolsNode`\\n- `GraphAgentState` maintains message history and usage tracking\\n- `GraphRunContext` provides execution context\\n\\n**Tool System (`tools.py`, `toolsets/`)**\\n- `@agent.tool` decorator for function registration\\n- `RunContext[AgentDepsT]` provides dependency injection in tools\\n- Support for sync/async functions with automatic schema generation\\n\\n**Output Handling**\\n- `TextOutput`: Plain text responses\\n- `ToolOutput`: Structured data via tool calls\\n- `NativeOutput`: Provider-specific structured output\\n- `PromptedOutput`: Prompt-based structured extraction\\n\\n### Key Design Patterns\\n\\n**Dependency Injection**\\n```python\\n@dataclass\\nclass MyDeps:\\n    database: DatabaseConn\\n\\nagent = Agent(\\'openai:gpt-4o\\', deps_type=MyDeps)\\n\\n@agent.tool\\nasync def get_data(ctx: RunContext[MyDeps]) -> str:\\n    return await ctx.deps.database.fetch_data()\\n```\\n\\n**Type-Safe Agents**\\n```python\\nclass OutputModel(BaseModel):\\n    result: str\\n    confidence: float\\n\\nagent: Agent[MyDeps, OutputModel] = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=MyDeps,\\n    output_type=OutputModel\\n)\\n```\\n\\n## Workspace Structure\\n\\nThis is a uv workspace with multiple packages:\\n- **`pydantic_ai_slim/`**: Core framework (minimal dependencies)\\n- **`pydantic_evals/`**: Evaluation system\\n- **`pydantic_graph/`**: Graph execution engine\\n- **`examples/`**: Example applications\\n- **`clai/`**: CLI tool\\n\\n## Testing Strategy\\n\\n- **Unit tests**: `tests/` directory with comprehensive model and component coverage\\n- **VCR cassettes**: `tests/cassettes/` for recorded LLM API interactions\\n- **Test models**: Use `TestModel` for deterministic testing\\n- **Examples testing**: `tests/test_examples.py` validates all documentation examples\\n- **Multi-version testing**: Python 3.10-3.13 support\\n\\n## Key Configuration Files\\n\\n- **`pyproject.toml`**: Main workspace configuration with dependency groups\\n- **`pydantic_ai_slim/pyproject.toml`**: Core package with model optional dependencies\\n- **`Makefile`**: Development task automation\\n- **`uv.lock`**: Locked dependencies for reproducible builds\\n\\n## Important Implementation Notes\\n\\n- **Model Provider Integration**: Each provider in `models/` directory implements the `Model` abstract base class\\n- **Message System**: Vendor-agnostic message format in `messages.py` with rich content type support\\n- **Streaming Architecture**: Real-time response processing with validation during streaming\\n- **Error Handling**: Specific exception types with retry mechanisms at multiple levels\\n- **OpenTelemetry Integration**: Built-in observability support\\n\\n## Documentation Development\\n\\n- **Local docs**: `make docs-serve` (serves at http://localhost:8000)\\n- **Docs source**: `docs/` directory (MkDocs with Material theme)\\n- **API reference**: Auto-generated from docstrings using mkdocstrings\\n\\n## Dependencies Management\\n\\n- **Package manager**: uv (fast Python package manager)\\n- **Lock file**: `uv.lock` (commit this file)\\n- **Sync command**: `make sync` to update dependencies\\n- **Optional extras**: Define groups in `pyproject.toml` optional-dependencies\\n\\n## Best Practices\\n\\nThis is the list of best practices for working with the codebase.\\n\\n### Rename a class\\n\\nWhen asked to rename a class, you need to rename the class in the code and add a deprecation warning to the old class.\\n\\n```python\\nfrom typing_extensions import deprecated\\n\\nclass NewClass: ...  # This class was renamed from OldClass.\\n\\n@deprecated(\"Use `NewClass` instead.\")\\nclass OldClass(NewClass): ...\\n```\\n\\nIn the test suite, you MUST use the `NewClass` instead of the `OldClass`, and create a new test to verify the\\ndeprecation warning:\\n\\n```python\\ndef test_old_class_is_deprecated():\\n    with pytest.warns(DeprecationWarning, match=\"Use `NewClass` instead.\"):\\n        OldClass()\\n```\\n\\nIn the documentation, you should not have references to the old class, only the new class.\\n\\n### Writing documentation\\n\\nAlways reference Python objects with the \"`\" (backticks) around them, and link to the API reference, for example:\\n\\n```markdown\\nThe [`Agent`][pydantic_ai.agent.Agent] class is the main entry point for creating and running agents.\\n```\\n\\n### Coverage\\n\\nEvery pull request MUST have 100% coverage. You can check the coverage by running `make test`.',\n",
       " 'filename': 'pydantic-ai-main/CLAUDE.md'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydanticai_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e91883",
   "metadata": {},
   "source": [
    "### 1. Simple Chunking with Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a742758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93ae5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "033b08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently chunks: 575\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently chunks: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dae97363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'chunk': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       " 'title': 'Introduction',\n",
       " 'description': 'Example section for showcasing API endpoints',\n",
       " 'filename': 'docs-main/api-reference/introduction.mdx'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5dda9acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic chunks: 606\n"
     ]
    }
   ],
   "source": [
    "pydanticai_chunks = []\n",
    "\n",
    "for doc in pydanticai_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    pydanticai_chunks.extend(chunks)\n",
    "    \n",
    "print(f\"Pydantic chunks: {len(pydanticai_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "762af835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '# CLAUDE.md\\n\\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\\n\\n## Development Commands\\n\\n### Core Development Tasks\\n\\n- **Install dependencies**: `make install` (requires uv, pre-commit, and deno)\\n- **Run all checks**: `pre-commit run --all-files`\\n- **Run tests**: `make test`\\n- **Build docs**: `make docs` or `make docs-serve` (local development)\\n\\n### Single Test Commands\\n\\n- **Run specific test**: `uv run pytest tests/test_agent.py::test_function_name -v`\\n- **Run test file**: `uv run pytest tests/test_agent.py -v`\\n- **Run with debug**: `uv run pytest tests/test_agent.py -v -s`\\n\\n## Project Architecture\\n\\n### Core Components\\n\\n**Agent System (`pydantic_ai_slim/pydantic_ai/agent/`)**\\n- `Agent[AgentDepsT, OutputDataT]`: Main orchestrator class with generic types for dependency injection and output validation\\n- Entry points: `run()`, `run_sync()`, `run_stream()` methods\\n- Handles tool management, system prompts, and model interaction\\n\\n**Model Integration (`pydantic_ai_slim/pydantic_ai/models/`)**\\n- Unified interface across providers: OpenAI, Anthropic, Google, Groq, Cohere, Mistral, Bedrock, HuggingFace\\n- Model strings: `\"openai:gpt-4o\"`, `\"anthropic:claude-3-5-sonnet\"`, `\"google:gemini-1.5-pro\"`\\n- `ModelRequestParameters` for configuration, `StreamedResponse` for streaming\\n\\n**Graph-based Execution (`pydantic_graph/` + `_agent_graph.py`)**\\n- State machine execution through: `UserPromptNode` → `ModelRequestNode` → `CallToolsNode`\\n- `GraphAgentState` maintains message history and usage tracking\\n- `GraphRunContext` provides execution context\\n\\n**Tool System (`tools.py`, `toolsets/`)**\\n- `@agent.tool` decorator for function registration\\n- `RunContext[AgentDepsT]` provides dependency injection in tools\\n- Support for sync/async functions with automatic schema generation\\n\\n**Output Handling**\\n- `TextOutput`: Plain text responses\\n- `ToolOutput`: Structured data via tool calls\\n- `NativeOutput`: Provider-specific structured output\\n- `PromptedOutput`: Prompt-based structured extraction\\n\\n### Key Design Patterns\\n\\n**Dependency Injection**\\n```python\\n@dataclass\\nclass MyDeps:\\n    database: DatabaseConn\\n\\nagent = Agent(\\'openai:gpt-4o\\', deps_type=MyDeps)\\n\\n@agent.tool\\nasync def get_data(ctx: RunContext[MyDeps]) -> str:\\n    return await ctx.deps.database.fetch_data()\\n```\\n\\n**Type-Safe Agents**\\n```python\\nclass OutputModel(BaseModel):\\n    result: str\\n    confidence: float\\n\\nagent: Agent[MyDeps, OutputModel] = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=MyDeps,\\n    output_type=OutputModel\\n)\\n```\\n\\n## Workspace Structure\\n\\nThis is a uv workspace with multiple packages:\\n- **`pydantic_ai_slim/`**: Core framework (minimal dependencies)\\n- **`pydantic_evals/`**: Evaluation system\\n- **`pydantic_graph/`**: Graph execution engine\\n- **`examples/`**: Example applications\\n- **`clai/`**: CLI tool\\n\\n## Testing Strategy\\n\\n- **Unit tests**: `tests/` directory with comprehensive model and component coverage\\n- **VCR cassettes**: `tests/cassettes/` for recorded LLM API interactions\\n- **Test models**: Use `TestModel` for deterministic testing\\n- **Examples testing**: `tests/test_examples.py` validates all documentation examples\\n- **Multi-version testing**: Python 3.10-3.13 support\\n\\n## Key Configuration Files\\n\\n- **`pyproject.toml`**: Main workspace configuration with dependency groups\\n- **`pydantic_ai_slim/pyproject.toml`**: Core package with model optional dependencies\\n- **`Makefile`**: Development task automation\\n- **`uv.lock`**: Locked dependencies for reproducible builds\\n\\n## Important Implementation Notes\\n\\n- **Model Provider Integration**: Each provider in `models/` directory implements the `Model` abstract base class\\n- **Message System**: Vendor-agnostic message format in `messages.py` with rich content type support\\n- **Streaming Architecture**: Real-time response processing with validation during streaming\\n- **Error Handling**: Specific exception types with retry mechanisms at multiple levels\\n- **OpenTelemetry Integration**: Built-in observability support\\n\\n## Documentation Development\\n\\n- **Local docs**: `make docs-serve` (serves at http://localhost:8000)\\n- **Docs source**: `docs/` directory (MkDocs with Material theme)\\n- **API reference**: Auto-generated from docstrings using mkdocstrings\\n\\n## Dependencies Management\\n\\n- **Package manager**: uv (fast Python package manager)\\n- **Lock file**: `uv.lock` (commit this file)\\n- **Sync command**: `make sync` to update dependencies\\n- **Optional extras**: Define groups in `pyproject.toml` optional-dependencies\\n\\n## Best Practices\\n\\nThis is the list of best practices for working with the codebase.\\n\\n### Rename a class\\n\\nWhen asked to rename a class, you need to rename the class in the code and add a deprecation warning to the old class.\\n\\n```python\\nfrom typing_extensions import deprecated\\n\\nclass NewClass: ...  # This class was renamed from OldClass.\\n\\n@deprecated(\"Use `NewClass` instead.\")\\nclass OldClass(NewClass): ...\\n```\\n\\nIn the test suite, you MUST use the `NewClass` instead of the `OldClass`, and create a new test to verify the\\ndeprecation warning:\\n\\n```python\\ndef test_old_class_is_deprecated():\\n    with pytest.warns(DeprecationWarning, match=\"Use `NewClass` instead.\"):\\n        OldClass()\\n```\\n\\nIn the documentation, you should not have references to the old class, only the new class.\\n\\n### Writing documentation\\n\\nAlways reference Python objects with the \"`\" (backticks) around them, and link to the API reference, for example:\\n\\n```markdown\\nThe [`Agent`][pydantic_ai.agent.Agent] class is the main entry point for creating and running agents.\\n```\\n\\n### Coverage\\n\\nEvery pull request MUST have 100% coverage. You can check the coverage by running `make test`.',\n",
       " 'filename': 'pydantic-ai-main/CLAUDE.md'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydanticai_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0e8a3",
   "metadata": {},
   "source": [
    "### 2. Splitting by Paragraphs and Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ec13c",
   "metadata": {},
   "source": [
    "#### Paragraphs\n",
    "\n",
    "Use `\\n\\s*\\n` regex pattern for splitting:\n",
    "\n",
    "- `\\n` matches a newline\n",
    "- `\\s*` matches zero or more whitespace characters\n",
    "- `\\n` matches another newline\n",
    "- So `\\n\\s*\\n` matches two newlines with optional whitespace between them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7d39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting by paragraphs\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# text = evidently_docs[45]['content']\n",
    "text = pydanticai_docs[100]['content']\n",
    "paragraphs = re.split(r'\\n\\s*\\n', text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f37273b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output\" refers to the final value returned from [running an agent](agents.md#running-agents). This can be either plain text, [structured data](#structured-output), or the result of a [function](#output-functions) called with arguments provided by the model.\n",
      "\n",
      "The output is wrapped in [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] or [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] so that you can access other data, like [usage][pydantic_ai.usage.RunUsage] of the run and [message history](message-history.md#accessing-messages-from-results).\n",
      "\n",
      "Both `AgentRunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.\n",
      "\n",
      "A run ends when the model responds with one of the structured output types, or, if no output type is specified or `str` is one of the allowed options, when a plain text response is received. A run can also be cancelled if usage limits are exceeded, see [Usage Limits](agents.md#usage-limits).\n",
      "\n",
      "Here's an example using a Pydantic model as the `output_type`, forcing the model to respond with data matching our specification:\n",
      "\n",
      "```python {title=\"olympics.py\" line_length=\"90\"}\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "\n",
      "class CityLocation(BaseModel):\n",
      "    city: str\n",
      "    country: str\n",
      "\n",
      "\n",
      "agent = Agent('google-gla:gemini-1.5-flash', output_type=CityLocation)\n",
      "result = agent.run_sync('Where were the olympics held in 2012?')\n",
      "print(result.output)\n",
      "#> city='London' country='United Kingdom'\n",
      "print(result.usage())\n",
      "#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n",
      "```\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "## Output data {#structured-output}\n",
      "\n",
      "The [`Agent`][pydantic_ai.Agent] class constructor takes an `output_type` argument that takes one or more types or [output functions](#output-functions). It supports simple scalar types, list and dict types (including `TypedDict`s and [`StructuredDict`s](#structured-dict)), dataclasses and Pydantic models, as well as type unions -- generally everything supported as type hints in a Pydantic model. You can also pass a list of multiple choices.\n",
      "\n",
      "By default, Pydantic AI leverages the model's tool calling capability to make it return structured data. When multiple output types are specified (in a union or list), each member is registered with the model as a separate output tool in order to reduce the complexity of the schema and maximise the chances a model will respond correctly. This has been shown to work well across a wide range of models. If you'd like to change the names of the output tools, use a model's native structured output feature, or pass the output schema to the model in its [instructions](agents.md#instructions), you can use an [output mode](#output-modes) marker class.\n",
      "\n",
      "When no output type is specified, or when `str` is among the output types, any plain text response from the model will be used as the output data.\n",
      "If `str` is not among the output types, the model is forced to return structured data or call an output function.\n",
      "\n",
      "If the output type schema is not of type `\"object\"` (e.g. it's `int` or `list[int]`), the output type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\n",
      "\n",
      "Structured outputs (like tools) use Pydantic to build the JSON schema used for the tool, and to validate the data returned by the model.\n",
      "\n",
      "!!! note \"Type checking considerations\"\n",
      "    The Agent class is generic in its output type, and this type is carried through to `AgentRunResult.output` and `StreamedRunResult.output` so that your IDE or static type checker can warn you when your code doesn't properly take into account all the possible values those outputs could have.\n",
      "\n",
      "    Static type checkers like pyright and mypy will do their best to infer the agent's output type from the `output_type` you've specified, but they're not always able to do so correctly when you provide functions or multiple types in a union or list, even though Pydantic AI will behave correctly. When this happens, your type checker will complain even when you're confident you've passed a valid `output_type`, and you'll need to help the type checker by explicitly specifying the generic parameters on the `Agent` constructor. This is shown in the second example below and the output functions example further down.\n",
      "\n",
      "    Specifically, there are three valid uses of `output_type` where you'll need to do this:\n",
      "\n",
      "    1. When using a union of types, e.g. `output_type=Foo | Bar`. Until [PEP-747](https://peps.python.org/pep-0747/) \"Annotating Type Forms\" lands in Python 3.15, type checkers do not consider these a valid value for `output_type`. In addition to the generic parameters on the `Agent` constructor, you'll need to add `# type: ignore` to the line that passes the union to `output_type`. Alternatively, you can use a list: `output_type=[Foo, Bar]`.\n",
      "    2. With mypy: When using a list, as a functionally equivalent alternative to a union, or because you're passing in [output functions](#output-functions). Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19142) with mypy to try and get this fixed.\n",
      "    3. With mypy: when using an async output function. Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19143) with mypy to try and get this fixed.\n",
      "\n",
      "Here's an example of returning either text or structured data:\n",
      "\n",
      "```python {title=\"box_or_error.py\"}\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "\n",
      "class Box(BaseModel):\n",
      "    width: int\n",
      "    height: int\n",
      "    depth: int\n",
      "    units: str\n",
      "\n",
      "\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o-mini',\n",
      "    output_type=[Box, str], # (1)!\n",
      "    system_prompt=(\n",
      "        \"Extract me the dimensions of a box, \"\n",
      "        \"if you can't extract all data, ask the user to try again.\"\n",
      "    ),\n",
      ")\n",
      "\n",
      "result = agent.run_sync('The box is 10x20x30')\n",
      "print(result.output)\n",
      "#> Please provide the units for the dimensions (e.g., cm, in, m).\n",
      "\n",
      "result = agent.run_sync('The box is 10x20x30 cm')\n",
      "print(result.output)\n",
      "#> width=10 height=20 depth=30 units='cm'\n",
      "```\n",
      "\n",
      "1. This could also have been a union: `output_type=Box | str`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "Here's an example of using a union return type, which will register multiple output tools and wrap non-object schemas in an object:\n",
      "\n",
      "```python {title=\"colors_or_sizes.py\"}\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "agent = Agent[None, list[str] | list[int]](\n",
      "    'openai:gpt-4o-mini',\n",
      "    output_type=list[str] | list[int],  # type: ignore # (1)!\n",
      "    system_prompt='Extract either colors or sizes from the shapes provided.',\n",
      ")\n",
      "\n",
      "result = agent.run_sync('red square, blue circle, green triangle')\n",
      "print(result.output)\n",
      "#> ['red', 'blue', 'green']\n",
      "\n",
      "result = agent.run_sync('square size 10, circle size 20, triangle size 30')\n",
      "print(result.output)\n",
      "#> [10, 20, 30]\n",
      "```\n",
      "\n",
      "1. As explained in the \"Type checking considerations\" section above, using a union rather than a list requires explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "### Output functions\n",
      "\n",
      "Instead of plain text or structured data, you may want the output of your agent run to be the result of a function called with arguments provided by the model, for example to further process or validate the data provided through the arguments (with the option to tell the model to try again), or to hand off to another agent.\n",
      "\n",
      "Output functions are similar to [function tools](tools.md), but the model is forced to call one of them, the call ends the agent run, and the result is not passed back to the model.\n",
      "\n",
      "As with tool functions, output function arguments provided by the model are validated using Pydantic, they can optionally take [`RunContext`][pydantic_ai.tools.RunContext] as the first argument, and they can raise [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] to ask the model to try again with modified arguments (or with a different output type).\n",
      "\n",
      "To specify output functions, you set the agent's `output_type` to either a single function (or bound instance method), or a list of functions. The list can also contain other output types like simple scalars or entire Pydantic models.\n",
      "You typically do not want to also register your output function as a tool (using the `@agent.tool` decorator or `tools` argument), as this could confuse the model about which it should be calling.\n",
      "\n",
      "Here's an example of all of these features in action:\n",
      "\n",
      "```python {title=\"output_functions.py\"}\n",
      "import re\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent, ModelRetry, RunContext, UnexpectedModelBehavior\n",
      "\n",
      "\n",
      "class Row(BaseModel):\n",
      "    name: str\n",
      "    country: str\n",
      "\n",
      "\n",
      "tables = {\n",
      "    'capital_cities': [\n",
      "        Row(name='Amsterdam', country='Netherlands'),\n",
      "        Row(name='Mexico City', country='Mexico'),\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "class SQLFailure(BaseModel):\n",
      "    \"\"\"An unrecoverable failure. Only use this when you can't change the query to make it work.\"\"\"\n",
      "\n",
      "    explanation: str\n",
      "\n",
      "\n",
      "def run_sql_query(query: str) -> list[Row]:\n",
      "    \"\"\"Run a SQL query on the database.\"\"\"\n",
      "\n",
      "    select_table = re.match(r'SELECT (.+) FROM (\\w+)', query)\n",
      "    if select_table:\n",
      "        column_names = select_table.group(1)\n",
      "        if column_names != '*':\n",
      "            raise ModelRetry(\"Only 'SELECT *' is supported, you'll have to do column filtering manually.\")\n",
      "\n",
      "        table_name = select_table.group(2)\n",
      "        if table_name not in tables:\n",
      "            raise ModelRetry(\n",
      "                f\"Unknown table '{table_name}' in query '{query}'. Available tables: {', '.join(tables.keys())}.\"\n",
      "            )\n",
      "\n",
      "        return tables[table_name]\n",
      "\n",
      "    raise ModelRetry(f\"Unsupported query: '{query}'.\")\n",
      "\n",
      "\n",
      "sql_agent = Agent[None, list[Row] | SQLFailure](\n",
      "    'openai:gpt-4o',\n",
      "    output_type=[run_sql_query, SQLFailure],\n",
      "    instructions='You are a SQL agent that can run SQL queries on a database.',\n",
      ")\n",
      "\n",
      "\n",
      "async def hand_off_to_sql_agent(ctx: RunContext, query: str) -> list[Row]:\n",
      "    \"\"\"I take natural language queries, turn them into SQL, and run them on a database.\"\"\"\n",
      "\n",
      "    # Drop the final message with the output tool call, as it shouldn't be passed on to the SQL agent\n",
      "    messages = ctx.messages[:-1]\n",
      "    try:\n",
      "        result = await sql_agent.run(query, message_history=messages)\n",
      "        output = result.output\n",
      "        if isinstance(output, SQLFailure):\n",
      "            raise ModelRetry(f'SQL agent failed: {output.explanation}')\n",
      "        return output\n",
      "    except UnexpectedModelBehavior as e:\n",
      "        # Bubble up potentially retryable errors to the router agent\n",
      "        if (cause := e.__cause__) and isinstance(cause, ModelRetry):\n",
      "            raise ModelRetry(f'SQL agent failed: {cause.message}') from e\n",
      "        else:\n",
      "            raise\n",
      "\n",
      "\n",
      "class RouterFailure(BaseModel):\n",
      "    \"\"\"Use me when no appropriate agent is found or the used agent failed.\"\"\"\n",
      "\n",
      "    explanation: str\n",
      "\n",
      "\n",
      "router_agent = Agent[None, list[Row] | RouterFailure](\n",
      "    'openai:gpt-4o',\n",
      "    output_type=[hand_off_to_sql_agent, RouterFailure],\n",
      "    instructions='You are a router to other agents. Never try to solve a problem yourself, just pass it on.',\n",
      ")\n",
      "\n",
      "result = router_agent.run_sync('Select the names and countries of all capitals')\n",
      "print(result.output)\n",
      "\"\"\"\n",
      "[\n",
      "    Row(name='Amsterdam', country='Netherlands'),\n",
      "    Row(name='Mexico City', country='Mexico'),\n",
      "]\n",
      "\"\"\"\n",
      "\n",
      "result = router_agent.run_sync('Select all pets')\n",
      "print(repr(result.output))\n",
      "\"\"\"\n",
      "RouterFailure(explanation=\"The requested table 'pets' does not exist in the database. The only available table is 'capital_cities', which does not contain data about pets.\")\n",
      "\"\"\"\n",
      "\n",
      "result = router_agent.run_sync('How do I fly from Amsterdam to Mexico City?')\n",
      "print(repr(result.output))\n",
      "\"\"\"\n",
      "RouterFailure(explanation='I am not equipped to provide travel information, such as flights from Amsterdam to Mexico City.')\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "#### Text output\n",
      "\n",
      "If you provide an output function that takes a string, Pydantic AI will by default create an output tool like for any other output function. If instead you'd like the model to provide the string using plain text output, you can wrap the function in the [`TextOutput`][pydantic_ai.output.TextOutput] marker class. If desired, this marker class can be used alongside one or more [`ToolOutput`](#tool-output) marker classes (or unmarked types or functions) in a list provided to `output_type`.\n",
      "\n",
      "```python {title=\"text_output_function.py\"}\n",
      "from pydantic_ai import Agent, TextOutput\n",
      "\n",
      "\n",
      "def split_into_words(text: str) -> list[str]:\n",
      "    return text.split()\n",
      "\n",
      "\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    output_type=TextOutput(split_into_words),\n",
      ")\n",
      "result = agent.run_sync('Who was Albert Einstein?')\n",
      "print(result.output)\n",
      "#> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical', 'physicist.']\n",
      "```\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "### Output modes\n",
      "\n",
      "Pydantic AI implements three different methods to get a model to output structured data:\n",
      "\n",
      "1. [Tool Output](#tool-output), where tool calls are used to produce the output.\n",
      "2. [Native Output](#native-output), where the model is required to produce text content compliant with a provided JSON schema.\n",
      "3. [Prompted Output](#prompted-output), where a prompt is injected into the model instructions including the desired JSON schema, and we attempt to parse the model's plain-text response as appropriate.\n",
      "\n",
      "#### Tool Output\n",
      "\n",
      "In the default Tool Output mode, the output JSON schema of each output type (or function) is provided to the model as the parameters schema of a special output tool. This is the default as it's supported by virtually all models and has been shown to work very well.\n",
      "\n",
      "If you'd like to change the name of the output tool, pass a custom description to aid the model, or turn on or off strict mode, you can wrap the type(s) in the [`ToolOutput`][pydantic_ai.output.ToolOutput] marker class and provide the appropriate arguments. Note that by default, the description is taken from the docstring specified on a Pydantic model or output function, so specifying it using the marker class is typically not necessary.\n",
      "\n",
      "To dynamically modify or filter the available output tools during an agent run, you can define an agent-wide `prepare_output_tools` function that will be called ahead of each step of a run. This function should be of type [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc], which takes the [`RunContext`][pydantic_ai.tools.RunContext] and a list of [`ToolDefinition`][pydantic_ai.tools.ToolDefinition], and returns a new list of tool definitions (or `None` to disable all tools for that step). This is analogous to the [`prepare_tools` function](tools-advanced.md#prepare-tools) for non-output tools.\n",
      "\n",
      "```python {title=\"tool_output.py\"}\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent, ToolOutput\n",
      "\n",
      "\n",
      "class Fruit(BaseModel):\n",
      "    name: str\n",
      "    color: str\n",
      "\n",
      "\n",
      "class Vehicle(BaseModel):\n",
      "    name: str\n",
      "    wheels: int\n",
      "\n",
      "\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    output_type=[ # (1)!\n",
      "        ToolOutput(Fruit, name='return_fruit'),\n",
      "        ToolOutput(Vehicle, name='return_vehicle'),\n",
      "    ],\n",
      ")\n",
      "result = agent.run_sync('What is a banana?')\n",
      "print(repr(result.output))\n",
      "#> Fruit(name='banana', color='yellow')\n",
      "```\n",
      "\n",
      "1. If we were passing just `Fruit` and `Vehicle` without custom tool names, we could have used a union: `output_type=Fruit | Vehicle`. However, as `ToolOutput` is an object rather than a type, we have to use a list.\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "#### Native Output\n",
      "\n",
      "Native Output mode uses a model's native \"Structured Outputs\" feature (aka \"JSON Schema response format\"), where the model is forced to only output text matching the provided JSON schema. Note that this is not supported by all models, and sometimes comes with restrictions. For example, Anthropic does not support this at all, and Gemini cannot use tools at the same time as structured output, and attempting to do so will result in an error.\n",
      "\n",
      "To use this mode, you can wrap the output type(s) in the [`NativeOutput`][pydantic_ai.output.NativeOutput] marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient.\n",
      "\n",
      "```python {title=\"native_output.py\" requires=\"tool_output.py\"}\n",
      "from pydantic_ai import Agent, NativeOutput\n",
      "\n",
      "from tool_output import Fruit, Vehicle\n",
      "\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    output_type=NativeOutput(\n",
      "        [Fruit, Vehicle], # (1)!\n",
      "        name='Fruit_or_vehicle',\n",
      "        description='Return a fruit or vehicle.'\n",
      "    ),\n",
      ")\n",
      "result = agent.run_sync('What is a Ford Explorer?')\n",
      "print(repr(result.output))\n",
      "#> Vehicle(name='Ford Explorer', wheels=4)\n",
      "```\n",
      "\n",
      "1. This could also have been a union: `output_type=Fruit | Vehicle`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "#### Prompted Output\n",
      "\n",
      "In this mode, the model is prompted to output text matching the provided JSON schema through its [instructions](agents.md#instructions) and it's up to the model to interpret those instructions correctly. This is usable with all models, but is often the least reliable approach as the model is not forced to match the schema.\n",
      "\n",
      "While we would generally suggest starting with tool or native output, in some cases this mode may result in higher quality outputs, and for models without native tool calling or structured output support it is the only option for producing structured outputs.\n",
      "\n",
      "If the model API supports the \"JSON Mode\" feature (aka \"JSON Object response format\") to force the model to output valid JSON, this is enabled, but it's still up to the model to abide by the schema. Pydantic AI will validate the returned structured data and tell the model to try again if validation fails, but if the model is not intelligent enough this may not be sufficient.\n",
      "\n",
      "To use this mode, you can wrap the output type(s) in the [`PromptedOutput`][pydantic_ai.output.PromptedOutput] marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient. Additionally, it supports an `template` argument lets you specify a custom instructions template to be used instead of the [default][pydantic_ai.profiles.ModelProfile.prompted_output_template].\n",
      "\n",
      "```python {title=\"prompted_output.py\" requires=\"tool_output.py\"}\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent, PromptedOutput\n",
      "\n",
      "from tool_output import Vehicle\n",
      "\n",
      "\n",
      "class Device(BaseModel):\n",
      "    name: str\n",
      "    kind: str\n",
      "\n",
      "\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    output_type=PromptedOutput(\n",
      "        [Vehicle, Device], # (1)!\n",
      "        name='Vehicle or device',\n",
      "        description='Return a vehicle or device.'\n",
      "    ),\n",
      ")\n",
      "result = agent.run_sync('What is a MacBook?')\n",
      "print(repr(result.output))\n",
      "#> Device(name='MacBook', kind='laptop')\n",
      "\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    output_type=PromptedOutput(\n",
      "        [Vehicle, Device],\n",
      "        template='Gimme some JSON: {schema}'\n",
      "    ),\n",
      ")\n",
      "result = agent.run_sync('What is a Ford Explorer?')\n",
      "print(repr(result.output))\n",
      "#> Vehicle(name='Ford Explorer', wheels=4)\n",
      "```\n",
      "\n",
      "1. This could also have been a union: `output_type=Vehicle | Device`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "### Custom JSON schema {#structured-dict}\n",
      "\n",
      "If it's not feasible to define your desired structured output object using a Pydantic `BaseModel`, dataclass, or `TypedDict`, for example when you get a JSON schema from an external source or generate it dynamically, you can use the [`StructuredDict()`][pydantic_ai.output.StructuredDict] helper function to generate a `dict[str, Any]` subclass with a JSON schema attached that Pydantic AI will pass to the model.\n",
      "\n",
      "Note that Pydantic AI will not perform any validation of the received JSON object and it's up to the model to correctly interpret the schema and any constraints expressed in it, like required fields or integer value ranges.\n",
      "\n",
      "The output type will be a `dict[str, Any]` and it's up to your code to defensively read from it in case the model made a mistake. You can use an [output validator](#output-validator-functions) to reflect validation errors back to the model and get it to try again.\n",
      "\n",
      "Along with the JSON schema, you can optionally pass `name` and `description` arguments to provide additional context to the model:\n",
      "\n",
      "```python\n",
      "from pydantic_ai import Agent, StructuredDict\n",
      "\n",
      "HumanDict = StructuredDict(\n",
      "    {\n",
      "        'type': 'object',\n",
      "        'properties': {\n",
      "            'name': {'type': 'string'},\n",
      "            'age': {'type': 'integer'}\n",
      "        },\n",
      "        'required': ['name', 'age']\n",
      "    },\n",
      "    name='Human',\n",
      "    description='A human with a name and age',\n",
      ")\n",
      "\n",
      "agent = Agent('openai:gpt-4o', output_type=HumanDict)\n",
      "result = agent.run_sync('Create a person')\n",
      "#> {'name': 'John Doe', 'age': 30}\n",
      "```\n",
      "\n",
      "### Output validators {#output-validator-functions}\n",
      "\n",
      "Some validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. Pydantic AI provides a way to add validation functions via the [`agent.output_validator`][pydantic_ai.Agent.output_validator] decorator.\n",
      "\n",
      "If you want to implement separate validation logic for different output types, it's recommended to use [output functions](#output-functions) instead, to save you from having to do `isinstance` checks inside the output validator.\n",
      "If you want the model to output plain text, do your own processing or validation, and then have the agent's final output be the result of your function, it's recommended to use an [output function](#output-functions) with the [`TextOutput` marker class](#text-output).\n",
      "\n",
      "Here's a simplified variant of the [SQL Generation example](examples/sql-gen.md):\n",
      "\n",
      "```python {title=\"sql_gen.py\"}\n",
      "from fake_database import DatabaseConn, QueryError\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent, RunContext, ModelRetry\n",
      "\n",
      "\n",
      "class Success(BaseModel):\n",
      "    sql_query: str\n",
      "\n",
      "\n",
      "class InvalidRequest(BaseModel):\n",
      "    error_message: str\n",
      "\n",
      "\n",
      "Output = Success | InvalidRequest\n",
      "agent = Agent[DatabaseConn, Output](\n",
      "    'google-gla:gemini-1.5-flash',\n",
      "    output_type=Output,  # type: ignore\n",
      "    deps_type=DatabaseConn,\n",
      "    system_prompt='Generate PostgreSQL flavored SQL queries based on user input.',\n",
      ")\n",
      "\n",
      "\n",
      "@agent.output_validator\n",
      "async def validate_sql(ctx: RunContext[DatabaseConn], output: Output) -> Output:\n",
      "    if isinstance(output, InvalidRequest):\n",
      "        return output\n",
      "    try:\n",
      "        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n",
      "    except QueryError as e:\n",
      "        raise ModelRetry(f'Invalid query: {e}') from e\n",
      "    else:\n",
      "        return output\n",
      "\n",
      "\n",
      "result = agent.run_sync(\n",
      "    'get me users who were last active yesterday.', deps=DatabaseConn()\n",
      ")\n",
      "print(result.output)\n",
      "#> sql_query='SELECT * FROM users WHERE last_active::date = today() - interval 1 day'\n",
      "```\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "## Streamed Results\n",
      "\n",
      "There two main challenges with streamed results:\n",
      "\n",
      "1. Validating structured responses before they're complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748).\n",
      "2. When receiving a response, we don't know if it's the final response without starting to stream it and peeking at the content. Pydantic AI streams just enough of the response to sniff out if it's a tool call or an output, then streams the whole thing and calls tools, or returns the stream as a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult].\n",
      "\n",
      "!!! note\n",
      "    As the `run_stream()` method will consider the first output matching the `output_type` to be the final output,\n",
      "    it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\n",
      "\n",
      "    If you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools,\n",
      "    use [`agent.run()`][pydantic_ai.agent.AbstractAgent.run] with an `event_stream_handler` ([docs](agents.md#streaming-all-events)) or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] ([docs](agents.md#streaming-all-events-and-output)) instead.\n",
      "\n",
      "### Streaming Text\n",
      "\n",
      "Example of streamed text output:\n",
      "\n",
      "```python {title=\"streamed_hello_world.py\" line_length=\"120\"}\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "agent = Agent('google-gla:gemini-1.5-flash')  # (1)!\n",
      "\n",
      "\n",
      "async def main():\n",
      "    async with agent.run_stream('Where does \"hello world\" come from?') as result:  # (2)!\n",
      "        async for message in result.stream_text():  # (3)!\n",
      "            print(message)\n",
      "            #> The first known\n",
      "            #> The first known use of \"hello,\n",
      "            #> The first known use of \"hello, world\" was in\n",
      "            #> The first known use of \"hello, world\" was in a 1974 textbook\n",
      "            #> The first known use of \"hello, world\" was in a 1974 textbook about the C\n",
      "            #> The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n",
      "```\n",
      "\n",
      "1. Streaming works with the standard [`Agent`][pydantic_ai.Agent] class, and doesn't require any special setup, just a model that supports streaming (currently all models support streaming).\n",
      "2. The [`Agent.run_stream()`][pydantic_ai.agent.AbstractAgent.run_stream] method is used to start a streamed run, this method returns a context manager so the connection can be closed when the stream completes.\n",
      "3. Each item yield by [`StreamedRunResult.stream_text()`][pydantic_ai.result.StreamedRunResult.stream_text] is the complete text response, extended as new data is received.\n",
      "\n",
      "_(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)_\n",
      "\n",
      "We can also stream text as deltas rather than the entire text in each item:\n",
      "\n",
      "```python {title=\"streamed_delta_hello_world.py\"}\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "agent = Agent('google-gla:gemini-1.5-flash')\n",
      "\n",
      "\n",
      "async def main():\n",
      "    async with agent.run_stream('Where does \"hello world\" come from?') as result:\n",
      "        async for message in result.stream_text(delta=True):  # (1)!\n",
      "            print(message)\n",
      "            #> The first known\n",
      "            #> use of \"hello,\n",
      "            #> world\" was in\n",
      "            #> a 1974 textbook\n",
      "            #> about the C\n",
      "            #> programming language.\n",
      "```\n",
      "\n",
      "1. [`stream_text`][pydantic_ai.result.StreamedRunResult.stream_text] will error if the response is not text.\n",
      "\n",
      "_(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)_\n",
      "\n",
      "!!! warning \"Output message not included in `messages`\"\n",
      "    The final output message will **NOT** be added to result messages if you use `.stream_text(delta=True)`,\n",
      "    see [Messages and chat history](message-history.md) for more information.\n",
      "\n",
      "### Streaming Structured Output\n",
      "\n",
      "Here's an example of streaming a user profile as it's built:\n",
      "\n",
      "```python {title=\"streamed_user_profile.py\" line_length=\"120\"}\n",
      "from datetime import date\n",
      "\n",
      "from typing_extensions import NotRequired, TypedDict\n",
      "\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "\n",
      "class UserProfile(TypedDict):\n",
      "    name: str\n",
      "    dob: NotRequired[date]\n",
      "    bio: NotRequired[str]\n",
      "\n",
      "\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    output_type=UserProfile,\n",
      "    system_prompt='Extract a user profile from the input',\n",
      ")\n",
      "\n",
      "\n",
      "async def main():\n",
      "    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n",
      "    async with agent.run_stream(user_input) as result:\n",
      "        async for profile in result.stream_output():\n",
      "            print(profile)\n",
      "            #> {'name': 'Ben'}\n",
      "            #> {'name': 'Ben'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n",
      "```\n",
      "\n",
      "_(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)_\n",
      "\n",
      "As setting an `output_type` uses the [Tool Output](#tool-output) mode by default, this will only work if the model supports streaming tool arguments. For models that don't, like Gemini, try [Native Output](#native-output) or [Prompted Output](#prompted-output) instead.\n",
      "\n",
      "### Streaming Model Responses\n",
      "\n",
      "If you want fine-grained control of validation, you can use the following pattern to get the entire partial [`ModelResponse`][pydantic_ai.messages.ModelResponse]:\n",
      "\n",
      "```python {title=\"streamed_user_profile.py\" line_length=\"120\"}\n",
      "from datetime import date\n",
      "\n",
      "from pydantic import ValidationError\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "\n",
      "class UserProfile(TypedDict, total=False):\n",
      "    name: str\n",
      "    dob: date\n",
      "    bio: str\n",
      "\n",
      "\n",
      "agent = Agent('openai:gpt-4o', output_type=UserProfile)\n",
      "\n",
      "\n",
      "async def main():\n",
      "    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n",
      "    async with agent.run_stream(user_input) as result:\n",
      "        async for message, last in result.stream_responses(debounce_by=0.01):  # (1)!\n",
      "            try:\n",
      "                profile = await result.validate_response_output(  # (2)!\n",
      "                    message,\n",
      "                    allow_partial=not last,\n",
      "                )\n",
      "            except ValidationError:\n",
      "                continue\n",
      "            print(profile)\n",
      "            #> {'name': 'Ben'}\n",
      "            #> {'name': 'Ben'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n",
      "            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n",
      "```\n",
      "\n",
      "1. [`stream_responses`][pydantic_ai.result.StreamedRunResult.stream_responses] streams the data as [`ModelResponse`][pydantic_ai.messages.ModelResponse] objects, thus iteration can't fail with a `ValidationError`.\n",
      "2. [`validate_response_output`][pydantic_ai.result.StreamedRunResult.validate_response_output] validates the data, `allow_partial=True` enables pydantic's [`experimental_allow_partial` flag on `TypeAdapter`][pydantic.type_adapter.TypeAdapter.validate_json].\n",
      "\n",
      "_(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)_\n",
      "\n",
      "## Examples\n",
      "\n",
      "The following examples demonstrate how to use streamed responses in Pydantic AI:\n",
      "\n",
      "- [Stream markdown](examples/stream-markdown.md)\n",
      "- [Stream Whales](examples/stream-whales.md)\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b86ca32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Output\" refers to the final value returned from [running an agent](agents.md#running-agents). This can be either plain text, [structured data](#structured-output), or the result of a [function](#output-functions) called with arguments provided by the model.',\n",
       " 'The output is wrapped in [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] or [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] so that you can access other data, like [usage][pydantic_ai.usage.RunUsage] of the run and [message history](message-history.md#accessing-messages-from-results).',\n",
       " 'Both `AgentRunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eff6c7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 167\n"
     ]
    }
   ],
   "source": [
    "print(f\"Paragraphs: {len(paragraphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9eb41",
   "metadata": {},
   "source": [
    "#### Sections\n",
    "\n",
    "Markdown documents have this structure:\n",
    "\n",
    "```text\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdd6dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48545d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 3\n"
     ]
    }
   ],
   "source": [
    "sections = split_markdown_by_level(text, level=2)\n",
    "print(f\"Sections: {len(sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f194b627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Output data {#structured-output}\\n\\nThe [`Agent`][pydantic_ai.Agent] class constructor takes an `output_type` argument that takes one or more types or [output functions](#output-functions). It supports simple scalar types, list and dict types (including `TypedDict`s and [`StructuredDict`s](#structured-dict)), dataclasses and Pydantic models, as well as type unions -- generally everything supported as type hints in a Pydantic model. You can also pass a list of multiple choices.\\n\\nBy default, Pydantic AI leverages the model\\'s tool calling capability to make it return structured data. When multiple output types are specified (in a union or list), each member is registered with the model as a separate output tool in order to reduce the complexity of the schema and maximise the chances a model will respond correctly. This has been shown to work well across a wide range of models. If you\\'d like to change the names of the output tools, use a model\\'s native structured output feature, or pass the output schema to the model in its [instructions](agents.md#instructions), you can use an [output mode](#output-modes) marker class.\\n\\nWhen no output type is specified, or when `str` is among the output types, any plain text response from the model will be used as the output data.\\nIf `str` is not among the output types, the model is forced to return structured data or call an output function.\\n\\nIf the output type schema is not of type `\"object\"` (e.g. it\\'s `int` or `list[int]`), the output type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\\n\\nStructured outputs (like tools) use Pydantic to build the JSON schema used for the tool, and to validate the data returned by the model.\\n\\n!!! note \"Type checking considerations\"\\n    The Agent class is generic in its output type, and this type is carried through to `AgentRunResult.output` and `StreamedRunResult.output` so that your IDE or static type checker can warn you when your code doesn\\'t properly take into account all the possible values those outputs could have.\\n\\n    Static type checkers like pyright and mypy will do their best to infer the agent\\'s output type from the `output_type` you\\'ve specified, but they\\'re not always able to do so correctly when you provide functions or multiple types in a union or list, even though Pydantic AI will behave correctly. When this happens, your type checker will complain even when you\\'re confident you\\'ve passed a valid `output_type`, and you\\'ll need to help the type checker by explicitly specifying the generic parameters on the `Agent` constructor. This is shown in the second example below and the output functions example further down.\\n\\n    Specifically, there are three valid uses of `output_type` where you\\'ll need to do this:\\n\\n    1. When using a union of types, e.g. `output_type=Foo | Bar`. Until [PEP-747](https://peps.python.org/pep-0747/) \"Annotating Type Forms\" lands in Python 3.15, type checkers do not consider these a valid value for `output_type`. In addition to the generic parameters on the `Agent` constructor, you\\'ll need to add `# type: ignore` to the line that passes the union to `output_type`. Alternatively, you can use a list: `output_type=[Foo, Bar]`.\\n    2. With mypy: When using a list, as a functionally equivalent alternative to a union, or because you\\'re passing in [output functions](#output-functions). Pyright does handle this correctly, and we\\'ve filed [an issue](https://github.com/python/mypy/issues/19142) with mypy to try and get this fixed.\\n    3. With mypy: when using an async output function. Pyright does handle this correctly, and we\\'ve filed [an issue](https://github.com/python/mypy/issues/19143) with mypy to try and get this fixed.\\n\\nHere\\'s an example of returning either text or structured data:\\n\\n```python {title=\"box_or_error.py\"}\\n\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent\\n\\n\\nclass Box(BaseModel):\\n    width: int\\n    height: int\\n    depth: int\\n    units: str\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o-mini\\',\\n    output_type=[Box, str], # (1)!\\n    system_prompt=(\\n        \"Extract me the dimensions of a box, \"\\n        \"if you can\\'t extract all data, ask the user to try again.\"\\n    ),\\n)\\n\\nresult = agent.run_sync(\\'The box is 10x20x30\\')\\nprint(result.output)\\n#> Please provide the units for the dimensions (e.g., cm, in, m).\\n\\nresult = agent.run_sync(\\'The box is 10x20x30 cm\\')\\nprint(result.output)\\n#> width=10 height=20 depth=30 units=\\'cm\\'\\n```\\n\\n1. This could also have been a union: `output_type=Box | str`. However, as explained in the \"Type checking considerations\" section above, that would\\'ve required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nHere\\'s an example of using a union return type, which will register multiple output tools and wrap non-object schemas in an object:\\n\\n```python {title=\"colors_or_sizes.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent[None, list[str] | list[int]](\\n    \\'openai:gpt-4o-mini\\',\\n    output_type=list[str] | list[int],  # type: ignore # (1)!\\n    system_prompt=\\'Extract either colors or sizes from the shapes provided.\\',\\n)\\n\\nresult = agent.run_sync(\\'red square, blue circle, green triangle\\')\\nprint(result.output)\\n#> [\\'red\\', \\'blue\\', \\'green\\']\\n\\nresult = agent.run_sync(\\'square size 10, circle size 20, triangle size 30\\')\\nprint(result.output)\\n#> [10, 20, 30]\\n```\\n\\n1. As explained in the \"Type checking considerations\" section above, using a union rather than a list requires explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n### Output functions\\n\\nInstead of plain text or structured data, you may want the output of your agent run to be the result of a function called with arguments provided by the model, for example to further process or validate the data provided through the arguments (with the option to tell the model to try again), or to hand off to another agent.\\n\\nOutput functions are similar to [function tools](tools.md), but the model is forced to call one of them, the call ends the agent run, and the result is not passed back to the model.\\n\\nAs with tool functions, output function arguments provided by the model are validated using Pydantic, they can optionally take [`RunContext`][pydantic_ai.tools.RunContext] as the first argument, and they can raise [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] to ask the model to try again with modified arguments (or with a different output type).\\n\\nTo specify output functions, you set the agent\\'s `output_type` to either a single function (or bound instance method), or a list of functions. The list can also contain other output types like simple scalars or entire Pydantic models.\\nYou typically do not want to also register your output function as a tool (using the `@agent.tool` decorator or `tools` argument), as this could confuse the model about which it should be calling.\\n\\nHere\\'s an example of all of these features in action:\\n\\n```python {title=\"output_functions.py\"}\\nimport re\\n\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent, ModelRetry, RunContext, UnexpectedModelBehavior\\n\\n\\nclass Row(BaseModel):\\n    name: str\\n    country: str\\n\\n\\ntables = {\\n    \\'capital_cities\\': [\\n        Row(name=\\'Amsterdam\\', country=\\'Netherlands\\'),\\n        Row(name=\\'Mexico City\\', country=\\'Mexico\\'),\\n    ]\\n}\\n\\n\\nclass SQLFailure(BaseModel):\\n    \"\"\"An unrecoverable failure. Only use this when you can\\'t change the query to make it work.\"\"\"\\n\\n    explanation: str\\n\\n\\ndef run_sql_query(query: str) -> list[Row]:\\n    \"\"\"Run a SQL query on the database.\"\"\"\\n\\n    select_table = re.match(r\\'SELECT (.+) FROM (\\\\w+)\\', query)\\n    if select_table:\\n        column_names = select_table.group(1)\\n        if column_names != \\'*\\':\\n            raise ModelRetry(\"Only \\'SELECT *\\' is supported, you\\'ll have to do column filtering manually.\")\\n\\n        table_name = select_table.group(2)\\n        if table_name not in tables:\\n            raise ModelRetry(\\n                f\"Unknown table \\'{table_name}\\' in query \\'{query}\\'. Available tables: {\\', \\'.join(tables.keys())}.\"\\n            )\\n\\n        return tables[table_name]\\n\\n    raise ModelRetry(f\"Unsupported query: \\'{query}\\'.\")\\n\\n\\nsql_agent = Agent[None, list[Row] | SQLFailure](\\n    \\'openai:gpt-4o\\',\\n    output_type=[run_sql_query, SQLFailure],\\n    instructions=\\'You are a SQL agent that can run SQL queries on a database.\\',\\n)\\n\\n\\nasync def hand_off_to_sql_agent(ctx: RunContext, query: str) -> list[Row]:\\n    \"\"\"I take natural language queries, turn them into SQL, and run them on a database.\"\"\"\\n\\n    # Drop the final message with the output tool call, as it shouldn\\'t be passed on to the SQL agent\\n    messages = ctx.messages[:-1]\\n    try:\\n        result = await sql_agent.run(query, message_history=messages)\\n        output = result.output\\n        if isinstance(output, SQLFailure):\\n            raise ModelRetry(f\\'SQL agent failed: {output.explanation}\\')\\n        return output\\n    except UnexpectedModelBehavior as e:\\n        # Bubble up potentially retryable errors to the router agent\\n        if (cause := e.__cause__) and isinstance(cause, ModelRetry):\\n            raise ModelRetry(f\\'SQL agent failed: {cause.message}\\') from e\\n        else:\\n            raise\\n\\n\\nclass RouterFailure(BaseModel):\\n    \"\"\"Use me when no appropriate agent is found or the used agent failed.\"\"\"\\n\\n    explanation: str\\n\\n\\nrouter_agent = Agent[None, list[Row] | RouterFailure](\\n    \\'openai:gpt-4o\\',\\n    output_type=[hand_off_to_sql_agent, RouterFailure],\\n    instructions=\\'You are a router to other agents. Never try to solve a problem yourself, just pass it on.\\',\\n)\\n\\nresult = router_agent.run_sync(\\'Select the names and countries of all capitals\\')\\nprint(result.output)\\n\"\"\"\\n[\\n    Row(name=\\'Amsterdam\\', country=\\'Netherlands\\'),\\n    Row(name=\\'Mexico City\\', country=\\'Mexico\\'),\\n]\\n\"\"\"\\n\\nresult = router_agent.run_sync(\\'Select all pets\\')\\nprint(repr(result.output))\\n\"\"\"\\nRouterFailure(explanation=\"The requested table \\'pets\\' does not exist in the database. The only available table is \\'capital_cities\\', which does not contain data about pets.\")\\n\"\"\"\\n\\nresult = router_agent.run_sync(\\'How do I fly from Amsterdam to Mexico City?\\')\\nprint(repr(result.output))\\n\"\"\"\\nRouterFailure(explanation=\\'I am not equipped to provide travel information, such as flights from Amsterdam to Mexico City.\\')\\n\"\"\"\\n```\\n\\n#### Text output\\n\\nIf you provide an output function that takes a string, Pydantic AI will by default create an output tool like for any other output function. If instead you\\'d like the model to provide the string using plain text output, you can wrap the function in the [`TextOutput`][pydantic_ai.output.TextOutput] marker class. If desired, this marker class can be used alongside one or more [`ToolOutput`](#tool-output) marker classes (or unmarked types or functions) in a list provided to `output_type`.\\n\\n```python {title=\"text_output_function.py\"}\\nfrom pydantic_ai import Agent, TextOutput\\n\\n\\ndef split_into_words(text: str) -> list[str]:\\n    return text.split()\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    output_type=TextOutput(split_into_words),\\n)\\nresult = agent.run_sync(\\'Who was Albert Einstein?\\')\\nprint(result.output)\\n#> [\\'Albert\\', \\'Einstein\\', \\'was\\', \\'a\\', \\'German-born\\', \\'theoretical\\', \\'physicist.\\']\\n```\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n### Output modes\\n\\nPydantic AI implements three different methods to get a model to output structured data:\\n\\n1. [Tool Output](#tool-output), where tool calls are used to produce the output.\\n2. [Native Output](#native-output), where the model is required to produce text content compliant with a provided JSON schema.\\n3. [Prompted Output](#prompted-output), where a prompt is injected into the model instructions including the desired JSON schema, and we attempt to parse the model\\'s plain-text response as appropriate.\\n\\n#### Tool Output\\n\\nIn the default Tool Output mode, the output JSON schema of each output type (or function) is provided to the model as the parameters schema of a special output tool. This is the default as it\\'s supported by virtually all models and has been shown to work very well.\\n\\nIf you\\'d like to change the name of the output tool, pass a custom description to aid the model, or turn on or off strict mode, you can wrap the type(s) in the [`ToolOutput`][pydantic_ai.output.ToolOutput] marker class and provide the appropriate arguments. Note that by default, the description is taken from the docstring specified on a Pydantic model or output function, so specifying it using the marker class is typically not necessary.\\n\\nTo dynamically modify or filter the available output tools during an agent run, you can define an agent-wide `prepare_output_tools` function that will be called ahead of each step of a run. This function should be of type [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc], which takes the [`RunContext`][pydantic_ai.tools.RunContext] and a list of [`ToolDefinition`][pydantic_ai.tools.ToolDefinition], and returns a new list of tool definitions (or `None` to disable all tools for that step). This is analogous to the [`prepare_tools` function](tools-advanced.md#prepare-tools) for non-output tools.\\n\\n```python {title=\"tool_output.py\"}\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent, ToolOutput\\n\\n\\nclass Fruit(BaseModel):\\n    name: str\\n    color: str\\n\\n\\nclass Vehicle(BaseModel):\\n    name: str\\n    wheels: int\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    output_type=[ # (1)!\\n        ToolOutput(Fruit, name=\\'return_fruit\\'),\\n        ToolOutput(Vehicle, name=\\'return_vehicle\\'),\\n    ],\\n)\\nresult = agent.run_sync(\\'What is a banana?\\')\\nprint(repr(result.output))\\n#> Fruit(name=\\'banana\\', color=\\'yellow\\')\\n```\\n\\n1. If we were passing just `Fruit` and `Vehicle` without custom tool names, we could have used a union: `output_type=Fruit | Vehicle`. However, as `ToolOutput` is an object rather than a type, we have to use a list.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n#### Native Output\\n\\nNative Output mode uses a model\\'s native \"Structured Outputs\" feature (aka \"JSON Schema response format\"), where the model is forced to only output text matching the provided JSON schema. Note that this is not supported by all models, and sometimes comes with restrictions. For example, Anthropic does not support this at all, and Gemini cannot use tools at the same time as structured output, and attempting to do so will result in an error.\\n\\nTo use this mode, you can wrap the output type(s) in the [`NativeOutput`][pydantic_ai.output.NativeOutput] marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient.\\n\\n```python {title=\"native_output.py\" requires=\"tool_output.py\"}\\nfrom pydantic_ai import Agent, NativeOutput\\n\\nfrom tool_output import Fruit, Vehicle\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    output_type=NativeOutput(\\n        [Fruit, Vehicle], # (1)!\\n        name=\\'Fruit_or_vehicle\\',\\n        description=\\'Return a fruit or vehicle.\\'\\n    ),\\n)\\nresult = agent.run_sync(\\'What is a Ford Explorer?\\')\\nprint(repr(result.output))\\n#> Vehicle(name=\\'Ford Explorer\\', wheels=4)\\n```\\n\\n1. This could also have been a union: `output_type=Fruit | Vehicle`. However, as explained in the \"Type checking considerations\" section above, that would\\'ve required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n#### Prompted Output\\n\\nIn this mode, the model is prompted to output text matching the provided JSON schema through its [instructions](agents.md#instructions) and it\\'s up to the model to interpret those instructions correctly. This is usable with all models, but is often the least reliable approach as the model is not forced to match the schema.\\n\\nWhile we would generally suggest starting with tool or native output, in some cases this mode may result in higher quality outputs, and for models without native tool calling or structured output support it is the only option for producing structured outputs.\\n\\nIf the model API supports the \"JSON Mode\" feature (aka \"JSON Object response format\") to force the model to output valid JSON, this is enabled, but it\\'s still up to the model to abide by the schema. Pydantic AI will validate the returned structured data and tell the model to try again if validation fails, but if the model is not intelligent enough this may not be sufficient.\\n\\nTo use this mode, you can wrap the output type(s) in the [`PromptedOutput`][pydantic_ai.output.PromptedOutput] marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient. Additionally, it supports an `template` argument lets you specify a custom instructions template to be used instead of the [default][pydantic_ai.profiles.ModelProfile.prompted_output_template].\\n\\n```python {title=\"prompted_output.py\" requires=\"tool_output.py\"}\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent, PromptedOutput\\n\\nfrom tool_output import Vehicle\\n\\n\\nclass Device(BaseModel):\\n    name: str\\n    kind: str\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    output_type=PromptedOutput(\\n        [Vehicle, Device], # (1)!\\n        name=\\'Vehicle or device\\',\\n        description=\\'Return a vehicle or device.\\'\\n    ),\\n)\\nresult = agent.run_sync(\\'What is a MacBook?\\')\\nprint(repr(result.output))\\n#> Device(name=\\'MacBook\\', kind=\\'laptop\\')\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    output_type=PromptedOutput(\\n        [Vehicle, Device],\\n        template=\\'Gimme some JSON: {schema}\\'\\n    ),\\n)\\nresult = agent.run_sync(\\'What is a Ford Explorer?\\')\\nprint(repr(result.output))\\n#> Vehicle(name=\\'Ford Explorer\\', wheels=4)\\n```\\n\\n1. This could also have been a union: `output_type=Vehicle | Device`. However, as explained in the \"Type checking considerations\" section above, that would\\'ve required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n### Custom JSON schema {#structured-dict}\\n\\nIf it\\'s not feasible to define your desired structured output object using a Pydantic `BaseModel`, dataclass, or `TypedDict`, for example when you get a JSON schema from an external source or generate it dynamically, you can use the [`StructuredDict()`][pydantic_ai.output.StructuredDict] helper function to generate a `dict[str, Any]` subclass with a JSON schema attached that Pydantic AI will pass to the model.\\n\\nNote that Pydantic AI will not perform any validation of the received JSON object and it\\'s up to the model to correctly interpret the schema and any constraints expressed in it, like required fields or integer value ranges.\\n\\nThe output type will be a `dict[str, Any]` and it\\'s up to your code to defensively read from it in case the model made a mistake. You can use an [output validator](#output-validator-functions) to reflect validation errors back to the model and get it to try again.\\n\\nAlong with the JSON schema, you can optionally pass `name` and `description` arguments to provide additional context to the model:\\n\\n```python\\nfrom pydantic_ai import Agent, StructuredDict\\n\\nHumanDict = StructuredDict(\\n    {\\n        \\'type\\': \\'object\\',\\n        \\'properties\\': {\\n            \\'name\\': {\\'type\\': \\'string\\'},\\n            \\'age\\': {\\'type\\': \\'integer\\'}\\n        },\\n        \\'required\\': [\\'name\\', \\'age\\']\\n    },\\n    name=\\'Human\\',\\n    description=\\'A human with a name and age\\',\\n)\\n\\nagent = Agent(\\'openai:gpt-4o\\', output_type=HumanDict)\\nresult = agent.run_sync(\\'Create a person\\')\\n#> {\\'name\\': \\'John Doe\\', \\'age\\': 30}\\n```\\n\\n### Output validators {#output-validator-functions}\\n\\nSome validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. Pydantic AI provides a way to add validation functions via the [`agent.output_validator`][pydantic_ai.Agent.output_validator] decorator.\\n\\nIf you want to implement separate validation logic for different output types, it\\'s recommended to use [output functions](#output-functions) instead, to save you from having to do `isinstance` checks inside the output validator.\\nIf you want the model to output plain text, do your own processing or validation, and then have the agent\\'s final output be the result of your function, it\\'s recommended to use an [output function](#output-functions) with the [`TextOutput` marker class](#text-output).\\n\\nHere\\'s a simplified variant of the [SQL Generation example](examples/sql-gen.md):\\n\\n```python {title=\"sql_gen.py\"}\\nfrom fake_database import DatabaseConn, QueryError\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent, RunContext, ModelRetry\\n\\n\\nclass Success(BaseModel):\\n    sql_query: str\\n\\n\\nclass InvalidRequest(BaseModel):\\n    error_message: str\\n\\n\\nOutput = Success | InvalidRequest\\nagent = Agent[DatabaseConn, Output](\\n    \\'google-gla:gemini-1.5-flash\\',\\n    output_type=Output,  # type: ignore\\n    deps_type=DatabaseConn,\\n    system_prompt=\\'Generate PostgreSQL flavored SQL queries based on user input.\\',\\n)\\n\\n\\n@agent.output_validator\\nasync def validate_sql(ctx: RunContext[DatabaseConn], output: Output) -> Output:\\n    if isinstance(output, InvalidRequest):\\n        return output\\n    try:\\n        await ctx.deps.execute(f\\'EXPLAIN {output.sql_query}\\')\\n    except QueryError as e:\\n        raise ModelRetry(f\\'Invalid query: {e}\\') from e\\n    else:\\n        return output\\n\\n\\nresult = agent.run_sync(\\n    \\'get me users who were last active yesterday.\\', deps=DatabaseConn()\\n)\\nprint(result.output)\\n#> sql_query=\\'SELECT * FROM users WHERE last_active::date = today() - interval 1 day\\'\\n```\\n\\n_(This example is complete, it can be run \"as is\")_'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a58abb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently sections: 262\n"
     ]
    }
   ],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "        \n",
    "print(f\"Evidently sections: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f89e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Introduction',\n",
       " 'description': 'Example section for showcasing API endpoints',\n",
       " 'filename': 'docs-main/api-reference/introduction.mdx',\n",
       " 'section': '## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "269b9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic sections: 264\n"
     ]
    }
   ],
   "source": [
    "pydanticai_chunks = []\n",
    "\n",
    "for doc in pydanticai_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        pydanticai_chunks.append(section_doc)\n",
    "        \n",
    "print(f\"Pydantic sections: {len(pydanticai_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83896a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'pydantic-ai-main/CLAUDE.md',\n",
       " 'section': '## Development Commands\\n\\n### Core Development Tasks\\n\\n- **Install dependencies**: `make install` (requires uv, pre-commit, and deno)\\n- **Run all checks**: `pre-commit run --all-files`\\n- **Run tests**: `make test`\\n- **Build docs**: `make docs` or `make docs-serve` (local development)\\n\\n### Single Test Commands\\n\\n- **Run specific test**: `uv run pytest tests/test_agent.py::test_function_name -v`\\n- **Run test file**: `uv run pytest tests/test_agent.py -v`\\n- **Run with debug**: `uv run pytest tests/test_agent.py -v -s`'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydanticai_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cca2c0",
   "metadata": {},
   "source": [
    "### 3. Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9462ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# openai_client = OpenAI()\n",
    "\n",
    "\n",
    "# def llm(prompt, model='gpt-4o-mini'):\n",
    "#     messages = [\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "\n",
    "#     response = openai_client.responses.create(\n",
    "#         model='gpt-4o-mini',\n",
    "#         input=messages\n",
    "#     )\n",
    "\n",
    "#     return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d462d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "def llm(prompt, model=\"openai/gpt-oss-20b\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        messages = messages,\n",
    "        model = model,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0ebde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596cf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23f408",
   "metadata": {},
   "source": [
    "The prompt asks the LLM to:\n",
    "\n",
    "- Split the document logically (not just by length)\n",
    "- Make sections self-contained\n",
    "- Use a specific output format that's easy to parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e03fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a679ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78695d50a8b4287b9e3c8725dbe95f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently sections: 124\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs[:10]): # Limiting to first 10 docs for cost control\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    \n",
    "    if len(doc_content) == 0:\n",
    "        continue \n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "        \n",
    "print(f\"Evidently sections: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f0ddf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "## Optional API Reference Folder\n",
      "\n",
      "If you are not looking to build API reference documentation, you can delete this section by removing the `api-reference` folder.\n",
      "\n",
      "================================================================================\n",
      "## Getting Started\n",
      "\n",
      "There are two ways to build API documentation:\n",
      "\n",
      "1. **OpenAPI** – use an OpenAPI specification file.  \n",
      "2. **MDX Components** – use custom MDX components for documentation.\n",
      "\n",
      "For the starter kit, we are using the following OpenAPI specification.\n",
      "\n",
      "================================================================================\n",
      "## API Specification\n",
      "\n",
      "**Plant Store Endpoints**\n",
      "\n",
      "- **Specification File:**  \n",
      "  <https://github.com/mintlify/starter/blob/main/api-reference/openapi.json>\n",
      "\n",
      "  The OpenAPI specification file defines all of the Plant Store API endpoints, request/response schemas, and metadata. You can view or download it directly from the link above.\n",
      "\n",
      "================================================================================\n",
      "## Authentication\n",
      "\n",
      "All API endpoints are authenticated using Bearer tokens. The security configuration is defined in the OpenAPI specification as follows:\n",
      "\n",
      "```json\n",
      "\"security\": [\n",
      "  {\n",
      "    \"bearerAuth\": []\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Clients must include a valid Bearer token in the `Authorization` header to access the endpoints.\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.11 Release\n",
      "\n",
      "**Date**: 2025‑07‑18  \n",
      "**Version**: 0.7.11  \n",
      "\n",
      "Full release notes can be found on [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).  \n",
      "This release introduces several new example notebooks:\n",
      "\n",
      "- **Synthetic data generation** – a code example demonstrating how to generate synthetic datasets for evaluation purposes:  \n",
      "  [datagen.ipynb](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.10 Release and Automated Prompt Optimization\n",
      "\n",
      "**Date**: 2025‑07‑09  \n",
      "**Version**: 0.7.10  \n",
      "\n",
      "Full release notes are available on [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).  \n",
      "\n",
      "**New Feature** – *Automated Prompt Optimization*  \n",
      "- A new automated system that optimizes prompts for LLM judges.  \n",
      "- Blog post detailing the approach: [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\n",
      "\n",
      "**Example notebooks** that showcase prompt optimization for different tasks:\n",
      "\n",
      "- Code review binary LLM judge prompt optimization:  \n",
      "  [code_review_example.ipynb](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)  \n",
      "- Topic multi‑class LLM judge prompt optimization:  \n",
      "  [bookings_example.ipynb](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)  \n",
      "- Tweet generation prompt optimization:  \n",
      "  [tw\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.9 Release\n",
      "\n",
      "**Date**: 2025‑06‑27  \n",
      "**Version**: 0.7.9  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.8 Release\n",
      "\n",
      "**Date**: 2025‑06‑19  \n",
      "**Version**: 0.7.8  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.7 Release\n",
      "\n",
      "**Date**: 2025‑06‑04  \n",
      "**Version**: 0.7.7  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.6 Release\n",
      "\n",
      "**Date**: 2025‑05‑25  \n",
      "**Version**: 0.7.6  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in evidently_chunks[:10]:\n",
    "    print(\"=\"*80)\n",
    "    print(chunk['section'][:1000])  # Print first 1000 characters of the section\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251100c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
