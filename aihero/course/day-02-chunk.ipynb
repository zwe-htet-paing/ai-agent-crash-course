{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861342ab",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "\n",
    "- https://docs.google.com/document/d/12wVi866gQDFSw09LZdTixYltqY4_P0g7x04IyYDil5o/edit?tab=t.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed964730",
   "metadata": {},
   "source": [
    "### Why We Need to Prepare Large Documents Before Using Them\n",
    "\n",
    "Large documents create several problems:\n",
    "\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG, this process is referred to as \"chunking.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d5b33",
   "metadata": {},
   "source": [
    "### Loading Data from Day-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121dd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ingest import read_repo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e062c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e91883",
   "metadata": {},
   "source": [
    "### 1. Simple Chunking with Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a742758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ae5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033b08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently chunks: 575\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently chunks: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0e8a3",
   "metadata": {},
   "source": [
    "### 2. Splitting by Paragraphs and Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ec13c",
   "metadata": {},
   "source": [
    "#### Paragraphs\n",
    "\n",
    "Use `\\n\\s*\\n` regex pattern for splitting:\n",
    "\n",
    "- `\\n` matches a newline\n",
    "- `\\s*` matches zero or more whitespace characters\n",
    "- `\\n` matches another newline\n",
    "- So `\\n\\s*\\n` matches two newlines with optional whitespace between them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7d39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting by paragraphs\n",
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r'\\n\\s*\\n', text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37273b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In this tutorial, you will learn how to perform regression testing for LLM '\n",
      " 'outputs.\\n'\n",
      " '\\n'\n",
      " 'You can compare new and old responses after changing a prompt, model, or '\n",
      " 'anything else in your system. By re-running the same inputs with new '\n",
      " 'parameters, you can spot any significant changes. This helps you push '\n",
      " 'updates with confidence or identify issues to fix.\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " \"  **This example uses Evidently Cloud.** You'll run evals in Python and \"\n",
      " 'upload them. You can also skip the upload and view Reports locally. For '\n",
      " 'self-hosted, replace `CloudWorkspace` with `Workspace`.\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " '# Tutorial scope\\n'\n",
      " '\\n'\n",
      " \"Here's what we'll do:\\n\"\n",
      " '\\n'\n",
      " '* **Create a toy dataset**. Build a small Q&A dataset with answers and '\n",
      " 'reference responses.\\n'\n",
      " '\\n'\n",
      " '* **Get new answers**. Imitate generating new answers to the same question.\\n'\n",
      " '\\n'\n",
      " '* **Create and run a Report with Tests**. Compare the answers using '\n",
      " 'LLM-as-a-judge to evaluate length, correctness and style consistency.\\n'\n",
      " '\\n'\n",
      " '* **Build a monitoring Dashboard**. Get plots to track the results of Tests '\n",
      " 'over time.\\n'\n",
      " '\\n'\n",
      " '<Note>\\n'\n",
      " \"  To simplify things, we won't create an actual LLM app, but will simulate \"\n",
      " 'generating new outputs.\\n'\n",
      " '</Note>\\n'\n",
      " '\\n'\n",
      " 'To complete the tutorial, you will need:\\n'\n",
      " '\\n'\n",
      " '* Basic Python knowledge.\\xa0\\n'\n",
      " '\\n'\n",
      " '* An OpenAI API key to use for the LLM evaluator.\\n'\n",
      " '\\n'\n",
      " '* An Evidently Cloud account to track test results. If not yet, [sign '\n",
      " 'up](https://www.evidentlyai.com/register) for a free account.\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  You can see all the code in [Jupyter '\n",
      " 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) '\n",
      " 'or click to [open in '\n",
      " 'Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " '## 1. Installation and Imports\\n'\n",
      " '\\n'\n",
      " 'Install Evidently:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'pip install evidently[llm] \\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Import the required modules:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'import pandas as pd\\n'\n",
      " 'from evidently.future.datasets import Dataset\\n'\n",
      " 'from evidently.future.datasets import DataDefinition\\n'\n",
      " 'from evidently.future.datasets import Descriptor\\n'\n",
      " 'from evidently.future.descriptors import *\\n'\n",
      " 'from evidently.future.report import Report\\n'\n",
      " 'from evidently.future.presets import TextEvals\\n'\n",
      " 'from evidently.future.metrics import *\\n'\n",
      " 'from evidently.future.tests import *\\n'\n",
      " '\\n'\n",
      " 'from evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'To connect to Evidently Cloud:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'from evidently.ui.workspace.cloud import CloudWorkspace\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '**Optional.** To create monitoring panels as code:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'from evidently.ui.dashboards import DashboardPanelPlot\\n'\n",
      " 'from evidently.ui.dashboards import DashboardPanelTestSuite\\n'\n",
      " 'from evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\n'\n",
      " 'from evidently.ui.dashboards import TestSuitePanelType\\n'\n",
      " 'from evidently.ui.dashboards import ReportFilter\\n'\n",
      " 'from evidently.ui.dashboards import PanelValue\\n'\n",
      " 'from evidently.ui.dashboards import PlotType\\n'\n",
      " 'from evidently.ui.dashboards import CounterAgg\\n'\n",
      " 'from evidently.tests.base_test import TestStatus\\n'\n",
      " 'from evidently.renderers.html_widgets import WidgetSize\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Pass your OpenAI key:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'import os\\n'\n",
      " 'os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '## 2. Create a Project\\n'\n",
      " '\\n'\n",
      " 'Connect to Evidently Cloud. Replace with your actual token:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'ws = CloudWorkspace(token=\"YOUR_API_TOKEN\", '\n",
      " 'url=\"https://app.evidently.cloud\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Create a Project:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'project = ws.create_project(\"Regression testing example\", '\n",
      " 'org_id=\"YOUR_ORG_ID\")\\n'\n",
      " 'project.description = \"My project description\"\\n'\n",
      " 'project.save()\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '## 3. Prepare the Dataset\\n'\n",
      " '\\n'\n",
      " 'Create a toy dataset with questions and reference answers.&#x20;\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'data = [\\n'\n",
      " '    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air '\n",
      " 'scatter blue light from the sun more than they scatter red light.\"],\\n'\n",
      " '    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because '\n",
      " 'their wings create lift by forcing air to move faster over the top of the '\n",
      " 'wing than underneath, which creates lower pressure on top.\"],\\n'\n",
      " '    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted '\n",
      " 'on its axis, which causes different parts of the Earth to receive more or '\n",
      " 'less sunlight throughout the year.\"],\\n'\n",
      " '    [\"How do magnets work?\", \"Magnets work because they have a magnetic '\n",
      " 'field that can attract or repel certain metals, like iron, due to the '\n",
      " 'alignment of their atomic particles.\"],\\n'\n",
      " '    [\"Why does the moon change shape?\", \"The moon changes shape, or goes '\n",
      " 'through phases, because we see different portions of its illuminated half as '\n",
      " 'it orbits the Earth.\"]\\n'\n",
      " ']\\n'\n",
      " '\\n'\n",
      " 'columns = [\"question\", \"target_response\"]\\n'\n",
      " '\\n'\n",
      " 'ref_data = pd.DataFrame(data, columns=columns)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Get a quick preview:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " \"pd.set_option('display.max_colwidth', None)\\n\"\n",
      " 'ref_data.head()\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Here is how the data looks:\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n'\n",
      " '\\n'\n",
      " '**Optional: quick data exploration.** You might want to have a quick look at '\n",
      " \"some data statistics to help you set conditions for Tests. Let's check the \"\n",
      " 'text length and sentence count distribution.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'ref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\n'\n",
      " 'data_definition=DataDefinition(),\\n'\n",
      " 'descriptors=[\\n'\n",
      " '    TextLength(\"target_response\", alias=\"Length\"),\\n'\n",
      " '    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n'\n",
      " '])\\n'\n",
      " 'ref_dataset.as_dataframe()\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'In this code, you:\\n'\n",
      " '\\n'\n",
      " '* Created an Evidently Dataset object with automatic [data '\n",
      " 'definition](/docs/library/data_definition).\\n'\n",
      " '\\n'\n",
      " '* Added two built-in descriptors on text length and symbol count. ([See '\n",
      " 'others](/metrics/all_descriptors)).\\n'\n",
      " '\\n'\n",
      " '* Exported results as a dataframe.\\n'\n",
      " '\\n'\n",
      " 'Here is the preview:\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n'\n",
      " '\\n'\n",
      " 'In a small dataset, you can grasp it all at once. For a larger dataset, you '\n",
      " 'can add a summary report to see the distribution.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'report = Report([\\n'\n",
      " '    TextEvals(),\\n'\n",
      " '])\\n'\n",
      " '\\n'\n",
      " 'my_eval = report.run(ref_dataset, None)\\n'\n",
      " 'my_eval\\n'\n",
      " '\\n'\n",
      " '#my_eval.as_dict()\\n'\n",
      " '#my_eval.json()\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'This renders the Report directly in the interactive Python environment like '\n",
      " 'Jupyter notebook or Colab. See other [export '\n",
      " 'options](/docs/library/output_formats).\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n'\n",
      " '\\n'\n",
      " '## 4. Get new answers\\n'\n",
      " '\\n'\n",
      " 'Suppose you generate new responses using your LLM after changing a prompt. '\n",
      " 'We will imitate it by adding a new column with new responses to the '\n",
      " 'DataFrame:\\n'\n",
      " '\\n'\n",
      " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n'\n",
      " '  Run this code to generate a new dataset.\\n'\n",
      " '\\n'\n",
      " '  ```python\\n'\n",
      " '  data = [\\n'\n",
      " '    [\"Why is the sky blue?\",\\n'\n",
      " '     \"The sky is blue because molecules in the air scatter blue light from '\n",
      " 'the sun more than they scatter red light.\",\\n'\n",
      " '     \"The sky appears blue because air molecules scatter the sun’s blue '\n",
      " 'light more than they scatter other colors.\"],\\n'\n",
      " '\\n'\n",
      " '    [\"How do airplanes stay in the air?\",\\n'\n",
      " '     \"Airplanes stay in the air because their wings create lift by forcing '\n",
      " 'air to move faster over the top of the wing than underneath, which creates '\n",
      " 'lower pressure on top.\",\\n'\n",
      " '     \"Airplanes stay airborne because the shape of their wings causes air to '\n",
      " 'move faster over the top than the bottom, generating lift.\"],\\n'\n",
      " '\\n'\n",
      " '    [\"Why do we have seasons?\",\\n'\n",
      " '     \"We have seasons because the Earth is tilted on its axis, which causes '\n",
      " 'different parts of the Earth to receive more or less sunlight throughout the '\n",
      " 'year.\",\\n'\n",
      " '     \"Seasons occur because of the tilt of the Earth’s axis, leading to '\n",
      " 'varying amounts of sunlight reaching different areas as the Earth orbits the '\n",
      " 'sun.\"],\\n'\n",
      " '\\n'\n",
      " '    [\"How do magnets work?\",\\n'\n",
      " '     \"Magnets work because they have a magnetic field that can attract or '\n",
      " 'repel certain metals, like iron, due to the alignment of their atomic '\n",
      " 'particles.\",\\n'\n",
      " '     \"Magnets generate a magnetic field, which can attract metals like iron '\n",
      " 'by causing the electrons in those metals to align in a particular way, '\n",
      " 'creating an attractive or repulsive force.\"],\\n'\n",
      " '\\n'\n",
      " '    [\"Why does the moon change shape?\",\\n'\n",
      " '     \"The moon changes shape, or goes through phases, because we see '\n",
      " 'different portions of its illuminated half as it orbits the Earth.\",\\n'\n",
      " '     \"The moon appears to change shape as it orbits Earth, which is because '\n",
      " 'we see different parts of its lit-up half at different times. The sun lights '\n",
      " 'up half of the moon, but as the moon moves around the Earth, we see varying '\n",
      " \"portions of that lit-up side. So, the moon's shape in the sky seems to \"\n",
      " 'change gradually, from a thin crescent to a full circle and back to a '\n",
      " 'crescent again.\"]\\n'\n",
      " '  ]\\n'\n",
      " '\\n'\n",
      " '  columns = [\"question\", \"target_response\", \"response\"]\\n'\n",
      " '\\n'\n",
      " '  eval_data = pd.DataFrame(data, columns=columns)\\n'\n",
      " '  ```\\n'\n",
      " '</Accordion>\\n'\n",
      " '\\n'\n",
      " 'Here is the resulting dataset with the added new column:\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  **How to connect it with your app?** Replace this step with calling your '\n",
      " 'LLM app to score the inputs and add the new responses to the DataFrame. You '\n",
      " 'can also use our **`tracely`** library to instrument your app and get traces '\n",
      " 'as a tabular dataset. Check the tutorial with [tracing '\n",
      " 'workflow](/quickstart_tracing).\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " '## 5. Design the Test suite\\n'\n",
      " '\\n'\n",
      " 'To compare new answers with old ones, we need evaluation metrics. You can '\n",
      " 'use deterministic or embeddings-based metrics like Semantic Similarity. '\n",
      " 'However, you often need more custom criteria. Using **LLM-as-a-judge** is '\n",
      " 'useful for this, letting you define what to detect.\\n'\n",
      " '\\n'\n",
      " 'Let’s formulate what we want to Tests:\\n'\n",
      " '\\n'\n",
      " '* **Length check**. All new responses must be no longer than 200 symbols.\\n'\n",
      " '\\n'\n",
      " '* **Correctness**. All new responses should not contradict the reference '\n",
      " 'answer.\\n'\n",
      " '\\n'\n",
      " '* **Style**. All new responses should match the style of the reference.\\n'\n",
      " '\\n'\n",
      " \"Text length is easy to check, but for Correctness and Style, we'll write our \"\n",
      " 'custom LLM judges.\\n'\n",
      " '\\n'\n",
      " '### Correctness judge\\n'\n",
      " '\\n'\n",
      " 'We implement the correctness evaluator, using an Evidenty template for '\n",
      " 'binary classification. We ask the LLM to classify each response as \"correct\" '\n",
      " 'or \"incorrect\" based on the `target_response` column and provide reasoning '\n",
      " 'for its decision.\\n'\n",
      " '\\n'\n",
      " '<Note>\\n'\n",
      " 'You can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n'\n",
      " '</Note>\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'correctness = BinaryClassificationPromptTemplate(\\n'\n",
      " '        criteria = \"\"\"An ANSWER is correct when it is the same as the '\n",
      " 'REFERENCE in all facts and details, even if worded differently.\\n'\n",
      " '        The ANSWER is incorrect if it contradicts the REFERENCE, adds '\n",
      " 'additional claims, omits or changes details.\\n'\n",
      " '        REFERENCE:\\n'\n",
      " '        =====\\n'\n",
      " '        {target_response}\\n'\n",
      " '        =====\"\"\",\\n'\n",
      " '        target_category=\"incorrect\",\\n'\n",
      " '        non_target_category=\"correct\",\\n'\n",
      " '        uncertainty=\"unknown\",\\n'\n",
      " '        include_reasoning=True,\\n'\n",
      " '        pre_messages=[(\"system\", \"You are an expert evaluator. You will be '\n",
      " 'given an ANSWER and REFERENCE\")],\\n'\n",
      " '        )\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'We recommend splitting each evaluation criterion into separate judges and '\n",
      " 'using a simple grading scale, like binary classifiers, for better '\n",
      " 'reliability.\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML '\n",
      " 'system you should align with your preferences. We recommend running a couple '\n",
      " 'of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  **Template parameters.** For an explanation of each parameter, check the '\n",
      " '[LLM judge ](/metrics/customize_llm_judge)docs.\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " '### Style judge\\n'\n",
      " '\\n'\n",
      " \"Using a similar approach, we'll create a custom judge for style match: it \"\n",
      " 'should look whether the style (not the contents!) of both responses remains '\n",
      " 'similar.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'style_match = BinaryClassificationPromptTemplate(\\n'\n",
      " '        criteria = \"\"\"An ANSWER is style-matching when it matches the '\n",
      " 'REFERENCE answer in STYLE, even if the meaning is different.\\n'\n",
      " 'The ANSWER is style-mismatched when it diverges from the REFERENCE answer in '\n",
      " 'STYLE, even if the meaning is the same.\\n'\n",
      " '\\n'\n",
      " 'Consider the following STYLE attributes:\\n'\n",
      " '- tone (friendly, formal, casual, sarcastic, etc.)\\n'\n",
      " '- sentence structure (simple, compound, complex, etc.)\\n'\n",
      " '- verbosity level (relative length of answers)\\n'\n",
      " '- and other similar attributes that may reflect difference in STYLE.\\n'\n",
      " '\\n'\n",
      " 'You must focus only on STYLE. Ignore any differences in contents.\\n'\n",
      " '\\n'\n",
      " '=====\\n'\n",
      " '{target_response}\\n'\n",
      " '=====\"\"\",\\n'\n",
      " '        target_category=\"style-mismatched\",\\n'\n",
      " '        non_target_category=\"style-matching\",\\n'\n",
      " '        uncertainty=\"unknown\",\\n'\n",
      " '        include_reasoning=True,\\n'\n",
      " '        pre_messages=[(\"system\", \"You are an expert evaluator. You will be '\n",
      " 'given an ANSWER and REFERENCE\")],\\n'\n",
      " '        )\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'This could be useful to detect more subtle changes, like LLM becoming '\n",
      " 'suddenly more verbose.\\n'\n",
      " '\\n'\n",
      " 'At the same time, these types of checks are much more subjective and we can '\n",
      " 'expect some variability in the judge responses, so we can treat this test as '\n",
      " '\"non-critical\".\\n'\n",
      " '\\n'\n",
      " '## 6. Run the evaluation\\n'\n",
      " '\\n'\n",
      " 'Now, we can run tests that evaluate for correctness, style and text length. '\n",
      " 'We do this in two steps.\\n'\n",
      " '\\n'\n",
      " '**Score the data**. First, we define the row-level '\n",
      " '[descriptors](/docs/library/descriptors) we want to add. They will process '\n",
      " 'each individual response and add the score/label to the dataset.\\n'\n",
      " '\\n'\n",
      " \"We'll include the two evaluators we just created, and built-in \"\n",
      " '`TextLength()` descriptor.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'descriptors=[LLMEval(\"response\",\\n'\n",
      " '            template=correctness,\\n'\n",
      " '            provider = \"openai\",\\n'\n",
      " '            model = \"gpt-4o-mini\",\\n'\n",
      " '            alias=\"Correctness\",\\n'\n",
      " '            additional_columns={\"target_response\": \"target_response\"}),\\n'\n",
      " '     LLMEval(\"response\",\\n'\n",
      " '            template=style_match,\\n'\n",
      " '            provider = \"openai\",\\n'\n",
      " '            model = \"gpt-4o-mini\",\\n'\n",
      " '            alias=\"Style\",\\n'\n",
      " '            additional_columns={\"target_response\": \"target_response\"}),\\n'\n",
      " '    TextLength(\"response\", alias=\"Length\")]\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  **Understand Descriptors**. See the list of other built-in '\n",
      " '[descriptors](/metrics/all_descriptors).\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " 'To add these descriptors to the dataset, run:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'eval_dataset.add_descriptors(descriptors=descriptors)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'To preview the results of this step locally:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'eval_dataset.as_dataframe()\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_scored-min.png)\\n'\n",
      " '\\n'\n",
      " 'However, simply looking at the dataset is not very useful: we need to '\n",
      " 'summarize the results and assess if the results are up to the mark. For '\n",
      " 'that, we need a Report with the added tests.\\n'\n",
      " '\\n'\n",
      " \"**Create a Report**. Let's formulate the Report:\\n\"\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'report = Report([\\n'\n",
      " '    TextEvals(),\\n'\n",
      " '    MaxValue(column=\"Length\", tests=[lte(200)]),\\n'\n",
      " '    CategoryCount(column=\"Correctness\", category=\"incorrect\", '\n",
      " 'tests=[eq(0)]),\\n'\n",
      " '    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, '\n",
      " 'is_critical=False)]),\\n'\n",
      " '])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'What happens in this code:\\n'\n",
      " '\\n'\n",
      " '* We create an Evidently Report to compute aggregate Metrics.\\n'\n",
      " '\\n'\n",
      " '* We use `TextEvals` to summarize all descriptors.\\n'\n",
      " '\\n'\n",
      " '* We also add Tests for specific values we want to validate. You add Tests '\n",
      " 'by picking a metric you want to assess, and adding a condition to it. (See '\n",
      " '[available Metrics](/metrics/all_metrics)).\\n'\n",
      " '\\n'\n",
      " '* To set test conditions, you define the expectations using parameters like '\n",
      " '`gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test '\n",
      " 'docs](/docs/library/tests)).\\n'\n",
      " '\\n'\n",
      " '* We also label one of the tests (style match) as non-critical. This means '\n",
      " 'it will trigger warning instead of a fail, and will be visually labeled '\n",
      " 'yellow in the Report and the monitoring panel.\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  If you want to test share instead of count, use `share_tests` instead of '\n",
      " '`tests`.\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " '**Run the Report**. Now that our Report with its test conditions is ready - '\n",
      " \"let's run it! We will apply it to the `eval_dataset` that we prepared \"\n",
      " 'earlier, and send it to the Evidently Cloud.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'my_eval = report.run(eval_dataset, None)\\n'\n",
      " 'ws.add_run(project.id, my_eval, include_data=True)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " \"Including data is optional but useful for most LLM use cases since you'd \"\n",
      " 'want to see not just the aggregate results but also the raw texts outputs.\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  You can preview the results in your Python notebook: call `my_eval` or '\n",
      " '`my_eval.json()`.&#x20;\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " 'To view the results, navigate to the Evidently Platform. Go to the [Home '\n",
      " 'Page](https://app.evidently.cloud/), enter your Project, and find the '\n",
      " \"Reports section in the left menu. Here, you'll see the Report you can \"\n",
      " 'explore.\\n'\n",
      " '\\n'\n",
      " 'The Report will have two sections. Metrics show a summary or all values, and '\n",
      " 'Tests will show the pass/fail results in the next tab. You will also see the '\n",
      " 'Dataset with added scores and explanations.\\n'\n",
      " '\\n'\n",
      " 'Report view, with \"Style\" metric selected:\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_report1-min.png)\\n'\n",
      " '\\n'\n",
      " '**Note**: your explanations will vary since LLMs are non-deterministic.\\n'\n",
      " '\\n'\n",
      " 'The Test Suite with all Test results:&#x20;\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n'\n",
      " '\\n'\n",
      " 'You can see that we failed the Length check. To find the failed output, you '\n",
      " 'can sort the column \"Length\" in order and find the longest response.\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  **Using Tags**. You can optionally attach Tags to your Reports to '\n",
      " 'associate this specific run with some parameter, like a prompt version. '\n",
      " 'Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " '## 7. Test again\\n'\n",
      " '\\n'\n",
      " \"Let's say you made yet another change to the prompt. Our reference dataset \"\n",
      " 'stays the same, but we generate a new set of answers that we want to compare '\n",
      " 'to this reference.\\n'\n",
      " '\\n'\n",
      " 'Here is the toy `eval_data_2` to imitate the result of the change.\\n'\n",
      " '\\n'\n",
      " '<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n'\n",
      " '  ```python\\n'\n",
      " '  data = [\\n'\n",
      " '      [\"Why is the sky blue?\",\\n'\n",
      " '       \"The sky is blue because molecules in the air scatter blue light from '\n",
      " 'the sun more than they scatter red light.\",\\n'\n",
      " '       \"The sky looks blue because air molecules scatter the blue light from '\n",
      " 'the sun more effectively than other colors.\"],\\n'\n",
      " '\\n'\n",
      " '      [\"How do airplanes stay in the air?\",\\n'\n",
      " '       \"Airplanes stay in the air because their wings create lift by forcing '\n",
      " 'air to move faster over the top of the wing than underneath, which creates '\n",
      " 'lower pressure on top.\",\\n'\n",
      " '       \"Airplanes fly by generating lift through the wings, which makes the '\n",
      " 'air move faster above them, lowering the pressure.\"],\\n'\n",
      " '\\n'\n",
      " '      [\"Why do we have seasons?\",\\n'\n",
      " '       \"We have seasons because the Earth is tilted on its axis, which '\n",
      " 'causes different parts of the Earth to receive more or less sunlight '\n",
      " 'throughout the year.\",\\n'\n",
      " '       \"Seasons change because the distance between the Earth and the sun '\n",
      " 'varies throughout the year.\"],  # This response contradicts the reference.\\n'\n",
      " '\\n'\n",
      " '      [\"How do magnets work?\",\\n'\n",
      " '       \"Magnets work because they have a magnetic field that can attract or '\n",
      " 'repel certain metals, like iron, due to the alignment of their atomic '\n",
      " 'particles.\",\\n'\n",
      " '       \"Magnets operate by creating a magnetic field, which interacts with '\n",
      " 'certain metals like iron due to the specific alignment of atomic '\n",
      " 'particles.\"],\\n'\n",
      " '\\n'\n",
      " '      [\"Why does the moon change shape?\",\\n'\n",
      " '       \"The moon changes shape, or goes through phases, because we see '\n",
      " 'different portions of its illuminated half as it orbits the Earth.\",\\n'\n",
      " '       \"The moon\\'s phases occur because we observe varying portions of its '\n",
      " 'lit half as it moves around the Earth.\"]\\n'\n",
      " '  ]\\n'\n",
      " '\\n'\n",
      " '  columns = [\"question\", \"target_response\", \"response\"]\\n'\n",
      " '\\n'\n",
      " '  eval_data_2 = pd.DataFrame(data, columns=columns)\\n'\n",
      " '  ```\\n'\n",
      " '</Accordion>\\n'\n",
      " '\\n'\n",
      " 'Create a new dataset:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'eval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\n'\n",
      " 'data_definition=DataDefinition())\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '**Repeat the same evaluation as before.** Since we already defined the '\n",
      " 'descriptors and Report composition with conditional checks, we only need to '\n",
      " 'apply it to the new data:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'eval_dataset_2.add_descriptors(descriptors=descriptors)\\n'\n",
      " 'my_eval_2 = report.run(eval_dataset_2, None)\\n'\n",
      " 'ws.add_run(project.id, my_eval_2, include_data=True)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '**Explore the new Report.** This time, the response length is within bounds, '\n",
      " 'but one of the responses is incorrect: you can see the explanation of the '\n",
      " 'contradition picked up by the LLM judge.\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n'\n",
      " '\\n'\n",
      " 'There is also a \"softer\" fail for one of the responses that now has a '\n",
      " 'different tone.\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_style-min.png)\\n'\n",
      " '\\n'\n",
      " '## 8. Get a Dashboard\\n'\n",
      " '\\n'\n",
      " 'As you run multiple Reports, you may want to track results in time to see if '\n",
      " 'you are improving. You can configure a Dashboard, both in UI or '\n",
      " 'programmatically.&#x20;\\n'\n",
      " '\\n'\n",
      " \"Let's create a couple of Panels using Dashboards as code approach so that \"\n",
      " \"it's easy to reproduce. The following code will add:\\n\"\n",
      " '\\n'\n",
      " '* A counter panel to show the SUCCESS rate of the latest Test run.\\n'\n",
      " '\\n'\n",
      " '* A test monitoring panel to show all Test results over time.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'project.dashboard.add_panel(\\n'\n",
      " '     DashboardPanelTestSuiteCounter(\\n'\n",
      " '        title=\"Latest Test run\",\\n'\n",
      " '        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n'\n",
      " '        size=WidgetSize.FULL,\\n'\n",
      " '        statuses=[TestStatus.SUCCESS],\\n'\n",
      " '        agg=CounterAgg.LAST,\\n'\n",
      " '    ),\\n'\n",
      " '    tab=\"Tests\"\\n'\n",
      " ')\\n'\n",
      " 'project.dashboard.add_panel(\\n'\n",
      " '    DashboardPanelTestSuite(\\n'\n",
      " '        title=\"Test results\",\\n'\n",
      " '        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n'\n",
      " '        size=WidgetSize.FULL,\\n'\n",
      " '        panel_type=TestSuitePanelType.DETAILED,\\n'\n",
      " '    ),\\n'\n",
      " '    tab=\"Tests\"\\n'\n",
      " ')\\n'\n",
      " 'project.save()\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'When you navigate to the UI, you will now see a Panel which shows a summary '\n",
      " 'of Test results (Success, Failure, and Warning) for each Report we ran. As '\n",
      " 'you add more Tests to the same Project, the Panels will be automatically '\n",
      " 'updated to show new Test results.\\n'\n",
      " '\\n'\n",
      " '![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n'\n",
      " '\\n'\n",
      " 'If you hover over individual Test results, you will able to see the specific '\n",
      " 'Test and conditions. You can click on it to open up the specific underlying '\n",
      " 'Report to explore.\\n'\n",
      " '\\n'\n",
      " '<Info>\\n'\n",
      " '  **Using Dashboards**. You can design and add other Panel types, like '\n",
      " 'simply plotting mean/max values or distributions of scores over time. Check '\n",
      " 'the [docs on Dashboards](/docs/platform/dashboard).\\n'\n",
      " '</Info>\\n'\n",
      " '\\n'\n",
      " \"**What's next?** As you design a similar Test Suite for your use case, you \"\n",
      " 'can integrate it with CI/CD workflows to run on every change. You can also '\n",
      " 'enable alerts to be sent to your email / Slack whenever the Tests fail.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff6c7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 153\n"
     ]
    }
   ],
   "source": [
    "print(f\"Paragraphs: {len(paragraphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9eb41",
   "metadata": {},
   "source": [
    "#### Sections\n",
    "\n",
    "Markdown documents have this structure:\n",
    "\n",
    "```text\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd6dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48545d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 8\n"
     ]
    }
   ],
   "source": [
    "sections = split_markdown_by_level(text, level=2)\n",
    "print(f\"Sections: {len(sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a58abb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently sections: 262\n"
     ]
    }
   ],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "        \n",
    "print(f\"Evidently sections: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b9e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8cca2c0",
   "metadata": {},
   "source": [
    "### 3. Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9462ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# openai_client = OpenAI()\n",
    "\n",
    "\n",
    "# def llm(prompt, model='gpt-4o-mini'):\n",
    "#     messages = [\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "\n",
    "#     response = openai_client.responses.create(\n",
    "#         model='gpt-4o-mini',\n",
    "#         input=messages\n",
    "#     )\n",
    "\n",
    "#     return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d462d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "def llm(prompt, model=\"openai/gpt-oss-20b\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        messages = messages,\n",
    "        model = model,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0ebde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596cf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23f408",
   "metadata": {},
   "source": [
    "The prompt asks the LLM to:\n",
    "\n",
    "- Split the document logically (not just by length)\n",
    "- Make sections self-contained\n",
    "- Use a specific output format that's easy to parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e03fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a679ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78695d50a8b4287b9e3c8725dbe95f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently sections: 124\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs[:10]): # Limiting to first 10 docs for cost control\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    \n",
    "    if len(doc_content) == 0:\n",
    "        continue \n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "        \n",
    "print(f\"Evidently sections: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f0ddf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "## Optional API Reference Folder\n",
      "\n",
      "If you are not looking to build API reference documentation, you can delete this section by removing the `api-reference` folder.\n",
      "\n",
      "================================================================================\n",
      "## Getting Started\n",
      "\n",
      "There are two ways to build API documentation:\n",
      "\n",
      "1. **OpenAPI** – use an OpenAPI specification file.  \n",
      "2. **MDX Components** – use custom MDX components for documentation.\n",
      "\n",
      "For the starter kit, we are using the following OpenAPI specification.\n",
      "\n",
      "================================================================================\n",
      "## API Specification\n",
      "\n",
      "**Plant Store Endpoints**\n",
      "\n",
      "- **Specification File:**  \n",
      "  <https://github.com/mintlify/starter/blob/main/api-reference/openapi.json>\n",
      "\n",
      "  The OpenAPI specification file defines all of the Plant Store API endpoints, request/response schemas, and metadata. You can view or download it directly from the link above.\n",
      "\n",
      "================================================================================\n",
      "## Authentication\n",
      "\n",
      "All API endpoints are authenticated using Bearer tokens. The security configuration is defined in the OpenAPI specification as follows:\n",
      "\n",
      "```json\n",
      "\"security\": [\n",
      "  {\n",
      "    \"bearerAuth\": []\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Clients must include a valid Bearer token in the `Authorization` header to access the endpoints.\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.11 Release\n",
      "\n",
      "**Date**: 2025‑07‑18  \n",
      "**Version**: 0.7.11  \n",
      "\n",
      "Full release notes can be found on [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).  \n",
      "This release introduces several new example notebooks:\n",
      "\n",
      "- **Synthetic data generation** – a code example demonstrating how to generate synthetic datasets for evaluation purposes:  \n",
      "  [datagen.ipynb](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.10 Release and Automated Prompt Optimization\n",
      "\n",
      "**Date**: 2025‑07‑09  \n",
      "**Version**: 0.7.10  \n",
      "\n",
      "Full release notes are available on [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).  \n",
      "\n",
      "**New Feature** – *Automated Prompt Optimization*  \n",
      "- A new automated system that optimizes prompts for LLM judges.  \n",
      "- Blog post detailing the approach: [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\n",
      "\n",
      "**Example notebooks** that showcase prompt optimization for different tasks:\n",
      "\n",
      "- Code review binary LLM judge prompt optimization:  \n",
      "  [code_review_example.ipynb](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)  \n",
      "- Topic multi‑class LLM judge prompt optimization:  \n",
      "  [bookings_example.ipynb](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)  \n",
      "- Tweet generation prompt optimization:  \n",
      "  [tw\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.9 Release\n",
      "\n",
      "**Date**: 2025‑06‑27  \n",
      "**Version**: 0.7.9  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.8 Release\n",
      "\n",
      "**Date**: 2025‑06‑19  \n",
      "**Version**: 0.7.8  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.7 Release\n",
      "\n",
      "**Date**: 2025‑06‑04  \n",
      "**Version**: 0.7.7  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\n",
      "\n",
      "================================================================================\n",
      "## Evidently 0.7.6 Release\n",
      "\n",
      "**Date**: 2025‑05‑25  \n",
      "**Version**: 0.7.6  \n",
      "\n",
      "Full release notes: [GitHub](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in evidently_chunks[:10]:\n",
    "    print(\"=\"*80)\n",
    "    print(chunk['section'][:1000])  # Print first 1000 characters of the section\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251100c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
