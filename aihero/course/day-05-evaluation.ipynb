{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39dc76ab",
   "metadata": {},
   "source": [
    "# Day 5: Evaluation\n",
    "\n",
    "https://docs.google.com/document/d/1laKd7zWd9P6vgM06gIQMX7_EHdaPy43o5qIfz_6-zgE/edit?tab=t.0#heading=h.6f40algdx96s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059ca3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x71224214efc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.ingest import read_repo_data\n",
    "from minsearch import Index\n",
    "\n",
    "# DataTalksClub FAQ\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131c5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='groq:openai/gpt-oss-20b'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09445a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ee890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a quick‑start guide for getting a **Kafka client** up and running in Python.  \n",
      "If you actually need to run a Kafka **broker** on your machine, the steps are a little different (see the note at the end).\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Install the Python Kafka client library\n",
      "\n",
      "There are two popular client libraries:\n",
      "\n",
      "| Library | Typical use case | Install command |\n",
      "|---------|------------------|-----------------|\n",
      "| `confluent-kafka` | High‑performance, native C library (recommended for production) | `pip install confluent-kafka` |\n",
      "| `kafka-python` | Pure‑Python, easier to install in minimal environments | `pip install kafka-python` |\n",
      "\n",
      "> **Tip:**  `confluent-kafka` is the library that the course’s example code uses.  It is fast and is the one shown in the FAQ entry *“Python Kafka: Installing dependencies for python3 06‑streaming/python/avro_example/producer.py”*.\n",
      "\n",
      "### Example\n",
      "\n",
      "```bash\n",
      "# Create (or activate) a virtual environment first\n",
      "python -m venv venv\n",
      "source venv/bin/activate   # Windows: venv\\Scripts\\activate\n",
      "\n",
      "# Install the client\n",
      "pip install confluent-kafka   # or: pip install kafka-python\n",
      "```\n",
      "\n",
      "> If you run into the error `ModuleNotFoundError: No module named 'kafka.vendor.six.moves'`, the FAQ *“How to fix the error 'ModuleNotFoundError: No module named 'kafka.vendor.six.moves'?”* recommends installing the newer `kafka-python-ng` package:\n",
      "\n",
      "```bash\n",
      "pip uninstall kafka-python\n",
      "pip install kafka-python-ng\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Verify the installation\n",
      "\n",
      "```python\n",
      "# test_import.py\n",
      "import confluent_kafka\n",
      "print(\"confluent_kafka is installed and available!\")\n",
      "```\n",
      "\n",
      "Run it:\n",
      "\n",
      "```bash\n",
      "python test_import.py\n",
      "```\n",
      "\n",
      "You should see the message printed with no errors.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. (Optional) Install a local Kafka broker for development\n",
      "\n",
      "If you need a Kafka broker on your laptop, the easiest way is to use Docker:\n",
      "\n",
      "```bash\n",
      "docker run -d \\\n",
      "  --name zookeeper \\\n",
      "  -p 2181:2181 \\\n",
      "  bitnami/zookeeper:latest\n",
      "\n",
      "docker run -d \\\n",
      "  --name kafka \\\n",
      "  -p 9092:9092 \\\n",
      "  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\\n",
      "  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\n",
      "  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n",
      "  bitnami/kafka:latest\n",
      "```\n",
      "\n",
      "Now your Python code can connect to `localhost:9092`.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Quick example (using `confluent-kafka`)\n",
      "\n",
      "```python\n",
      "from confluent_kafka import Producer, Consumer, KafkaError\n",
      "\n",
      "# Produce a single message\n",
      "p = Producer({'bootstrap.servers': 'localhost:9092'})\n",
      "p.produce('my_topic', b'Hello, Kafka!')\n",
      "p.flush()\n",
      "\n",
      "# Consume the message\n",
      "c = Consumer({\n",
      "    'bootstrap.servers': 'localhost:9092',\n",
      "    'group.id': 'my_group',\n",
      "    'auto.offset.reset': 'earliest'\n",
      "})\n",
      "c.subscribe(['my_topic'])\n",
      "msg = c.poll(1.0)\n",
      "if msg is None:\n",
      "    print(\"No message received\")\n",
      "elif msg.error():\n",
      "    print(f\"Error: {msg.error()}\")\n",
      "else:\n",
      "    print(f\"Received: {msg.value().decode('utf-8')}\")\n",
      "c.close()\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Recap\n",
      "\n",
      "1. **Install a client** (`confluent-kafka` or `kafka-python` via `pip`).  \n",
      "2. **Test the import** to make sure it works.  \n",
      "3. If you need a broker, spin up Zookeeper + Kafka with Docker (or install the broker natively).  \n",
      "\n",
      "Happy streaming!\n"
     ]
    }
   ],
   "source": [
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79c902",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "We want to capture the following information from each agent run:\n",
    "\n",
    "- **System prompt** used  \n",
    "- **Model** chosen  \n",
    "- **User query**  \n",
    "- **Tools** invoked  \n",
    "- **LLM ↔ tool interactions** (inputs/outputs)  \n",
    "- **Final response**  \n",
    "\n",
    "---\n",
    "\n",
    "#### Simple Approach\n",
    "- Implement a minimal logging system.  \n",
    "- Save logs as **JSON files** for easy review.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Important\n",
    "- This is **not production-ready**.  \n",
    "- In practice, send logs to a log collection system or use specialized LLM evaluation tools such as:  \n",
    "  - **Evidently**  \n",
    "  - **LangWatch**  \n",
    "  - **Arize Phoenix**  \n",
    "\n",
    "---\n",
    "\n",
    "#### Example JSON Log Structure\n",
    "```json\n",
    "{\n",
    "  \"agent_name\": \"support_agent\",\n",
    "  \"system_prompt\": \"You are a helpful assistant.\",\n",
    "  \"provider\": \"groq\",\n",
    "  \"model\": \"openai/gpt-oss-20b\",\n",
    "  \"tools\": [\"text_search\"],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is scaffolding in education?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Scaffolding in education means providing structured support...\"\n",
    "    }\n",
    "  ],\n",
    "  \"source\": \"user\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20de4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    # Convert internal message format into regular Python dictionaries\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bd4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Custom serializer for datetime objects\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    # Generate a unique filename with timestamp and random hex\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    # Save the completed logs to a JSON file\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc21b830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Yes – you can still earn a certificate even if you join the cohort late.**  \n",
      "\n",
      "The key requirements are:\n",
      "\n",
      "| Requirement | What you need to do |\n",
      "|-------------|---------------------|\n",
      "| **Be part of a live cohort** | You must register for a scheduled cohort (self‑paced tracks do not award certificates). |\n",
      "| **Submit the peer‑reviewed capstone projects on time** | These are the only assignments that are required for certification. As long as you turn in the capstone projects by their deadlines and receive peer reviews, you are eligible. |\n",
      "| **No need to do the homeworks if you join late** | The FAQ specifically states: “You do **not** need to do the homeworks if you join late, for example.” |\n",
      "\n",
      "---\n",
      "\n",
      "### How to get the certificate once you’re in\n",
      "\n",
      "1. **Join the cohort** – sign up via the course enrollment page for the edition you’re interested in (e.g., 2025: https://courses.datatalks.club/de-zoomcamp-2025/enrollment).\n",
      "2. **Complete the capstone(s)** – submit your projects before their deadlines and ensure you receive the required peer reviews.\n",
      "3. **Check the Telegram and course channel announcements** – the instructor will post when:\n",
      "   - Your full name appears correctly on the certificate (you can edit your profile if needed).\n",
      "   - Grading is finished.\n",
      "4. **Generate the PDF** – after the grading announcement, follow the instructions in the repository file `[certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md)` to produce the certificate document yourself.\n",
      "\n",
      "---\n",
      "\n",
      "#### Bottom line\n",
      "\n",
      "- **Late joiners are fine** – just hit the capstone deadlines.  \n",
      "- **Homeworks are optional** for certification (unless you want them for extra practice).  \n",
      "- **Only live cohorts** confer certificates; self‑paced tracks do not.\n",
      "\n",
      "Good luck, and enjoy the course!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_20251002_111315_0a47e6.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple interaction loop\n",
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a54a815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='can I join late and get a certificate?', timestamp=datetime.datetime(2025, 10, 2, 11, 57, 9, 637966, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.'),\n",
       " ModelResponse(parts=[ThinkingPart(content='We need to find relevant info from course materials. Search query: \"join late certificate\".'), ToolCallPart(tool_name='text_search', args='{\"query\":\"join late certificate\"}', tool_call_id='fc_eedb6518-a7b1-4df1-9f97-f043e80d3d38')], usage=RequestUsage(input_tokens=298, output_tokens=44), model_name='openai/gpt-oss-20b', timestamp=datetime.datetime(2025, 10, 2, 11, 57, 9, tzinfo=TzInfo(UTC)), provider_name='groq', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-52d1aa88-98aa-4abb-a699-bbf60eae62aa', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '3774a79c13', 'question': 'Certificate: Do I need to do the homeworks to get the certificate?', 'sort_order': 14, 'content': 'No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}, {'id': '6314bc3029', 'images': [{'description': 'image #1', 'id': 'image_1', 'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}], 'question': 'How do I get my certificate?', 'sort_order': 46, 'content': 'There\\'ll be an announcement in Telegram and the course channel for:\\n\\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\\n- Notifying when the grading is completed.\\n\\nYou will find it in your course profile (you need to be\\nlogged it). \\n\\nFor 2025 the link to the course profile is this:\\n\\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\\n\\nFor other editions, change \"2025\" to your edition.\\n\\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md'}, {'id': '4dbd2eea47', 'question': 'Homework: Are late submissions of homework allowed?', 'sort_order': 17, 'content': 'No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md'}, {'id': '542abbcb6a', 'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)', 'sort_order': 6, 'content': 'Firstly, make sure that you add `!` before `wget` if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of these two things (from CLI):\\n\\n- **Using the Python library wget installed with pip:**\\n\\n  ```bash\\n  python -m wget <url>\\n  ```\\n\\n- **Use the usual command and add `--no-check-certificate` at the end:**\\n\\n  ```bash\\n  !wget <website_url> --no-check-certificate\\n  ```', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/006_542abbcb6a_wget-error-cannot-verify-website-certificate-macos.md'}], tool_call_id='fc_eedb6518-a7b1-4df1-9f97-f043e80d3d38', timestamp=datetime.datetime(2025, 10, 2, 11, 57, 9, 919476, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.'),\n",
       " ModelResponse(parts=[ThinkingPart(content='We need to answer user question: \"can I join late and get a certificate?\" We found relevant FAQ: \"Certificate: Do I need to do the homeworks to get the certificate?\" which states \"No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.\" Also there is a question about \"Can I follow the course in a self-paced mode and get a certificate?\" but the user asks about joining late. So we can answer: yes, you can join late, as long as you complete capstone on time. Provide citation. Also might mention you need to join the live cohort, not self-paced. Provide reference. Provide instructions.\\n\\nLet\\'s cite the found search result. The link: \"https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\"\\n\\nWe should format link: [LINK TITLE](FULL_GITHUB_LINK). The link title could be something like \"Certificate: Do I need to do the homeworks to get the certificate?\" Provide.\\n\\nLet\\'s answer.'), TextPart(content='You **can** join a cohort a bit later and still receive a certificate, as long as you finish the required **peer‑reviewed capstone projects on time**.  \\nThe FAQ makes it clear that homeworks are *not* mandatory for the certificate, so a late enrollee who completes the capstone(s) within the scheduled window will get the same certificate as earlier participants.  \\n\\n> **Citation**: [Certificate: Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)  \\n\\nJust make sure you stay in the live cohort (self‑paced mode does not award certificates) and keep track of the capstone deadlines. If you finish everything on schedule, the certificate will be issued just the same.')], usage=RequestUsage(input_tokens=1126, output_tokens=461), model_name='openai/gpt-oss-20b', timestamp=datetime.datetime(2025, 10, 2, 11, 57, 10, tzinfo=TzInfo(UTC)), provider_name='groq', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-32c753b8-f68d-44d9-b8a1-4a60af0eaac8', finish_reason='stop')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf02c0",
   "metadata": {},
   "source": [
    "### Adding References\n",
    "\n",
    "Add references to the original documents in the agent’s responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4181bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='groq:openai/gpt-oss-20b'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9621e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can indeed join a cohort late and still earn a certificate—provided you finish the **peer‑reviewed capstone projects on time**.  \n",
      "The course does not require you to complete the homework assignments if you’re a late‑arriving student. Just make sure you stay on top of the capstone deadlines and participate in the required peer reviews while the cohort is running.\n",
      "\n",
      "> **Reference**:  \n",
      "> *[Certificate: Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_v2_20251002_115904_1cd9e6.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"can I join late and get a certificate?\"\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7efa74ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='can I join late and get a certificate?', timestamp=datetime.datetime(2025, 10, 2, 11, 59, 3, 812232, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.'),\n",
       " ModelResponse(parts=[ThinkingPart(content='The user asks: \"can I join late and get a certificate?\" They likely refer to a course. We need to search relevant information from the course materials. Let\\'s search.'), ToolCallPart(tool_name='text_search', args='{\"query\":\"join late certificate course\"}', tool_call_id='fc_e7f00dd1-c1a2-4f76-a56d-fee881676d2b')], usage=RequestUsage(input_tokens=298, output_tokens=62), model_name='openai/gpt-oss-20b', timestamp=datetime.datetime(2025, 10, 2, 11, 59, 4, tzinfo=TzInfo(UTC)), provider_name='groq', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-6dd6f1e2-71f7-4dde-b944-6b60392877d9', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '3774a79c13', 'question': 'Certificate: Do I need to do the homeworks to get the certificate?', 'sort_order': 14, 'content': 'No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md'}, {'id': '900f60fd25', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'sort_order': 15, 'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}, {'id': '6314bc3029', 'images': [{'description': 'image #1', 'id': 'image_1', 'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}], 'question': 'How do I get my certificate?', 'sort_order': 46, 'content': 'There\\'ll be an announcement in Telegram and the course channel for:\\n\\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\\n- Notifying when the grading is completed.\\n\\nYou will find it in your course profile (you need to be\\nlogged it). \\n\\nFor 2025 the link to the course profile is this:\\n\\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\\n\\nFor other editions, change \"2025\" to your edition.\\n\\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '4dbd2eea47', 'question': 'Homework: Are late submissions of homework allowed?', 'sort_order': 17, 'content': 'No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md'}], tool_call_id='fc_e7f00dd1-c1a2-4f76-a56d-fee881676d2b', timestamp=datetime.datetime(2025, 10, 2, 11, 59, 4, 137602, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.'),\n",
       " ModelResponse(parts=[ThinkingPart(content='We found relevant FAQ. User asked: \"can I join late and get a certificate?\" This relates to certificate and joining late. We saw answer: \"No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.\" That is about certificate for late joiners. So answer: yes you can join late as long as you finish capstone on time. Provide reference. Also note you must join live cohort, not self-paced. Provide info. Use link.'), TextPart(content='You can indeed join a cohort late and still earn a certificate—provided you finish the **peer‑reviewed capstone projects on time**.  \\nThe course does not require you to complete the homework assignments if you’re a late‑arriving student. Just make sure you stay on top of the capstone deadlines and participate in the required peer reviews while the cohort is running.\\n\\n> **Reference**:  \\n> *[Certificate: Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)*')], usage=RequestUsage(input_tokens=1110, output_tokens=277), model_name='openai/gpt-oss-20b', timestamp=datetime.datetime(2025, 10, 2, 11, 59, 4, tzinfo=TzInfo(UTC)), provider_name='groq', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-b22eda80-d53b-4d21-8513-c66e80cb71d0', finish_reason='stop')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581feee6",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "- **Vibe Check**:  \n",
    "  - A quick manual review to get a sense of how well the model is performing.  \n",
    "  - Collect ~10–20 examples for inspection.  \n",
    "  - Helps identify edge cases and define future evaluation criteria.  \n",
    "\n",
    "- **LLM as a Judge**:  \n",
    "  - Use one LLM to evaluate another’s outputs.  \n",
    "  - Checks can include:  \n",
    "    - Following instructions  \n",
    "    - Answer quality/relevance  \n",
    "    - References included  \n",
    "    - Proper tool usage  \n",
    "  - Automates evaluation once criteria are defined.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: The agent invoked search tool (in <LOG>)\n",
    "\n",
    "Only fill true/false and justification. Do not include any explanation or extra fields.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc19c48",
   "metadata": {},
   "source": [
    "#### Structured Output\n",
    "\n",
    "- Use structured output when expecting a well-defined response format.  \n",
    "- Define a **Pydantic class** with the expected schema.  \n",
    "- The LLM will generate output that matches the schema exactly, ensuring consistency and easier parsing.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64933a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bfad85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str = Field(description=\"The name of the check from the provided checklist.\")\n",
    "    justification: str = Field(description=\"A brief explanation for the check_pass result.\")\n",
    "    check_pass: bool = Field(description=\"True if the condition is met, False otherwise.\")\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str = Field(description=\"A concise final summary of the agent's performance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc46ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation agent with structured output\n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='groq:llama-3.3-70b-versatile', # Use the model that can handle structured output\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a2cda88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format for the evaluation agent - use XML-like tags for better understanding\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "cebad1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load log file\n",
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c7b267ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: You are a helpful assistant for a course.  \n",
      "\n",
      "Use the search tool to find relevant information from the course materials before answering questions.  \n",
      "\n",
      "If you can find specific information through search, use it to provide accurate answers.\n",
      "\n",
      "Always include references by citing the filename of the source material you used.  \n",
      "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
      "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
      "\n",
      "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
      "Question: can I join late and get a certificate?\n",
      "Answer: You can indeed join a cohort late and still earn a certificate—provided you finish the **peer‑reviewed capstone projects on time**.  \n",
      "The course does not require you to complete the homework assignments if you’re a late‑arriving student. Just make sure you stay on top of the capstone deadlines and participate in the required peer reviews while the cohort is running.\n",
      "\n",
      "> **Reference**:  \n",
      "> *[Certificate: Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)*\n",
      "Log: [{\"parts\": [{\"content\": \"can I join late and get a certificate?\", \"timestamp\": \"2025-10-02T11:59:03.812232+00:00\", \"part_kind\": \"user-prompt\"}], \"instructions\": \"You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen c ...\n"
     ]
    }
   ],
   "source": [
    "# Loads a saved interaction log\n",
    "log_record = load_log_file('./logs/faq_agent_v2_20251002_115904_1cd9e6.json')\n",
    "\n",
    "# Extracts the key components (instructions, question, answer, full log)\n",
    "instructions = log_record['system_prompt'][0]\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][-1]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "print(\"Instructions:\", instructions)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Log:\", log[:500], \"...\")  # Print only the first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7375115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt: <INSTRUCTIONS>You are a helpful assistant for a course.  \n",
      "\n",
      "Use the search tool to find relevant information from the course materials before answering questions.  \n",
      "\n",
      "If you can find specific information through search, use it to provide accurate answers.\n",
      "\n",
      "Always include references by citing the filename of the source material you used.  \n",
      "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
      "Format: [LINK TITLE](F ...\n"
     ]
    }
   ],
   "source": [
    "# Formats them into the evaluation prompt\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n",
    "\n",
    "print(\"User Prompt:\", user_prompt[:500], \"...\")  # Print only the first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c2bcd460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assistant provided a clear and accurate answer with proper citations, followed the instructions, and used the search tool effectively.\n",
      "check_name='instructions_follow' justification='The assistant followed the instructions by searching for relevant information and providing a reference.' check_pass=True\n",
      "check_name='instructions_avoid' justification='The assistant avoided doing things it was told not to do.' check_pass=True\n",
      "check_name='answer_relevant' justification=\"The response directly addresses the user's question.\" check_pass=True\n",
      "check_name='answer_clear' justification='The answer is clear and correct.' check_pass=True\n",
      "check_name='answer_citations' justification='The response includes proper citations and sources.' check_pass=True\n",
      "check_name='completeness' justification='The response is complete and covers all key aspects of the request.' check_pass=True\n",
      "check_name='tool_call_search' justification='The agent invoked the search tool.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "# Runs the evaluation agent\n",
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca3690",
   "metadata": {},
   "source": [
    "#### Reduce prompt verbosity: \n",
    "\n",
    "Keep the prompt concise by including only the necessary context, rather than the entire conversation log.\n",
    "\n",
    "- remove timestamps and IDs that aren't needed for evaluation\n",
    "- remove thinking part that are not necessary\n",
    "- replace actual search results with a placeholder\n",
    "- keep only the essential structure\n",
    "\n",
    "This reduces the number of tokens we send to the evaluation model, which lowers the costs and speeds up evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bcd5b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "            \n",
    "            if kind == 'thinking':\n",
    "                part['content'] = 'THINKING_REDACTED'\n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "046f86b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"kind\": \"request\",\n",
      "    \"parts\": [\n",
      "      {\n",
      "        \"content\": \"can I join late and get a certificate?\",\n",
      "        \"part_kind\": \"user-prompt\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"kind\": \"response\",\n",
      "    \"parts\": [\n",
      "      {\n",
      "        \"content\": \"THINKING_REDACTED\",\n",
      "        \"id\": null,\n",
      "        \"signature\": null,\n",
      "        \"provider_name\": null,\n",
      "        \"part_kind\": \"thinking\"\n",
      "      },\n",
      "      {\n",
      "        \"tool_name\": \"text_search\",\n",
      "        \"args\": \"{\\\"query\\\":\\\"join late certificate course\\\"}\",\n",
      "        \"part_kind\": \"tool-call\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"kind\": \"request\",\n",
      "    \"parts\": [\n",
      "      {\n",
      "        \"tool_name\": \"text_search\",\n",
      "        \"content\": \"RETURN_RESULTS_REDACTED\",\n",
      "        \"part_kind\": \"tool-return\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"kind\": \"response\",\n",
      "    \"parts\": [\n",
      "      {\n",
      "        \"content\": \"THINKING_REDACTED\",\n",
      "        \"id\": null,\n",
      "        \"signature\": null,\n",
      "        \"provider_name\": null,\n",
      "        \"part_kind\": \"thinking\"\n",
      "      },\n",
      "      {\n",
      "        \"content\": \"You can indeed join a cohort late and still earn a certificate\\u2014provided you finish the **peer\\u2011reviewed capstone projects on time**.  \\nThe course does not require you to complete the homework assignments if you\\u2019re a late\\u2011arriving student. Just make sure you stay on top of the capstone deadlines and participate in the required peer reviews while the cohort is running.\\n\\n> **Reference**:  \\n> *[Certificate: Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)*\",\n",
      "        \"part_kind\": \"text\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "simple_log = simplify_log_messages(log_record['messages'])\n",
    "print(json.dumps(simple_log, indent=2))  # Print only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "395cc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][-1]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output \n",
    "\n",
    "\n",
    "log_record = load_log_file('./logs/faq_agent_v2_20251002_115904_1cd9e6.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cc2060ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='Used the search tool and gave a reference, but the citation still contains the placeholder \"faq-main\" instead of the full repository path as instructed.', check_pass=False), EvaluationCheck(check_name='instructions_avoid', justification='No prohibited actions were taken.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly answers the user's question about joining late and obtaining a certificate.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is concise, understandable, and accurately reflects the policy.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='Citation format is incorrect; it still includes \"faq-main\" instead of the full path replacement required.', check_pass=False), EvaluationCheck(check_name='completeness', justification='All key points (late join, certificate eligibility, capstone requirement, homework exemption) are covered.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The log shows a text_search tool call was made.', check_pass=True)] summary='The assistant answered the question clearly and completely, used the search tool, and provided a citation, but the citation does not fully comply with the required formatting, leading to failures on instruction adherence and citation quality.'\n"
     ]
    }
   ],
   "source": [
    "print(eval1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ecd0f",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "- Use AI to generate additional questions from sample records in the database.  \n",
    "- For each record:  \n",
    "  1. Ask an LLM to create a question based on the record.  \n",
    "  2. Use the generated question as input to the agent and log the response.  \n",
    "- Current approach is simple; more advanced methods could also track the source file for later verification.  \n",
    "- Adjust prompts according to your specific project or use case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a93ff236",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='groq:openai/gpt-oss-20b',\n",
    "    output_type=QuestionsList\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0ffee5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Randomly sample 10 records from the FAQ data\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef46980c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For example, when running `JsonConsumer.java`, you might see:\\n\\n```\\nConsuming form kafka started\\n\\nRESULTS:::0\\n\\nRESULTS:::0\\n\\nRESULTS:::0\\n```\\n\\nOr when running `JsonProducer.java`, you might encounter:\\n\\n```\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\n```\\n\\n**Solution:**\\n\\n1. Ensure the `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` in the scripts located at `src/main/java/org/example/` (e.g., `JsonConsumer.java`, `JsonProducer.java`) is pointing to the correct server URL (e.g., `europe-west3` vs `europe-west2`).\\n\\n2. Verify that the cluster key and secrets are updated in `src/main/java/org/example/Secrets.java` (`KAFKA_CLUSTER_KEY` and `KAFKA_CLUSTER_SECRET`).',\n",
       " 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\n\\nYou can also open any GitHub repository in a GitHub Codespace.',\n",
       " 'Before you can develop some data model on dbt, you should:\\n\\n1. **Create a Development Environment:** Ensure that your development environment is properly configured.\\n   \\n2. **Set Parameters:** Specify necessary parameters within the environment.\\n   \\nOnce the model has been developed, also create a deployment environment to create and run jobs.',\n",
       " 'Running out of storage while trying to backfill. I realized my GCP VM only has 30GB of storage and I was using it up quickly. Here are a couple of suggestions for managing storage:\\n\\n- **Clean up your GCP VM drive:** Use the command below to identify what is taking up the most space:\\n\\n  ```bash\\n  sudo du -sh *\\n  ```\\n\\n  - **(~1GB)** For me, the Anaconda installer was consuming a lot of space. If you no longer need it, you can delete it:\\n  \\n    ```bash\\n    rm -rf <anacondainstaller_fpath>\\n    ```\\n\\n  - **(~3GB)** Anaconda itself takes up a lot of space. You can’t delete it entirely if you need Python, but you can clean it up significantly:\\n    \\n    ```bash\\n    conda clean --all -y\\n    ```\\n\\n- **Clean up your Kestra files:** Use a purge flow. You can find a generic example here:\\n  \\n  [https://kestra.io/docs/administrator-guide/purge](https://kestra.io/docs/administrator-guide/purge)\\n  \\n  I wanted to perform the cleanup immediately, rather than waiting until the end of the month, so I adjusted the `endDate` to `\"{{ now() }}\"` and removed the trigger block. You can also choose whether to remove FAILED state executions.\\n\\n- **Clean up your PostgreSQL database:** You can manually delete tables in pgAdmin, or set up a workflow in Kestra for it. I found it easy to do manually.',\n",
       " 'The issue was related to network restrictions, as Google is not accessible in my country. I used a VPN and discovered that the terminal program does not automatically follow the system proxy, requiring separate proxy configuration settings.\\n\\n**Solution:**\\n\\n1. Open an Enhanced Mode in your VPN application, such as Clash.\\n2. Run `terraform apply` again.\\n\\nIf you encounter this issue, consult your VPN provider for assistance with configuration.',\n",
       " 'In [this video](https://www.youtube.com/watch?v=B1WwATwf-vY&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb), the data file is stored as `output.csv`. If the file extension is `csv.gz` instead of `csv`, it won\\'t store correctly.\\n\\nTo handle this:\\n\\n1. Replace `csv_name = \"output.csv\"` with the file name extracted from the URL. For example, for the yellow taxi data, use:\\n   \\n   ```python\\n   url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\\n   csv_name = url.split(\"/\")[-1]\\n   ```\\n\\n2. When you use `csv_name` with `pandas.read_csv`, it will work correctly because `pandas.read_csv` can directly read files with the `csv.gz` extension.\\n\\nExample:\\n\\n```python\\nimport pandas as pd\\n\\nurl = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\\ncsv_name = url.split(\"/\")[-1]\\n\\ndata = pd.read_csv(csv_name)\\n```',\n",
       " 'If your Bash prompt is stuck on the password command for postgres:\\n\\n<{IMAGE:image_1}>\\n\\nUse `winpty`:\\n\\n```bash\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\n```\\n\\nAlternatively, try using Windows Terminal or the terminal in VS Code.',\n",
       " \"Make sure that the port number is set as an integer in your `profiles.yml` file. Environment variables are usually strings, so you need to explicitly convert them to integers in Jinja. Update the line that sets the port with something like:\\n\\n```yaml\\nport: {{ env_var('DB_PORT') | int }}\\n```\\n\\nThis will ensure that the value is treated as an integer.\",\n",
       " 'Get the network name via:\\n\\n```bash\\ndocker network ls\\n```\\n\\nFor more details, refer to the [Docker network ls documentation](https://docs.docker.com/engine/reference/commandline/network_ls/).',\n",
       " 'When using the command `\\\\d <database name>` you get the error `column c.relhasoids does not exist`.\\n\\nResolution:\\n\\n1. Uninstall pgcli.\\n2. Reinstall pgcli.\\n3. Restart your PC.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20b3398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "426f5e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I keep seeing 'RESULTS:::0' when I run JsonConsumer.java, and I get a SaslAuthenticationException with JsonProducer.java—what steps should I take to correctly set up the bootstrap server and cluster credentials?\",\n",
       " 'How do I launch a GitHub Codespace for my data engineering project, and what pre-installed tools (like Docker, Python, etc.) are available in that environment?',\n",
       " 'Before I can develop a dbt data model, what configuration steps do I need to perform, and how do I set up both a development and a deployment environment to run the jobs?',\n",
       " 'My GCP VM is filling up while backfilling data. What practical methods can I use to free up disk space, especially regarding Anaconda, Kestra files, and PostgreSQL data?',\n",
       " 'I’m behind a network restriction that blocks Google, and my Terraform runs fail. How can I configure my VPN or proxy settings so that the terminal program respects the system proxy?',\n",
       " 'The NYC taxi data I download is a .csv.gz file, but my script expects a .csv file. How can I modify my code to correctly handle and read the compressed CSV?',\n",
       " 'On Windows, pgcli prompts me for a password and then hangs. What is winpty and how can I use it to interact with PostgreSQL from the terminal?',\n",
       " \"My dbt profile's port is being read as a string, causing issues. How can I modify profiles.yml to ensure the port is treated as an integer using Jinja?\",\n",
       " 'I need to troubleshoot container networking. What command should I run to list all Docker networks, and how can I interpret the output?',\n",
       " 'When I run `\\\\d <database name>` in pgcli I get an error about `c.relhasoids` not existing. What causes this error and how can I resolve it by reinstalling pgcli?']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85708298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ec41ea82e043d69ec5a9bd16d4ef9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I keep seeing 'RESULTS:::0' when I run JsonConsumer.java, and I get a SaslAuthenticationException with JsonProducer.java—what steps should I take to correctly set up the bootstrap server and cluster credentials?\n",
      "The two symptoms you’re seeing are both caused by an incorrect Kafka connection configuration.\n",
      "\n",
      "| Symptom | Likely Cause | Fix |\n",
      "|---------|--------------|-----|\n",
      "| `JsonConsumer.java` prints `RESULTS:::0` repeatedly | Consumer is connecting to the wrong broker (or no broker). | • Open `src/main/java/org/example/JsonConsumer.java` (and `JsonProducer.java`).<br>• Verify that `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` points to the **correct** broker URL (e.g., `europe-west3` if that’s where your cluster lives). |\n",
      "| `JsonProducer.java` throws `SaslAuthenticationException: Authentication failed` | The SASL credentials (key/secret) used by the producer don’t match the cluster’s credentials. | • Open `src/main/java/org/example/Secrets.java`.<br>• Update `KAFKA_CLUSTER_KEY` and `KAFKA_CLUSTER_SECRET` with the values from the Kafka console (or from the cluster’s settings in Google Cloud / Confluent Cloud, etc.). |\n",
      "\n",
      "**Quick checklist**\n",
      "\n",
      "1. **Bootstrap server**  \n",
      "   ```java\n",
      "   props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,\n",
      "             \"your-cluster-region-your-cluster-id.svc.cluster.local:9092\");\n",
      "   ```\n",
      "   (Make sure you’re using the region that the cluster is actually deployed in – `europe-west3` vs `europe-west2` can change the endpoint.)\n",
      "\n",
      "2. **SASL credentials**  \n",
      "   ```java\n",
      "   props.put(\"sasl.jaas.config\",\n",
      "             \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"\" + KAFKA_CLUSTER_KEY + \"\\\" password=\\\"\" + KAFKA_CLUSTER_SECRET + \"\\\";\");\n",
      "   props.put(\"security.protocol\", \"SASL_SSL\");\n",
      "   props.put(\"sasl.mechanism\", \"PLAIN\");\n",
      "   ```\n",
      "\n",
      "3. **Re‑build and run**  \n",
      "   ```bash\n",
      "   ./gradlew clean build\n",
      "   java -cp build/libs/<your-jar>.jar:out src/main/java/org/example/JsonProducer.java\n",
      "   java -cp build/libs/<your-jar>.jar:out src/main/java/org/example/JsonConsumer.java\n",
      "   ```\n",
      "\n",
      "Once the bootstrap URL and credentials match the cluster’s actual settings, the consumer will start pulling messages (`RESULTS:::` will show the real count) and the producer will send data without the SASL error.\n",
      "\n",
      "*Reference:*  \n",
      "[Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-6/024_cd8a62fc55_java-kafka-when-running-the-producerconsumeretc-ja.md)\n",
      "\n",
      "How do I launch a GitHub Codespace for my data engineering project, and what pre-installed tools (like Docker, Python, etc.) are available in that environment?\n",
      "**Launching a GitHub Codespace**\n",
      "\n",
      "1. **Open your repository on GitHub**  \n",
      "   - Navigate to the repo that contains your data‑engineering project.  \n",
      "\n",
      "2. **Start a new Codespace**  \n",
      "   - Click the **Codespaces** button (the “+” icon) near the top right of the repo page.  \n",
      "   - Choose **New codespace**.  \n",
      "   - Pick the branch you want to work on (or create a new one).  \n",
      "   - GitHub will provision a fresh Linux VM and open the project in the web‑based VS Code editor.  \n",
      "\n",
      "   *Alternatively, you can use the CLI:*  \n",
      "   ```bash\n",
      "   gh codespace create --repo user/repo --branch main\n",
      "   ```  \n",
      "   (requires the GitHub CLI and that you have Codespaces permissions.)\n",
      "\n",
      "3. **Connect to the Codespace**  \n",
      "   - Once the provisioning finishes, you’ll see the Codespace running in a new browser tab (or you can connect via the VS Code desktop app).  \n",
      "   - The terminal inside Codespace is already authenticated to your GitHub account and has network access to the rest of the world.\n",
      "\n",
      "**Pre‑installed tools in a Codespace**\n",
      "\n",
      "GitHub Codespaces is built on top of a lightweight Linux container that comes with a curated set of data‑science / engineering utilities. The most common ones you’ll see are:\n",
      "\n",
      "| Tool | Typical version | What you can do |\n",
      "|------|-----------------|-----------------|\n",
      "| **Docker** | Latest stable (e.g., 20.x) | Run containers, build images, manage services. |\n",
      "| **Docker Compose** | Latest stable | Compose multi‑container Docker apps. |\n",
      "| **Python** | 3.10+ (plus `pip` & `venv`) | Run scripts, install packages, create virtual environments. |\n",
      "| **Node.js** | 18+ | Run JavaScript/TypeScript tooling, npm/yarn. |\n",
      "| **Git** | 2.x | Clone, fetch, push, etc. |\n",
      "| **VS Code extensions** | Pre‑installed set | Python, Docker, GitLens, Jupyter, etc. |\n",
      "| **Jupyter Notebook & JupyterLab** | 3.x | Interactive notebooks. |\n",
      "| **tmux, htop, curl, wget** | Standard Linux tools | Terminal multiplexing, system monitoring, HTTP utilities. |\n",
      "| **bash / zsh** | Standard | Shell scripting. |\n",
      "| **PostgreSQL client (`psql`)** | Included via Docker | Connect to Postgres containers or remote DBs. |\n",
      "| **R, RStudio Server** | Optional (via extensions) | For R‑based workflows. |\n",
      "\n",
      "> **Source**  \n",
      "> “Environment – Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?” – *faq-main/_questions/data‑engineering‑zoomcamp/general/023_5b4fb0c0a8_environment-is-github-codespaces-an-alternative-to.md*  \n",
      "> [Environment](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/023_5b4fb0c0a8_environment-is-github-codespaces-an-alternative-to.md)\n",
      "\n",
      "These tools are ready to use out of the box, so you can jump straight into building your ETL pipelines, spinning up Docker‑based services, or prototyping Python scripts without any additional setup. If you need a specific language version or a tool not shipped by default, you can install it in your Codespace just like you would on any Linux machine.\n",
      "\n",
      "Before I can develop a dbt data model, what configuration steps do I need to perform, and how do I set up both a development and a deployment environment to run the jobs?\n",
      "Below is a step‑by‑step checklist that walks you from “nothing set up yet” to having a **development** and a **deployment** environment that can run dbt jobs.  \n",
      "The steps are derived from the course FAQ and the dbt‑Cloud setup documentation in the repository.\n",
      "\n",
      "| Step | What you do | Key things to verify | Source |\n",
      "|------|-------------|----------------------|--------|\n",
      "| **1. Prepare your source data** | • Create a BigQuery project and dataset (e.g., `dev_dataset`).  <br>• Load the raw tables (or create a copy of a public dataset).  <br>• Verify the dataset **location** (region) – it must match the location you’ll set in dbt Cloud. | *No cross‑region mismatch* | [Setup: No development environment](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/006_29469cf158_setup-no-development-environment.md) |\n",
      "| **2. Create a GitHub repository** | • Push your dbt project files to GitHub (including `dbt_project.yml`, `profiles.yml`, models, tests). | *Repo must be public or have the correct permission for dbt‑Cloud* | – |\n",
      "| **3. Add a BigQuery service account** | • In GCP → IAM & Admin → Service Accounts, create a service account.  <br>• Assign the roles: <br> • BigQuery Job User  <br> • BigQuery Data Owner <br> • BigQuery Data Editor <br> • Storage Object Admin <br> • Storage Admin | *The JSON key file is required for dbt‑Cloud* | [Setup: Connecting dbt Cloud with BigQuery Error](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/007_b4a5d32d6b_setup-connecting-dbt-cloud-with-bigquery-error.md) |\n",
      "| **4. Create a **Development Environment** in dbt‑Cloud** | • In dbt‑Cloud → **Settings** → **Environments** → **Create**.  <br>• Choose the repo you just pushed and the branch (usually `dev` or `main`).  <br>• For the **Credentials** field, upload the service‑account JSON you created.  <br>• Set the **Dataset** name (e.g., `dev_dataset`) and **Location** that matches your BigQuery dataset. | • `dbt debug` should return *All good!*. <br>• The *Develop* tab will now show your models. | [Setup: I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/061_dfd4ddbf39_setup-ive-set-github-and-bigquery-to-dbt-successfu.md) |\n",
      "| **5. Verify the development environment** | • Run `dbt debug` in the dbt‑Cloud terminal or via the UI to confirm the connection. <br>• Build a model (`dbt build` or `dbt run` in the UI) to make sure tables materialize. | *No permission or location errors* | – |\n",
      "| **6. Create a **Deployment Environment** (e.g., Production)** | • Duplicate the dev environment or create a new one.  <br>• Use a **Production**‑type environment. <br>• Supply a **different** service account if you want stricter permissions (often the same account but you can change the dataset name). <br>• Set the **Dataset** to the production dataset (e.g., `prod_dataset`). <br>• Ensure the **Location** matches the production BigQuery dataset. | • `dbt debug` must succeed in the prod environment too. | – |\n",
      "| **7. Configure a job** | • In dbt‑Cloud → **Jobs** → **Create**. <br>• Select the **Deployment Environment** you just created. <br>• Choose the target schema (usually your prod dataset). <br>• Set the *Model* scope (all models, a subset, or specific models). | • The job can be scheduled or run on‑demand. <br>• Confirm the job runs without errors. | – |\n",
      "| **8. Optional – Branching strategy** | • Keep a separate **dev** branch for iterative changes and merge to **main** (or **prod**) when ready. <br>• In the deployment environment, set the branch to `main`. | *Pull requests must be merged before the prod job can run* | – |\n",
      "\n",
      "### Quick‑reference checklist\n",
      "\n",
      "- [ ] BigQuery project & dataset created (region = region)\n",
      "- [ ] GitHub repo ready, dbt files committed\n",
      "- [ ] Service‑account JSON uploaded to dbt‑Cloud (roles: Job User, Data Owner, etc.)\n",
      "- [ ] Development environment created → `dbt debug` passes → models build\n",
      "- [ ] Deployment (prod) environment created → `dbt debug` passes → tables materialize\n",
      "- [ ] Job created in dbt‑Cloud → points to prod env → scheduled/running\n",
      "\n",
      "If you run into a “Dataset not found in location” or “Access denied” error, double‑check the dataset name, the region in BigQuery, and that the service account has the *BigQuery Job User* role (see the FAQ on location mismatches).\n",
      "\n",
      "Happy modeling!\n",
      "\n",
      "My GCP VM is filling up while backfilling data. What practical methods can I use to free up disk space, especially regarding Anaconda, Kestra files, and PostgreSQL data?\n",
      "Here are some concrete, practical steps you can take to reclaim space on your GCP VM while backfilling data, with a focus on Anaconda, Kestra, and PostgreSQL:\n",
      "\n",
      "| Area | What to Clean | Commands / Actions | Notes |\n",
      "|------|---------------|--------------------|-------|\n",
      "| **GCP VM root drive** | Identify the biggest offenders | `sudo du -sh * | sort -hr | head -20` | Gives you a quick snapshot of what’s taking up space. |\n",
      "| **Anaconda installer** | Remove the download file if you still have it | `rm -rf /path/to/Anaconda*.sh` | The installer is usually ~1 GB. |\n",
      "| **Anaconda environment** | Clean cached packages, unused packages, and temp files | `conda clean --all -y` | Frees ~2–3 GB depending on usage. |\n",
      "| **Kestra files** | Purge old executions and logs | <br>1. In Kestra, create a “purge” flow (see <https://kestra.io/docs/administrator-guide/purge>). <br>2. Adjust the `endDate` to `\"{{ now() }}\"` to purge everything up to the current moment. <br>3. Optionally set `removeFailedExecutions: true` if you don’t need FAILED state logs. | Keeps Kestra’s local storage lean. |\n",
      "| **Kestra storage directory** | Manually remove old data (only if you’re sure you won’t need it) | `rm -rf /app/storage/*` | Use the path you configured in the Kestra Docker/compose setup. |\n",
      "| **PostgreSQL database** | Delete unnecessary tables or rows | 1. Use `pgAdmin` or `psql` to drop tables: `DROP TABLE IF EXISTS my_old_table;` <br>2. Or run a custom SQL purge: `DELETE FROM executions WHERE end_date < NOW() - INTERVAL '30 days';` | Be careful – backup before dropping. |\n",
      "| **PostgreSQL logs & temp files** | Remove old log files | `find /var/log/postgresql/ -type f -name '*.log' -mtime +30 -delete` | Keeps only the last 30 days of logs. |\n",
      "| **Other temp directories** | Clean system temp | `sudo rm -rf /tmp/*` | Use with caution; only remove if not in use. |\n",
      "| **Package caches** | Remove unused package caches | `sudo apt-get clean` (if apt is used) | Frees cache used by system updates. |\n",
      "\n",
      "### Quick‑start script\n",
      "\n",
      "```bash\n",
      "#!/usr/bin/env bash\n",
      "set -euo pipefail\n",
      "\n",
      "echo \"=== Identifying largest directories ===\"\n",
      "sudo du -sh / | sort -hr | head -20\n",
      "\n",
      "echo \"=== Removing Anaconda installer if present ===\"\n",
      "rm -f /path/to/Anaconda*.sh\n",
      "\n",
      "echo \"=== Cleaning Anaconda caches ===\"\n",
      "conda clean --all -y\n",
      "\n",
      "echo \"=== Purging Kestra executions via API (example) ===\"\n",
      "curl -X POST -H \"Content-Type: application/json\" \\\n",
      "     -d '{\"name\":\"purge_flow\"}' \\\n",
      "     http://localhost:8080/api/v1/flows/run\n",
      "\n",
      "echo \"=== Deleting old Kestra storage files ===\"\n",
      "rm -rf /app/storage/*\n",
      "\n",
      "echo \"=== Cleaning old PostgreSQL logs ===\"\n",
      "find /var/log/postgresql/ -type f -name '*.log' -mtime +30 -delete\n",
      "\n",
      "echo \"=== Cleaning system apt cache ===\"\n",
      "sudo apt-get clean\n",
      "\n",
      "echo \"=== Finished! ===\"\n",
      "```\n",
      "\n",
      "> **Tip:** Run the script in a `screen` or `tmux` session so you can reconnect if the SSH session drops.\n",
      "\n",
      "---\n",
      "\n",
      "#### Source\n",
      "\n",
      "This answer is based on the guidance provided in the FAQ entry *“Running out of storage while trying to backfill”* found at:\n",
      "\n",
      "- [Running out of storage while trying to backfill](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-2/007_00e8093b90_running-out-of-storage-when-using-kestra-with-post.md)\n",
      "\n",
      "Feel free to adjust paths and dates to match your environment. Happy cleaning!\n",
      "\n",
      "I’m behind a network restriction that blocks Google, and my Terraform runs fail. How can I configure my VPN or proxy settings so that the terminal program respects the system proxy?\n",
      "The issue you’re seeing is a classic case of a VPN or proxy that blocks outbound traffic to Google (or the Terraform registry). Terraform’s CLI does **not** automatically pick up the system‑wide proxy settings, so you have to configure it explicitly in the terminal session.\n",
      "\n",
      "**Quick fix (from the FAQ)**  \n",
      "Open your VPN in “Enhanced Mode” (for example, **Clash**), then re‑run the command:\n",
      "\n",
      "```bash\n",
      "terraform apply\n",
      "```\n",
      "\n",
      "The VPN’s enhanced mode forwards DNS and HTTP(S) traffic correctly, letting Terraform reach `storage.googleapis.com` and the provider registry.\n",
      "\n",
      "> *Source:* [Terraform: Error: Post “storage.googleapis.com”…](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/115_31053272e0_terraform-error-post-storagegoogleapiscomhttpsstor.md)\n",
      "\n",
      "---\n",
      "\n",
      "### If you’re on a system proxy instead of a VPN\n",
      "\n",
      "Terraform respects the following environment variables, which you can set in your shell profile (`~/.bashrc`, `~/.zshrc`, etc.):\n",
      "\n",
      "| Variable | Purpose |\n",
      "|----------|---------|\n",
      "| `HTTP_PROXY` | For plain HTTP |\n",
      "| `HTTPS_PROXY` | For HTTPS |\n",
      "| `NO_PROXY` | Comma‑separated list of hosts that bypass the proxy |\n",
      "\n",
      "Example for a corporate proxy:\n",
      "\n",
      "```bash\n",
      "export HTTP_PROXY=\"http://proxy.company.com:8080\"\n",
      "export HTTPS_PROXY=\"http://proxy.company.com:8080\"\n",
      "export NO_PROXY=\"localhost,127.0.0.1,.company.com\"\n",
      "```\n",
      "\n",
      "After adding these, reload your shell (`source ~/.bashrc`) and try `terraform init` / `apply` again.\n",
      "\n",
      "---\n",
      "\n",
      "### Common pitfalls\n",
      "\n",
      "| Problem | Fix |\n",
      "|---------|-----|\n",
      "| **Terraform still can’t reach the registry** | Verify that the proxy actually allows outbound traffic to `*.googleapis.com` and `registry.terraform.io`. |\n",
      "| **You’re behind a restrictive firewall** | Ask your IT team to whitelist those domains or provide a dedicated VPN profile that routes only the required traffic. |\n",
      "| **Proxy requires authentication** | Use `HTTP_PROXY=\"http://user:pass@proxy.company.com:8080\"` (be careful with credentials in shell history). |\n",
      "\n",
      "---\n",
      "\n",
      "### When all else fails\n",
      "\n",
      "1. **Check network connectivity**  \n",
      "   ```bash\n",
      "   curl -I https://registry.terraform.io\n",
      "   ```\n",
      "2. **Run Terraform with verbose logging**  \n",
      "   ```bash\n",
      "   TF_LOG=debug terraform init\n",
      "   ```\n",
      "   Inspect the logs for DNS or connection errors.\n",
      "\n",
      "If you still hit the same timeout, it’s likely a deeper network restriction. In that case, consult your network or VPN vendor for a configuration that allows Terraform’s traffic to flow (e.g., enabling “split‑tunneling” or a dedicated VPN profile).\n",
      "\n",
      "Good luck, and let me know if you hit any specific error messages!\n",
      "\n",
      "The NYC taxi data I download is a .csv.gz file, but my script expects a .csv file. How can I modify my code to correctly handle and read the compressed CSV?\n",
      "To read the compressed `.csv.gz` files that the NYC Taxi data repo hosts, you don’t need to unzip them manually—`pandas` can load them directly.  \n",
      "Just change the part of your script that opens the file so it points to the gzipped file (or let `pandas.read_csv` detect the compression automatically).\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# URL (or local path) to the gzipped CSV\n",
      "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n",
      "\n",
      "# Option 1: let pandas auto‑detect the gzip compression\n",
      "df = pd.read_csv(url, low_memory=False)\n",
      "\n",
      "# Option 2: explicitly specify the compression (useful if you want to be explicit)\n",
      "df = pd.read_csv(url, compression='gzip', low_memory=False)\n",
      "```\n",
      "\n",
      "If you’re downloading the file locally first, just use the local filename:\n",
      "\n",
      "```python\n",
      "csv_path = \"yellow_tripdata_2021-01.csv.gz\"   # keep the .gz extension\n",
      "df = pd.read_csv(csv_path, compression='gzip', low_memory=False)\n",
      "```\n",
      "\n",
      "Because the file already contains the `.gz` extension, `pandas.read_csv` knows to decompress it on the fly, so your code will work the same way it did for a plain `.csv`.\n",
      "\n",
      "**Reference**  \n",
      "- “Taxi Data: How to handle *.csv.gz taxi data files?” – https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/002_d677df9ccb_taxi-data-how-to-handle-csvgz-taxi-data-files.md  \n",
      "- “Python: Pandas can read *.csv.gzip” – https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/087_e0c1900c47_python-pandas-can-read-csvgzip.md\n",
      "\n",
      "On Windows, pgcli prompts me for a password and then hangs. What is winpty and how can I use it to interact with PostgreSQL from the terminal?\n",
      "**What is *winpty*?**  \n",
      "On Windows, many command‑line tools (including `pgcli`) expect a POSIX‑style terminal.  \n",
      "When you run them inside Git‑Bash, PowerShell, or the Windows Terminal, the program\n",
      "can hang or fail to receive the password because the terminal interface isn’t\n",
      "properly exposed. `winpty` is a small wrapper that creates a proper PTY (pseudo‑TTY)\n",
      "for these programs, allowing them to read input and display output correctly.\n",
      "\n",
      "**How to use `winpty` with `pgcli`**\n",
      "\n",
      "1. Open **Git‑Bash** (or any Bash‑like terminal that supports `winpty`).\n",
      "2. Run `pgcli` through `winpty`:\n",
      "\n",
      "   ```bash\n",
      "   winpty pgcli -h localhost -p 5432 -u root -d ny_taxi\n",
      "   ```\n",
      "\n",
      "   - `-h localhost` – host\n",
      "   - `-p 5432` – port\n",
      "   - `-u root` – username\n",
      "   - `-d ny_taxi` – database\n",
      "\n",
      "3. You’ll now see the password prompt and can type your password without the\n",
      "   program hanging.\n",
      "\n",
      "Alternatively, you can use Windows Terminal or VS Code’s integrated terminal; just\n",
      "prefix the command with `winpty` if you still encounter the hang.\n",
      "\n",
      "> **Reference**: [PGCLI - stuck on password prompt](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/064_6704978d67_pgcli-stuck-on-password-prompt.md)\n",
      "\n",
      "My dbt profile's port is being read as a string, causing issues. How can I modify profiles.yml to ensure the port is treated as an integer using Jinja?\n",
      "To make dbt read the port as an integer, cast the environment‑variable value to an integer in the **profiles.yml** file.  \n",
      "Add the `| int` filter (or `int()` in Jinja) to the `port` line:\n",
      "\n",
      "```yaml\n",
      "target: dev\n",
      "\n",
      "outputs:\n",
      "  dev:\n",
      "    type: postgres\n",
      "    host: {{ env_var('DB_HOST') }}\n",
      "    user: {{ env_var('DB_USER') }}\n",
      "    password: {{ env_var('DB_PASSWORD') }}\n",
      "    database: {{ env_var('DB_NAME') }}\n",
      "    port: {{ env_var('DB_PORT') | int }}   # ← cast to integer\n",
      "```\n",
      "\n",
      "This forces the value of `DB_PORT` (which is usually a string from the environment) to be treated as an integer by dbt.\n",
      "\n",
      "> Reference: [When configuring the profiles.yml file for dbt‑postgres with jinja templates with environment variables](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/085_5d92a0583a_when-configuring-the-profilesyml-file-for-dbt-post.md)\n",
      "\n",
      "I need to troubleshoot container networking. What command should I run to list all Docker networks, and how can I interpret the output?\n",
      "I couldn’t find a specific FAQ entry that covers listing Docker networks in this repository, so I’m sharing the standard command and how to read its output based on the Docker documentation.\n",
      "\n",
      "---\n",
      "\n",
      "## ✅ List all Docker networks\n",
      "\n",
      "```bash\n",
      "docker network ls\n",
      "```\n",
      "\n",
      "### Typical output\n",
      "\n",
      "| NETWORK ID | NAME        | DRIVER     | SCOPE |\n",
      "|------------|-------------|------------|-------|\n",
      "| a1b2c3d4e5 | bridge      | bridge     | local |\n",
      "| f6g7h8i9j0 | host        | host       | local |\n",
      "| k1l2m3n4o5 | my‑custom   | overlay    | swarm |\n",
      "\n",
      "### What each column means\n",
      "\n",
      "| Column | Meaning |\n",
      "|--------|---------|\n",
      "| **NETWORK ID** | The unique identifier Docker assigns to the network. You’ll use this ID (or the network name) in other commands. |\n",
      "| **NAME** | Human‑readable name. Built‑in networks include `bridge`, `host`, and `none`. User‑created networks (like `my‑custom`) appear here. |\n",
      "| **DRIVER** | The network driver that implements the network. Common drivers: `bridge` (default for containers on the same host), `overlay` (for multi‑host Swarm clusters), `macvlan`, etc. |\n",
      "| **SCOPE** | Whether the network is local to the host (`local`) or spans a swarm (`swarm`). For most non‑Swarm setups this will be `local`. |\n",
      "\n",
      "---\n",
      "\n",
      "## 🔎 Inspect a specific network\n",
      "\n",
      "If you want to see the detailed configuration—subnet, gateway, connected containers, options, etc.—run:\n",
      "\n",
      "```bash\n",
      "docker network inspect <network-name-or-id>\n",
      "```\n",
      "\n",
      "Example:\n",
      "\n",
      "```bash\n",
      "docker network inspect my-custom\n",
      "```\n",
      "\n",
      "This returns JSON with fields such as:\n",
      "\n",
      "- `Subnet`, `Gateway`\n",
      "- `IPAM` configuration\n",
      "- `Containers` (listing each container’s ID, IP address, aliases)\n",
      "- `Options`, `Labels`\n",
      "\n",
      "---\n",
      "\n",
      "### Quick sanity check\n",
      "\n",
      "If you’re troubleshooting why a container can’t reach another, verify:\n",
      "\n",
      "1. **Both containers are on the same network** (`docker network inspect <net>` shows both IDs under `Containers`).\n",
      "2. **Network driver** supports the needed features (e.g., `overlay` is required for cross‑host communication in a Swarm).\n",
      "3. **No conflicting IP ranges** (subnets that overlap can cause routing problems).\n",
      "\n",
      "---\n",
      "\n",
      "#### If you want a deeper dive or run into specific errors\n",
      "\n",
      "- Use `docker network ls -q` to get just IDs, then `docker network inspect <id>` for more details.\n",
      "- Check Docker’s official docs on [Networking](https://docs.docker.com/network/) for advanced configuration.\n",
      "\n",
      "---\n",
      "\n",
      "**No repository reference was found for this exact question, so the answer above is based on standard Docker usage.**\n",
      "\n",
      "When I run `\\d <database name>` in pgcli I get an error about `c.relhasoids` not existing. What causes this error and how can I resolve it by reinstalling pgcli?\n",
      "The error\n",
      "\n",
      "```\n",
      "column c.relhasoids does not exist\n",
      "```\n",
      "\n",
      "occurs when **pgcli** is trying to query a PostgreSQL catalog column that was removed in newer PostgreSQL releases (the `relhasoids` column was dropped in PostgreSQL 12).  \n",
      "If you’re running a recent PostgreSQL version (≥ 12) but still have an older pgcli that contains the legacy query, the client will fail with that message.\n",
      "\n",
      "**Fix – reinstall (or upgrade) pgcli**\n",
      "\n",
      "1. **Uninstall pgcli**  \n",
      "   ```bash\n",
      "   pip uninstall pgcli   # or whatever package manager you used\n",
      "   ```\n",
      "\n",
      "2. **Re‑install the latest pgcli**  \n",
      "   ```bash\n",
      "   pip install --upgrade pgcli\n",
      "   ```\n",
      "\n",
      "   (If you used conda, you can run `conda install -c conda-forge pgcli` instead.)\n",
      "\n",
      "3. **Restart your terminal or computer** (so the new binary is loaded).\n",
      "\n",
      "After reinstalling, run `\\d <database_name>` again and the `relhasoids` error should disappear.\n",
      "\n",
      "Reference:  \n",
      "[PGCLI - error column c.relhasoids does not exist](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/069_0a6a3ccf35_pgcli-error-column-crelhasoids-does-not-exist.md)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Use the generated questions as input to the agent and log the response\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f30d3a",
   "metadata": {},
   "source": [
    "- Repeat generation until sufficient data is collected (e.g., ~100 examples).  \n",
    "- For now, we can use the 10 existing log records.  \n",
    "- Benefits of AI-generated data:  \n",
    "  - Faster data creation  \n",
    "  - Can cover edge cases that might be overlooked  \n",
    "- Limitations:  \n",
    "  - May not reflect real user behavior  \n",
    "  - Might miss edge cases only real users encounter  \n",
    "  - May not capture full complexity of real queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10a9138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all AI-generated logs for evaluation\n",
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0ae84943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d24d4a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b32b4ee71d4bcc8029dc884b7d07e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification=\"The agent followed the user's instructions by providing a detailed answer and citing the source material.\", check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent avoided doing things it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question about configuring VPN or proxy settings for Terraform.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct, providing step-by-step instructions and examples.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations and sources, such as the GitHub repository link.', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response is complete and covers all key aspects of the request, including common pitfalls and troubleshooting steps.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked the search tool to find relevant information from the course materials.', check_pass=True)] summary='The agent provided a high-quality answer that follows instructions, is relevant, clear, and complete, and includes proper citations.'\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='The agent provided a clear and accurate answer using Jinja to solve the issue of the port being read as a string in the profiles.yml file.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='There are no indications that the agent did anything it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question about modifying the profiles.yml file to ensure the port is treated as an integer using Jinja.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear, providing a specific solution to the problem by casting the environment-variable value to an integer in the profiles.yml file.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes a proper citation with a link to the source material.', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response covers all key aspects of the request, including how to modify the profiles.yml file and providing a reference.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked the search tool as indicated in the log.', check_pass=True)] summary=\"The agent provided a clear, accurate, and relevant answer that directly addresses the user's question and includes proper citations.\"\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification=\"The agent followed the user's instructions.\", check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent avoided doing things it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations or sources when required.', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response is complete and covers all key aspects of the request', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked search tool', check_pass=True)] summary=\"The agent provided a clear and relevant answer to the user's question, followed the instructions, and included proper citations.\"\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='The agent followed the instructions by providing a detailed answer and citing the source material', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent avoided doing things it was told not to do', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations and sources when required', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response is complete and covers all key aspects of the request', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked the search tool to find relevant information from the course materials', check_pass=True)] summary=\"The AI agent's answer is of high quality, following instructions, providing relevant and clear information, and including proper citations.\"\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='The agent provided a step-by-step checklist for setting up development and deployment environments to run dbt jobs', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent did not do anything it was told not to do', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question about configuration steps and environment setup\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and provides a detailed checklist', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations and references to the course FAQ and dbt-Cloud setup documentation', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response covers all key aspects of the request, including setup and verification of development and deployment environments', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked the search tool in the log', check_pass=True)] summary='The agent provided a clear and relevant response that follows the instructions and includes proper citations and references'\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification=\"The agent followed the user's instructions and used the search tool.\", check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent avoided doing things it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations or sources when required.', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response is complete and covers all key aspects of the request.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked search tool.', check_pass=True)] summary=\"The agent provided a clear, relevant, and complete answer that directly addresses the user's question and includes proper citations.\"\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='The agent provided accurate steps to set up the bootstrap server and cluster credentials.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent did not do anything it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question about setting up the bootstrap server and cluster credentials.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct, providing a step-by-step guide to fixing the issue.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations and sources when required.', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response is complete and covers all key aspects of the request.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked the search tool to find relevant information from the course materials.', check_pass=True)] summary=\"The agent provided a clear and accurate response to the user's question, following the instructions and including proper citations.\"\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='The agent followed the instructions for handling the .csv.gz file and provided relevant code and references.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent did not do anything it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question about handling .csv.gz files.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct, providing code examples and explanations.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations and references to the source material.', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response covers all key aspects of the request, including code examples and references.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked the search tool to find relevant information from the course materials.', check_pass=True)] summary=\"The agent provided a clear and relevant response that directly addressed the user's question, including code examples and proper citations.\"\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='The agent used the search tool and provided an accurate answer with references.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent did not do anything it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='The response includes proper citations.', check_pass=True), EvaluationCheck(check_name='completeness', justification='The response is complete and covers all key aspects of the request.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked the search tool.', check_pass=True)] summary=\"The agent followed the user's instructions and provided a clear, relevant, and complete answer with proper citations.\"\n",
      "checklist=[EvaluationCheck(check_name='instructions_follow', justification='The agent used standard Docker commands for troubleshooting container networking as instructed.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='The agent avoided doing things it was told not to do.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The response directly addresses the user's question about Docker networks.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The answer is clear and correct.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification=\"The response includes a link to Docker's official documentation but does not follow the required citation format and does not cite any filenames from the repository.\", check_pass=False), EvaluationCheck(check_name='completeness', justification='The response covers all key aspects of the request, including the command to list networks and how to interpret the output.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The agent invoked a search tool as shown in the log.', check_pass=True)] summary='The agent provided a clear and correct answer but did not follow the citation format for the source material.'\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    print(eval_result)\n",
    "    eval_results.append((log_record, eval_result)) # Store both log and evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "354b004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform results into a tabular format for analysis\n",
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0d38179e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faq_agent_v2_20251002_170308_aa2d8d.json</td>\n",
       "      <td>I’m behind a network restriction that blocks G...</td>\n",
       "      <td>We have a relevant answer. Provide guidance ci...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faq_agent_v2_20251002_170415_dcb37b.json</td>\n",
       "      <td>My dbt profile's port is being read as a strin...</td>\n",
       "      <td>We found the relevant answer: \"When configurin...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faq_agent_v2_20251002_170132_d8333c.json</td>\n",
       "      <td>How do I launch a GitHub Codespace for my data...</td>\n",
       "      <td>We have the relevant answer: question 023_5b4f...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faq_agent_v2_20251002_170249_7f475b.json</td>\n",
       "      <td>My GCP VM is filling up while backfilling data...</td>\n",
       "      <td>We found relevant answer. Need to answer user ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faq_agent_v2_20251002_170221_515204.json</td>\n",
       "      <td>Before I can develop a dbt data model, what co...</td>\n",
       "      <td>We need to answer user: \"Before I can develop ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  \\\n",
       "0  faq_agent_v2_20251002_170308_aa2d8d.json   \n",
       "1  faq_agent_v2_20251002_170415_dcb37b.json   \n",
       "2  faq_agent_v2_20251002_170132_d8333c.json   \n",
       "3  faq_agent_v2_20251002_170249_7f475b.json   \n",
       "4  faq_agent_v2_20251002_170221_515204.json   \n",
       "\n",
       "                                            question  \\\n",
       "0  I’m behind a network restriction that blocks G...   \n",
       "1  My dbt profile's port is being read as a strin...   \n",
       "2  How do I launch a GitHub Codespace for my data...   \n",
       "3  My GCP VM is filling up while backfilling data...   \n",
       "4  Before I can develop a dbt data model, what co...   \n",
       "\n",
       "                                              answer  instructions_follow  \\\n",
       "0  We have a relevant answer. Provide guidance ci...                 True   \n",
       "1  We found the relevant answer: \"When configurin...                 True   \n",
       "2  We have the relevant answer: question 023_5b4f...                 True   \n",
       "3  We found relevant answer. Need to answer user ...                 True   \n",
       "4  We need to answer user: \"Before I can develop ...                 True   \n",
       "\n",
       "   instructions_avoid  answer_relevant  answer_clear  answer_citations  \\\n",
       "0                True             True          True              True   \n",
       "1                True             True          True              True   \n",
       "2                True             True          True              True   \n",
       "3                True             True          True              True   \n",
       "4                True             True          True              True   \n",
       "\n",
       "   completeness  tool_call_search  \n",
       "0          True              True  \n",
       "1          True              True  \n",
       "2          True              True  \n",
       "3          True              True  \n",
       "4          True              True  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5221d530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    1.0\n",
       "instructions_avoid     1.0\n",
       "answer_relevant        1.0\n",
       "answer_clear           1.0\n",
       "answer_citations       0.9\n",
       "completeness           1.0\n",
       "tool_call_search       1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba5440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601cf79b",
   "metadata": {},
   "source": [
    "### Evaluating Functions and Tools\n",
    "\n",
    "- Tools should be evaluated **separately from the agent**.  \n",
    "- For code tools: use **unit and integration tests**.  \n",
    "- For search functions: evaluate with **information retrieval metrics**, such as:  \n",
    "  - **Precision & Recall**: fraction of relevant results retrieved vs. missed  \n",
    "  - **Hit Rate**: % of queries returning at least one relevant result  \n",
    "  - **MRR (Mean Reciprocal Rank)**: position of the first relevant result in ranking  \n",
    "- Implement Hit Rate and MRR calculations in Python for automated evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "824f9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(search_function, test_queries):\n",
    "    results = []\n",
    "    \n",
    "    for query, expected_docs in test_queries:\n",
    "        search_results = search_function(query)\n",
    "        \n",
    "        # Calculate hit rate\n",
    "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        for i, doc in enumerate(search_results):\n",
    "            if doc['filename'] in expected_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            mrr = 0\n",
    "            \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'hit': relevant_found,\n",
    "            'mrr': mrr\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7cfc3194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'cd8a62fc55',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'sort_order': 24,\n",
       "  'content': 'For example, when running `JsonConsumer.java`, you might see:\\n\\n```\\nConsuming form kafka started\\n\\nRESULTS:::0\\n\\nRESULTS:::0\\n\\nRESULTS:::0\\n```\\n\\nOr when running `JsonProducer.java`, you might encounter:\\n\\n```\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\n```\\n\\n**Solution:**\\n\\n1. Ensure the `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` in the scripts located at `src/main/java/org/example/` (e.g., `JsonConsumer.java`, `JsonProducer.java`) is pointing to the correct server URL (e.g., `europe-west3` vs `europe-west2`).\\n\\n2. Verify that the cluster key and secrets are updated in `src/main/java/org/example/Secrets.java` (`KAFKA_CLUSTER_KEY` and `KAFKA_CLUSTER_SECRET`).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/024_cd8a62fc55_java-kafka-when-running-the-producerconsumeretc-ja.md'},\n",
       " {'id': '5b4fb0c0a8',\n",
       "  'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?',\n",
       "  'sort_order': 23,\n",
       "  'content': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\n\\nYou can also open any GitHub repository in a GitHub Codespace.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/023_5b4fb0c0a8_environment-is-github-codespaces-an-alternative-to.md'}]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2bc8a668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  ['faq-main/_questions/data-engineering-zoomcamp/module-6/024_cd8a62fc55_java-kafka-when-running-the-producerconsumeretc-ja.md']),\n",
       " ('Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?',\n",
       "  ['faq-main/_questions/data-engineering-zoomcamp/general/023_5b4fb0c0a8_environment-is-github-codespaces-an-alternative-to.md'])]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_queries = [(item['question'], [item['filename']]) for item in sample]\n",
    "tests_queries[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "cd4712a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_search_quality(text_search, tests_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "be2f2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate = sum(r['hit'] for r in test_results) / len(test_results) if test_results else 0\n",
    "avg_mrr = sum(r['mrr'] for r in test_results) / len(test_results) if test_results else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "715506ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 1.00\n",
      "Average MRR: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hit Rate: {hit_rate:.2f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397c1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
