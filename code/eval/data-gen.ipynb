{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30df742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LOGS_DIRECTORY'] = '../eval_logs'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9446becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "from main import initialize_index, initialize_agent\n",
    "from logs import log_interaction_to_file, LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce0c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/oliver/Desktop/Agents/ai-agent-crash-course/code/eval/../eval_logs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIR.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f26a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:06:23,122 [INFO] aihero: Starting AI Pydantic-AI Assistant for pydantic/pydantic-ai\n",
      "2025-10-04 01:06:23,123 [INFO] aihero: Initializing data ingestion...\n",
      "2025-10-04 01:06:26,846 [INFO] aihero: Fetched 114 documents from the repository.\n",
      "2025-10-04 01:06:26,847 [INFO] aihero: Chunking documents using markdown_sections...\n",
      "2025-10-04 01:06:26,849 [INFO] aihero: Total chunks created: 606\n",
      "2025-10-04 01:06:26,850 [INFO] aihero: Indexing documents...\n",
      "2025-10-04 01:06:26,915 [INFO] aihero: Data indexing completed successfully!\n",
      "2025-10-04 01:06:26,916 [INFO] aihero: Initializing search agent...\n",
      "2025-10-04 01:06:26,918 [INFO] aihero: Loading embedding model...\n",
      "2025-10-04 01:06:26,958 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0\n",
      "2025-10-04 01:06:26,959 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: multi-qa-distilbert-cos-v1\n",
      "2025-10-04 01:06:34,652 [INFO] aihero: Creating embeddings for vector search...\n",
      "2025-10-04 01:06:34,653 [INFO] aihero: Encoding document chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 606/606 [00:12<00:00, 50.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:06:47,003 [INFO] aihero: Agent initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "index = initialize_index()\n",
    "agent = initialize_agent(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d7acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create example user questions for an AI assistant that specializes in the **pydantic-ai** repository.\n",
    "\n",
    "Based on the provided documentation or repository content, generate realistic questions that developers might ask.\n",
    "\n",
    "The questions should:\n",
    "- Be natural and varied in phrasing\n",
    "- Range from beginner-friendly to advanced\n",
    "- Cover practical usage, installation, integration, features, and troubleshooting\n",
    "- Include both conceptual and code-related questions\n",
    "- Focus specifically on the pydantic-ai project\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='groq:openai/gpt-oss-120b',\n",
    "    output_type=QuestionsList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a50c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(index.docs, 10)\n",
    "prompt = [d['content'] for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa704ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 00:42:06,148 [INFO] httpx: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = await question_generator.run(user_prompt=json.dumps(prompt))\n",
    "question_list = result.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066e7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionsList(questions=['How can I use an ExternalToolset to handle deferred tool calls and feed the results back into a Pydantic‑AI agent?', 'Is it possible to stream structured responses from the model and display them live in a terminal UI (e.g. with Rich) using the StreamedResponse API?', 'What breaking changes were introduced in v0.7.6 of pydantic‑ai and how do I migrate my existing code?', 'In the Data Analyst example, how do I use agent dependencies to pass a pandas DataFrame from one tool to another within the same run?', 'How do I configure retry logic for tools in Pydantic‑AI using the pydantic_ai.retries module?', 'If a tool raises a ModelRetry, how can I ensure the agent automatically retries with a new prompt?', 'How do I set up a FastMCP server that allows sampling from a language model inside a custom tool?', 'Using FunctionModel, how can I mock LLM calls for unit testing a weather‑forecast agent?', 'What’s the correct way to use StateDeps with a custom Pydantic BaseModel to maintain per‑request state in an AG‑UI front‑end?', 'How can I specify an output_type that includes several Pydantic models and a plain string, and what should I keep in mind for type‑checking?'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question_list = QuestionsList(questions=['How can I use an ExternalToolset to handle deferred tool calls and feed the results back into a Pydantic‑AI agent?', 'Is it possible to stream structured responses from the model and display them live in a terminal UI (e.g. with Rich) using the StreamedResponse API?', 'What breaking changes were introduced in v0.7.6 of pydantic‑ai and how do I migrate my existing code?', 'In the Data Analyst example, how do I use agent dependencies to pass a pandas DataFrame from one tool to another within the same run?', 'How do I configure retry logic for tools in Pydantic‑AI using the pydantic_ai.retries module?', 'If a tool raises a ModelRetry, how can I ensure the agent automatically retries with a new prompt?', 'How do I set up a FastMCP server that allows sampling from a language model inside a custom tool?', 'Using FunctionModel, how can I mock LLM calls for unit testing a weather‑forecast agent?', 'What’s the correct way to use StateDeps with a custom Pydantic BaseModel to maintain per‑request state in an AG‑UI front‑end?', 'How can I specify an output_type that includes several Pydantic models and a plain string, and what should I keep in mind for type‑checking?'])\n",
    "question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d52ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b551fe3645487fa14c17928c383cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I use an ExternalToolset to handle deferred tool calls and feed the results back into a Pydantic‑AI agent?\n",
      "2025-10-04 01:09:21,565 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09a134bd2554a349b2bfc9f75b32912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:09:33,007 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To use an `ExternalToolset` for handling deferred tool calls and feeding results back into a Pydantic-AI agent, you can follow these steps:\n",
      "\n",
      "1. **Define External Tools**: Create an external tool that will define what actions to take if the task cannot be completed during the same agent run. This can be achieved by raising the `CallDeferred` exception when certain conditions are met, such as needing longer processing time or waiting for user approval.\n",
      "\n",
      "2. **Handling Deferred Requests**: When an external tool is called and cannot execute immediately, the agent run will yield a `DeferredToolRequests` object, which contains information about the tool calls. This includes tool names, their arguments, and unique identifiers for each tool call.\n",
      "\n",
      "3. **Execute the Tasks Externally**: Implement the logic to execute the external task (e.g., using a background worker or an upstream service) that corresponds to the deferred tool call. Make sure to keep track of which tool call corresponds to which operation.\n",
      "\n",
      "4. **Feed Results Back into the Agent**: Once the external tasks are completed, wrap the responses in a `DeferredToolResults` object. You will use this object along with the original message history when you start a new agent run to continue from where it left off.\n",
      "\n",
      "5. **Example Code**:\n",
      "   Here is a simplified example:\n",
      "   ```python\n",
      "   async def calculate_answer(ctx: RunContext, question: str) -> str:\n",
      "       # This function may allow deferring if answer is not available immediately\n",
      "       raise CallDeferred  # Indicate that this call needs to be deferred\n",
      "\n",
      "   # When ready to process results:\n",
      "   result = await agent.run('Your initial message here', deferred_tool_results=your_results)\n",
      "   ```\n",
      "\n",
      "6. **Running the Agent**: Make sure to pass the required parameters to your agent's run method, including the `deferred_tool_results`.\n",
      "\n",
      "By utilizing the `ExternalToolset` pattern, your agent can efficiently manage delays and dependencies on external factors while providing coherent interactions. For further details and examples, check the documentation on [Deferred Tools](https://github.com/pydantic/pydantic-ai/blob/main/docs/deferred-tools.md).\n",
      "\n",
      "This should guide you effectively in implementing the required functionalities in Pydantic-AI agents using deferred tool calls.\n",
      "\n",
      "Is it possible to stream structured responses from the model and display them live in a terminal UI (e.g. with Rich) using the StreamedResponse API?\n",
      "2025-10-04 01:09:35,235 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56663c11c08e449aa79ca2c877b71236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:09:44,422 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Yes, it is indeed possible to stream structured responses from the model and display them live in a terminal UI using the StreamedResponse API. The response can be streamed in real time, allowing for dynamic updates in the UI. \n",
      "\n",
      "The documentation provides an example where structured responses about whales are streamed and validated while displaying the data as a dynamic table using the [`rich`](https://github.com/Textualize/rich) library in Python. This demonstrates the integration of streaming structured outputs with a terminal UI effectively.\n",
      "\n",
      "### Example of Streaming Structured Output\n",
      "Here's a brief outline of how this can be set up:\n",
      "\n",
      "1. **Setup**: Ensure you have the necessary dependencies installed, including `rich` and `Pydantic AI`.\n",
      "2. **Code Example**: Below is a conceptual code snippet on how you might implement this:\n",
      "\n",
      "   ```python\n",
      "   from rich import print\n",
      "   from pydantic_ai import Agent\n",
      "\n",
      "   agent = Agent('openai:gpt-4.1', instructions='Provide structured information about whales.')\n",
      "\n",
      "   async def main():\n",
      "       async with agent.run_stream('Tell me about whales.') as result:\n",
      "           async for data in result.stream_output():\n",
      "               print(data)  # This can be formatted as needed using Rich\n",
      "\n",
      "   # Execute the main function\n",
      "   asyncio.run(main())\n",
      "   ```\n",
      "\n",
      "3. **Execution**: Run your application, and it would stream responses as they are generated, allowing you to visualize them in real time.\n",
      "\n",
      "### Running the Example\n",
      "You can run the example by executing a command like this in your terminal:\n",
      "\n",
      "```bash\n",
      "python -m pydantic_ai_examples.stream_whales\n",
      "```\n",
      "\n",
      "This will set up your agent and display the streamed structured responses live in the terminal using the `rich` library.\n",
      "\n",
      "For more detailed instructions and to see the complete code example, refer to the source [here](https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/stream-whales.md).\n",
      "\n",
      "What breaking changes were introduced in v0.7.6 of pydantic‑ai and how do I migrate my existing code?\n",
      "2025-10-04 01:09:45,981 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0610e79e1f459d8f53071ebadfbe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:09:53,883 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "In version v0.7.6 of Pydantic AI, a breaking change was introduced that specifically affects how retries are managed with the transport classes. The key change is:\n",
      "\n",
      "- **Breaking Change**: The `TenacityTransport` and `AsyncTenacityTransport` now require the use of `pydantic_ai.retries.RetryConfig` instead of `tenacity.Retrying` or `tenacity.AsyncRetrying`. The `RetryConfig` is a `TypedDict` containing the keyword arguments for `tenacity.retry`.\n",
      "\n",
      "### Migration Steps\n",
      "\n",
      "To migrate your existing code to be compliant with this change, you will need to:\n",
      "\n",
      "1. **Update Transport Creation**: Wherever you construct instances of `TenacityTransport` or `AsyncTenacityTransport`, replace the `Retrying` or `AsyncRetrying` objects with a `RetryConfig`.\n",
      "   \n",
      "   For instance, if you previously had:\n",
      "   ```python\n",
      "   from tenacity import Retrying\n",
      "\n",
      "   transport = TenacityTransport(retries=Retrying(...))\n",
      "   ```\n",
      "   You will now create it as:\n",
      "   ```python\n",
      "   from pydantic_ai.retries import RetryConfig\n",
      "   \n",
      "   retry_config = RetryConfig(max_attempts=5, retry_delay=2)  # Customize as necessary\n",
      "   transport = TenacityTransport(retries=retry_config)\n",
      "   ```\n",
      "\n",
      "2. **Ensure Compatibility**: Review other parts of your code that might still be using `Retrying` or `AsyncRetrying` directly and replace them with the `RetryConfig`.\n",
      "\n",
      "This change was inadvertently released in a patch version instead of a minor version, so you should ensure to adjust your code as soon as possible to avoid issues in future updates.\n",
      "\n",
      "For more details on this and other breaking changes, you can refer to the [CHANGELOG](https://github.com/pydantic/pydantic-ai/blob/main/docs/changelog.md).\n",
      "\n",
      "In the Data Analyst example, how do I use agent dependencies to pass a pandas DataFrame from one tool to another within the same run?\n",
      "2025-10-04 01:09:54,968 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1078bf6ee598482091b89992799af80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:10:06,070 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To pass a pandas DataFrame from one tool to another within the same run in the Data Analyst example using agent dependencies, you can follow these steps:\n",
      "\n",
      "1. **Define the Dependencies**: You start by creating a dataclass that will hold the pandas DataFrame and any other necessary objects required for your operations.\n",
      "\n",
      "2. **Initialize the Agent with Dependencies**: When you create your agent, you specify the type of the dependencies dataclass.\n",
      "\n",
      "3. **Use the Dependencies in the Tools**: When you run the tools within your agent, you'll pass an instance of your dependencies class. Within your system prompt or tool function, you can access the dependencies using the `RunContext`.\n",
      "\n",
      "### Example Code Snippet\n",
      "\n",
      "Here’s an illustrative code snippet based on the Data Analyst context:\n",
      "\n",
      "```python\n",
      "from dataclasses import dataclass\n",
      "import pandas as pd\n",
      "from pydantic_ai import Agent, RunContext\n",
      "\n",
      "@dataclass\n",
      "class DataAnalysisDependencies:\n",
      "    dataframe: pd.DataFrame\n",
      "\n",
      "# Initialize the Agent\n",
      "agent = Agent(\n",
      "    'openai:gpt-4',  # Specify your model here\n",
      "    deps_type=DataAnalysisDependencies\n",
      ")\n",
      "\n",
      "@agent.system_prompt\n",
      "async def analyze_data(ctx: RunContext[DataAnalysisDependencies]) -> str:\n",
      "    df = ctx.deps.dataframe\n",
      "    # Perform your data analysis here\n",
      "    result = df.describe()  # Example operation\n",
      "    return result.to_string()\n",
      "\n",
      "async def main():\n",
      "    # Assume you have a DataFrame ready for analysis\n",
      "    data = {\n",
      "        \"column1\": [1, 2, 3],\n",
      "        \"column2\": [4, 5, 6]\n",
      "    }\n",
      "    df = pd.DataFrame(data)\n",
      "    \n",
      "    deps = DataAnalysisDependencies(dataframe=df)\n",
      "    result = await agent.run(\"Analyze the given DataFrame.\", deps=deps)\n",
      "    print(result.output)\n",
      "\n",
      "# You would need to run this main function with asyncio\n",
      "```\n",
      "\n",
      "### Key Points:\n",
      "\n",
      "- The `DataAnalysisDependencies` class is used to encapsulate the DataFrame.\n",
      "- The agent is set up to accept the dependencies.\n",
      "- By utilizing `RunContext`, you can access the DataFrame within your analysis function.\n",
      "\n",
      "This structure allows you to effectively pass complex data types like a pandas DataFrame through your agent operations while maintaining type safety and clarity.\n",
      "\n",
      "For more information on dependencies and their use in Pydantic AI, you can refer to the documentation:\n",
      "- [Data Analyst Example Documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/data-analyst.md)\n",
      "- [Dependencies Documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md)\n",
      "\n",
      "How do I configure retry logic for tools in Pydantic‑AI using the pydantic_ai.retries module?\n",
      "2025-10-04 01:10:07,557 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088980b8a6b14efbb80d7cfeb57659ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:10:28,877 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To configure retry logic for tools in Pydantic-AI using the `pydantic_ai.retries` module, you can follow these steps:\n",
      "\n",
      "### 1. Install the Required Dependency\n",
      "Ensure you have the `tenacity` library installed, which is required for retry functionality. You can install it using the following command:\n",
      "```bash\n",
      "pip install pydantic-ai-slim[retries]\n",
      "```\n",
      "\n",
      "### 2. Create a Client with Retry Logic\n",
      "You can create an HTTP client with custom retry handling. Here is an example code snippet:\n",
      "\n",
      "```python\n",
      "from httpx import AsyncClient, HTTPStatusError\n",
      "from tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n",
      "from pydantic_ai import Agent\n",
      "from pydantic_ai.models.openai import OpenAIChatModel\n",
      "from pydantic_ai.providers.openai import OpenAIProvider\n",
      "from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n",
      "\n",
      "def create_retrying_client():\n",
      "    \"\"\"Create a client with smart retry handling for multiple error types.\"\"\"\n",
      "\n",
      "    def should_retry_status(response):\n",
      "        \"\"\"Raise exceptions for retryable HTTP status codes.\"\"\"\n",
      "        if response.status_code in (429, 502, 503, 504):\n",
      "            response.raise_for_status()  # This will raise HTTPStatusError\n",
      "\n",
      "    transport = AsyncTenacityTransport(\n",
      "        config=RetryConfig(\n",
      "            retry=retry_if_exception_type((HTTPStatusError, ConnectionError)),\n",
      "            wait=wait_retry_after(\n",
      "                fallback_strategy=wait_exponential(multiplier=1, max=60),\n",
      "                max_wait=300\n",
      "            ),\n",
      "            stop=stop_after_attempt(5),\n",
      "            reraise=True\n",
      "        ),\n",
      "        validate_response=should_retry_status\n",
      "    )\n",
      "    return AsyncClient(transport=transport)\n",
      "\n",
      "# Use the retrying client with a model\n",
      "client = create_retrying_client()\n",
      "model = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(http_client=client))\n",
      "agent = Agent(model)\n",
      "```\n",
      "\n",
      "### 3. Explanation of Key Components\n",
      "- **Transport**: Use `AsyncTenacityTransport` to handle retries.\n",
      "- **Retry Config**: Configure retry conditions using `RetryConfig`:\n",
      "  - `retry`: Specify which exceptions to retry.\n",
      "  - `wait`: Define the waiting strategy, utilizing `wait_retry_after` for smart delays.\n",
      "  - `stop`: Set how many attempts to make before stopping.\n",
      "- **Custom Logic**: Define how to determine if a response should be retried (e.g., using HTTP status codes).\n",
      "\n",
      "### 4. Wait Strategies\n",
      "You can use different strategies for waiting between retry attempts. The smart waiting strategy can automatically respect HTTP `Retry-After` headers using `wait_retry_after`.\n",
      "\n",
      "### 5. Using with Different Providers\n",
      "You can integrate the retry client with different providers by passing the client to the provider.\n",
      "\n",
      "By following these steps, you should be able to configure retry logic for HTTP requests in Pydantic-AI effectively. \n",
      "\n",
      "For detailed documentation, you can refer to the [retries documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md).\n",
      "\n",
      "If a tool raises a ModelRetry, how can I ensure the agent automatically retries with a new prompt?\n",
      "2025-10-04 01:10:30,244 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80270dc334c4b6695f495c54a521447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:10:41,058 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To ensure that an agent automatically retries with a new prompt after a `ModelRetry` is raised, you can utilize the `retries` parameter when defining your tool. This parameter specifies the number of times to retry if a `ModelRetry` exception occurs. Here's an overview of how to implement this:\n",
      "\n",
      "1. **Define Your Tool with Retries**: When creating your tool using the `@agent.tool` decorator, specify the `retries` argument to indicate how many attempts should be made in case of a retryable error.\n",
      "\n",
      "2. **Handle the `ModelRetry` Exception**: The tool can raise a `ModelRetry` exception, which will trigger the retry mechanism.\n",
      "\n",
      "Here is an example code snippet demonstrating this:\n",
      "\n",
      "```python\n",
      "from pydantic import BaseModel\n",
      "from pydantic_ai import Agent, RunContext, ModelRetry\n",
      "\n",
      "class ChatResult(BaseModel):\n",
      "    user_id: int\n",
      "    message: str\n",
      "\n",
      "agent = Agent('openai:gpt-4o')\n",
      "\n",
      "@agent.tool(retries=2)  # Specify to retry 2 times on ModelRetry\n",
      "def get_user_by_name(ctx: RunContext, name: str) -> int:\n",
      "    user_id = ctx.deps.users.get(name=name)\n",
      "    if user_id is None:\n",
      "        raise ModelRetry(f'No user found with name {name!r}, remember to provide their full name')\n",
      "    return user_id\n",
      "\n",
      "result = agent.run_sync('Send a message to John Doe asking for coffee next week')\n",
      "print(result.output)\n",
      "```\n",
      "\n",
      "In this example, if the `get_user_by_name` function encounters a `ModelRetry` because no user is found, it will automatically attempt the operation up to 2 additional times before failing completely.\n",
      "\n",
      "### Important Notes:\n",
      "- Each retry will use the original input prompt unless you implement logic to modify the prompt based on previous attempts.\n",
      "- You can check the current retry count through `ctx.retry` within your tool.\n",
      "- Make sure that your logic aligns with how you want to handle retries, possibly adjusting the prompt dynamically if needed.\n",
      "\n",
      "For further details, you can refer to the documentation on handling `ModelRetry` and defining agent tools: [Agent Documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/api/agent.md).\n",
      "\n",
      "How do I set up a FastMCP server that allows sampling from a language model inside a custom tool?\n",
      "2025-10-04 01:10:42,371 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99225a407f5045c6831d85cc3bc9942f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:10:56,837 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To set up a FastMCP server that allows sampling from a language model inside a custom tool, you can use the following example code. This setup involves creating a server with an agent and defining a tool that interacts with the language model using sampling capabilities.\n",
      "\n",
      "### Step 1: Create the FastMCP Server\n",
      "\n",
      "Here’s a basic implementation to create a FastMCP server that uses a language model for generating poetry:\n",
      "\n",
      "```python\n",
      "# mcp_server_sampling.py\n",
      "from mcp.server.fastmcp import Context, FastMCP\n",
      "from pydantic_ai import Agent\n",
      "from pydantic_ai.models.mcp_sampling import MCPSamplingModel\n",
      "\n",
      "server = FastMCP('Pydantic AI Server with sampling')\n",
      "server_agent = Agent(system_prompt='always reply in rhyme')\n",
      "\n",
      "@server.tool()\n",
      "async def poet(ctx: Context, theme: str) -> str:\n",
      "    \"\"\"Poem generator\"\"\"\n",
      "    r = await server_agent.run(f'write a poem about {theme}', model=MCPSamplingModel(session=ctx.session))\n",
      "    return r.output\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    server.run()  # run the server over stdio\n",
      "```\n",
      "\n",
      "### Step 2: Create a Client to Interact with the Server\n",
      "\n",
      "You will also need a client that can call this server. The following code demonstrates how you can interact with the `poet` tool:\n",
      "\n",
      "```python\n",
      "# mcp_client_sampling.py\n",
      "import asyncio\n",
      "import os\n",
      "from mcp import ClientSession, StdioServerParameters\n",
      "from mcp.client.stdio import stdio_client\n",
      "\n",
      "async def client():\n",
      "    server_params = StdioServerParameters(\n",
      "        command='python', args=['mcp_server_sampling.py'], env=os.environ\n",
      "    )\n",
      "    async with stdio_client(server_params) as (read, write):\n",
      "        async with ClientSession(read, write) as session:\n",
      "            await session.initialize()\n",
      "            result = await session.call_tool('poet', {'theme': 'socks'})\n",
      "            print(result.content[0].text)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    asyncio.run(client())\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "1. **Server Setup**: In the server code (`mcp_server_sampling.py`), you define a FastMCP server instance and configure an agent with a specific system prompt. The `poet` tool generates poetry based on the provided theme using the sampling model.\n",
      "\n",
      "2. **Client Interaction**: The client (`mcp_client_sampling.py`) connects to the server and calls the `poet` tool, passing a theme. The response from the server is printed out.\n",
      "\n",
      "### Notes\n",
      "\n",
      "- Ensure that both the server and client are run in an environment that can execute the server script.\n",
      "- The server relies on the Pydantic AI model capabilities to perform sampling.\n",
      "- Adjust the prompts and model as necessary depending on your application's needs.\n",
      "\n",
      "For additional details about sampling and tool creation, you can refer to the [MCP Server Documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/server.md) and [MCP Client Documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md).\n",
      "\n",
      "Using FunctionModel, how can I mock LLM calls for unit testing a weather‑forecast agent?\n",
      "2025-10-04 01:10:59,122 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc5ebc933584029a1d51b77a089276c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:11:19,044 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To mock LLM calls for unit testing a weather-forecast agent using `FunctionModel`, you can follow these steps as illustrated in the relevant documentation:\n",
      "\n",
      "1. **Define a Mock Function**: Create a function that simulates the LLM call. This function will process incoming messages, extract necessary information (like location and date), and return mock responses.\n",
      "\n",
      "2. **Use `FunctionModel`**: Utilize `FunctionModel` to replace the agent's default model with your custom function. This allows you to control how the function behaves during the test, making it return predictable results.\n",
      "\n",
      "Here's an example implementation based on the documentation:\n",
      "\n",
      "```python\n",
      "import re\n",
      "import pytest\n",
      "from pydantic_ai import models\n",
      "from pydantic_ai import ModelMessage, ModelResponse, TextPart, ToolCallPart\n",
      "from pydantic_ai.models.function import FunctionModel, AgentInfo\n",
      "from fake_database import DatabaseConn\n",
      "from weather_app import run_weather_forecast, weather_agent\n",
      "\n",
      "pytestmark = pytest.mark.anyio\n",
      "models.ALLOW_MODEL_REQUESTS = False\n",
      "\n",
      "def call_weather_forecast(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n",
      "    if len(messages) == 1:\n",
      "        # First call, extract the location and date from the user prompt\n",
      "        user_prompt = messages[0].parts[-1]\n",
      "        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n",
      "        assert m is not None\n",
      "        args = {'location': 'London', 'forecast_date': m.group()}  \n",
      "        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n",
      "    else:\n",
      "        # Assuming this is the response from the weather forecast tool\n",
      "        msg = messages[-1].parts[0]\n",
      "        assert msg.part_kind == 'tool-return'\n",
      "        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n",
      "\n",
      "async def test_forecast_future():\n",
      "    conn = DatabaseConn()\n",
      "    user_id = 1\n",
      "    with weather_agent.override(model=FunctionModel(call_weather_forecast)):\n",
      "        prompt = 'What will the weather be like in London on 2032-01-01?'\n",
      "        await run_weather_forecast([(prompt, user_id)], conn)\n",
      "\n",
      "    forecast = await conn.get_forecast(user_id)\n",
      "    assert forecast == 'The forecast is: Rainy with a chance of sun'\n",
      "```\n",
      "\n",
      "### Key Components Explained:\n",
      "- **`call_weather_forecast` Function**: This mock function simulates the behavior of the weather forecasting tool by determining if it is the first call (where it extracts the location and date) or a response call (where it returns a pre-defined message).\n",
      "- **`FunctionModel` Override**: The agent’s model is temporarily replaced with `FunctionModel(call_weather_forecast)` which ensures that during the test, the function you defined will handle the calls instead of the actual LLM.\n",
      "  \n",
      "This approach allows for comprehensive testing of the weather forecasting functionality without invoking any real LLM calls, facilitating isolated unit testing. \n",
      "\n",
      "For more details, see the documentation [here](https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md).\n",
      "\n",
      "What’s the correct way to use StateDeps with a custom Pydantic BaseModel to maintain per‑request state in an AG‑UI front‑end?\n",
      "2025-10-04 01:11:20,424 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e84819c1924e2692ce417f04022504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:11:31,524 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To use `StateDeps` with a custom Pydantic `BaseModel` to maintain per-request state in an AG-UI front-end, you need to follow these steps:\n",
      "\n",
      "1. **Create a Pydantic BaseModel**: Define a subclass of `BaseModel` which will hold the state you wish to manage for each request. \n",
      "\n",
      "2. **Define Your Agent**: Set up your AG-UI agent with the `StateDeps` dependency type, specifying your custom `BaseModel`.\n",
      "\n",
      "3. **Create an ASGI Application**: Convert your agent to an ASGI application that can handle requests. This can be done using `agent.to_ag_ui()`.\n",
      "\n",
      "Here's a complete example based on the documentation:\n",
      "\n",
      "```python\n",
      "from pydantic import BaseModel\n",
      "from pydantic_ai import Agent\n",
      "from pydantic_ai.ag_ui import StateDeps\n",
      "\n",
      "class DocumentState(BaseModel):\n",
      "    \"\"\"State for the document being written.\"\"\"\n",
      "    document: str = ''\n",
      "\n",
      "# Create the agent with StateDeps for DocumentState\n",
      "agent = Agent(\n",
      "    'openai:gpt-4.1',\n",
      "    instructions='Be fun!',\n",
      "    deps_type=StateDeps[DocumentState],\n",
      ")\n",
      "\n",
      "# Create the ASGI application\n",
      "app = agent.to_ag_ui(deps=StateDeps(DocumentState()))\n",
      "```\n",
      "\n",
      "### Running Your Application\n",
      "You can run this ASGI application using a server like `uvicorn`:\n",
      "\n",
      "```bash\n",
      "uvicorn ag_ui_state:app --host 0.0.0.0 --port 9000\n",
      "```\n",
      "\n",
      "### Important Notes:\n",
      "- Your custom dependencies type needs to implement the `StateHandler` protocol, which means it should be a dataclass with a non-optional `state` field. This ensures that the state is isolated between requests.\n",
      "- If the `state` field is of a Pydantic `BaseModel` type, the raw state dictionary in the request is automatically validated against it.\n",
      "\n",
      "For further details on handling events and state updates, you might find it useful to refer to the AG-UI documentation: [AG-UI Documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md).\n",
      "\n",
      "How can I specify an output_type that includes several Pydantic models and a plain string, and what should I keep in mind for type‑checking?\n",
      "2025-10-04 01:11:32,783 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8a94c5c2a14a6582f211124f133040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 01:11:46,046 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "To specify an `output_type` that includes multiple Pydantic models and a plain string, you can use a union or a list for defining the output types when initializing the `Agent`. Here's how you can do it:\n",
      "\n",
      "### Example Code\n",
      "```python\n",
      "from pydantic import BaseModel\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "class Box(BaseModel):\n",
      "    width: int\n",
      "    height: int\n",
      "    depth: int\n",
      "    units: str\n",
      "\n",
      "# Initialize an Agent with output_type including both a Pydantic model and a string\n",
      "agent = Agent(\n",
      "    'openai:gpt-4o-mini',  # Specify your model\n",
      "    output_type=[Box, str],  # Specify the Pydantic model and string type\n",
      "    system_prompt=(\n",
      "        \"Extract the dimensions of a box. \"\n",
      "        \"If unable to extract all data, ask the user to try again.\"\n",
      "    ),\n",
      ")\n",
      "\n",
      "result = agent.run_sync('The box is 10x20x30')  # Example input\n",
      "print(result.output)  # Outputs result based on the input\n",
      "```\n",
      "\n",
      "### Type-Checking Considerations\n",
      "When using unions or lists with the `output_type`, keep the following in mind:\n",
      "\n",
      "1. **Union Types**: If you use a union of types like `output_type=Foo | Bar`, type checkers may not recognize this correctly until Python 3.15 (PEP-747). For compatibility:\n",
      "   - Consider using a list instead: `output_type=[Foo, Bar]`.\n",
      "   - If using a union, you might have to add `# type: ignore` to suppress type checker warnings.\n",
      "\n",
      "2. **Static Type Checkers**: Tools like Pyright generally handle these situations better than mypy. If you're using mypy, check its compatibility with your setup. You might need to specify generic parameters on the `Agent` constructor explicitly for clarity.\n",
      "\n",
      "3. **Single Element Objects**: If your output type schema is not of type `object` (e.g., `int`, `list[int]`), it will be wrapped in a single-element object.\n",
      "\n",
      "4. **Consistency in Output Types**: Ensure that you have consistent handling of output types throughout your application, especially when multiple types are expected.\n",
      "\n",
      "To summarize, specify your output types as shown, and be aware of the nuances involved with static type checking when dealing with unions and multiple types.\n",
      "\n",
      "For more context, you can refer to the detailed documentation in the following sources:\n",
      "- [Output Documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in tqdm(question_list.questions):\n",
    "    print(question)\n",
    "    result = await agent.run(user_prompt=question)\n",
    "    print(result.output)\n",
    "    log_interaction_to_file(agent, result.new_messages(), source='ai-generated')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae20ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
