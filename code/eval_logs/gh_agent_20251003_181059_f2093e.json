{
  "agent_name": "gh_agent",
  "system_prompt": [
    "\nYou are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the course materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/pydantic/pydantic-ai/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.\n"
  ],
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "hybrid_search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "Using FunctionModel, how can I mock LLM calls for unit testing a weather\u2011forecast agent?",
          "timestamp": "2025-10-03T18:10:56.848100+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the course materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/pydantic/pydantic-ai/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "hybrid_search",
          "args": "{\"query\":\"FunctionModel mock LLM calls unit testing weather-forecast agent\",\"num_results\":5}",
          "tool_call_id": "call_3xuVg1uoFa7apP0wBDc4NLLg",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 231,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 31,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T18:10:57+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMeX3RBD7oqlDxR3rQN7prju1LWCF",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "hybrid_search",
          "content": [
            {
              "start": 10000,
              "content": "tion': 'London', 'forecast_date': m.group()}  # (2)!\n        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # second call, return the forecast\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\n\nasync def test_forecast_future():\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  # (3)!\n        prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n```\n\n1. We define a function `call_weather_forecast` that will be called by `FunctionModel` in place of the LLM, this function has access to the list of [`ModelMessage`][pydantic_ai.messages.ModelMessage]s that make up the run, and [`AgentInfo`][pydantic_ai.models.function.AgentInfo] which contains information about the agent and the function tools and return tools.\n2. Our function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location.\n3. We use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to replace the agent's model with our custom function.\n\n### Overriding model via pytest fixtures\n\nIf you're writing lots of tests that all require model to be overridden, you can use [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html) to override the model with [`TestModel`][pydantic_ai.models.test.TestModel] or [`FunctionModel`][pydantic_ai.models.function.FunctionModel] in a reusable way.\n\nHere's an example of a fixture that overrides the model with `TestModel`:\n\n```python {title=\"test_agent.py\" requires=\"weather_app.py\"}\nimport pytest\n\nfrom pydantic_ai.models.test import TestModel\n\nfrom weather_app import weather_agent\n\n\n@pytest.fixture\ndef overr",
              "filename": "docs/testing.md",
              "final_rrf_score": 0.031746031746031744,
              "search_type": "hybrid"
            },
            {
              "start": 4000,
              "content": "les, gets a weather forecast for each prompt, and stores the result in the database.\n\n**We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.**\n\nHere's how we would write tests using [`TestModel`][pydantic_ai.models.test.TestModel]:\n\n```python {title=\"test_weather_app.py\" call_name=\"test_forecast\" requires=\"weather_app.py\"}\nfrom datetime import timezone\nimport pytest\n\nfrom dirty_equals import IsNow, IsStr\n\nfrom pydantic_ai import models, capture_run_messages, RequestUsage\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai import (\n    ModelResponse,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    ModelRequest,\n)\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio  # (1)!\nmodels.ALLOW_MODEL_REQUESTS = False  # (2)!\n\n\nasync def test_forecast():\n    conn = DatabaseConn()\n    user_id = 1\n    with capture_run_messages() as messages:\n        with weather_agent.override(model=TestModel()):  # (3)!\n            prompt = 'What will the weather be like in London on 2024-11-28?'\n            await run_weather_forecast([(prompt, user_id)], conn)  # (4)!\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == '{\"weather_forecast\":\"Sunny with a chance of rain\"}'  # (5)!\n\n    assert messages == [  # (6)!\n        ModelRequest(\n            parts=[\n                SystemPromptPart(\n                    content='Providing a weather forecast at the locations the user provides.',\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n                UserPromptPart(\n                    content='What will the weather be like in London on 2024-11-28?',\n                    timestamp=IsNow(tz=timezone.utc),  # (7)!\n                ),\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='",
              "filename": "docs/testing.md",
              "final_rrf_score": 0.031009615384615385,
              "search_type": "hybrid"
            },
            {
              "start": 8000,
              "content": "y aligned with the domain, you could add [`custom_output_text='Sunny'`][pydantic_ai.models.test.TestModel.custom_output_text] when defining `TestModel`.\n6. So far we don't actually know which tools were called and with which values, we can use [`capture_run_messages`][pydantic_ai.capture_run_messages] to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\n7. The [`IsNow`][dirty_equals.IsNow] helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\n8. `TestModel` isn't doing anything clever to extract values from the prompt, so these values are hardcoded.\n\n### Unit testing with `FunctionModel`\n\nThe above tests are a great start, but careful readers will notice that the `WeatherService.get_forecast` is never called since `TestModel` calls `weather_forecast` with a date in the past.\n\nTo fully exercise `weather_forecast`, we need to use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to customise how the tools is called.\n\nHere's an example of using `FunctionModel` to test the `weather_forecast` tool with custom inputs\n\n```python {title=\"test_weather_app2.py\" call_name=\"test_forecast_future\" requires=\"weather_app.py\"}\nimport re\n\nimport pytest\n\nfrom pydantic_ai import models\nfrom pydantic_ai import (\n    ModelMessage,\n    ModelResponse,\n    TextPart,\n    ToolCallPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\nmodels.ALLOW_MODEL_REQUESTS = False\n\n\ndef call_weather_forecast(  # (1)!\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    if len(messages) == 1:\n        # first call, call the weather forecast tool\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'loca",
              "filename": "docs/testing.md",
              "final_rrf_score": 0.030776515151515152,
              "search_type": "hybrid"
            },
            {
              "start": 9000,
              "content": "dels.function.FunctionModel] to customise how the tools is called.\n\nHere's an example of using `FunctionModel` to test the `weather_forecast` tool with custom inputs\n\n```python {title=\"test_weather_app2.py\" call_name=\"test_forecast_future\" requires=\"weather_app.py\"}\nimport re\n\nimport pytest\n\nfrom pydantic_ai import models\nfrom pydantic_ai import (\n    ModelMessage,\n    ModelResponse,\n    TextPart,\n    ToolCallPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\nmodels.ALLOW_MODEL_REQUESTS = False\n\n\ndef call_weather_forecast(  # (1)!\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    if len(messages) == 1:\n        # first call, call the weather forecast tool\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'location': 'London', 'forecast_date': m.group()}  # (2)!\n        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # second call, return the forecast\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\n\nasync def test_forecast_future():\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  # (3)!\n        prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n```\n\n1. We define a function `call_weather_forecast` that will be called by `FunctionModel` in place of the LLM, this function has access to the list of [`ModelMessage`][pydantic_ai.messages.ModelMessage]s that make up the run, and [`Ag",
              "filename": "docs/testing.md",
              "final_rrf_score": 0.030621785881252923,
              "search_type": "hybrid"
            },
            {
              "start": 7000,
              "content": "     input_tokens=77,\n                output_tokens=16,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n        ),\n    ]\n```\n\n1. We're using [anyio](https://anyio.readthedocs.io/en/stable/) to run async tests.\n2. This is a safety measure to make sure we don't accidentally make real requests to the LLM while testing, see [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS] for more details.\n3. We're using [`Agent.override`][pydantic_ai.agent.Agent.override] to replace the agent's model with [`TestModel`][pydantic_ai.models.test.TestModel], the nice thing about `override` is that we can replace the model inside agent without needing access to the agent `run*` methods call site.\n4. Now we call the function we want to test inside the `override` context manager.\n5. But default, `TestModel` will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add [`custom_output_text='Sunny'`][pydantic_ai.models.test.TestModel.custom_output_text] when defining `TestModel`.\n6. So far we don't actually know which tools were called and with which values, we can use [`capture_run_messages`][pydantic_ai.capture_run_messages] to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\n7. The [`IsNow`][dirty_equals.IsNow] helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\n8. `TestModel` isn't doing anything clever to extract values from the prompt, so these values are hardcoded.\n\n### Unit testing with `FunctionModel`\n\nThe above tests are a great start, but careful readers will notice that the `WeatherService.get_forecast` is never called since `TestModel` calls `weather_forecast` with a date in the past.\n\nTo fully exercise `weather_forecast`, we need to use [`FunctionModel`][pydantic_ai.mo",
              "filename": "docs/testing.md",
              "final_rrf_score": 0.02967032967032967,
              "search_type": "hybrid"
            }
          ],
          "tool_call_id": "call_3xuVg1uoFa7apP0wBDc4NLLg",
          "metadata": null,
          "timestamp": "2025-10-03T18:10:59.158949+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the course materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/pydantic/pydantic-ai/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "To mock LLM calls for unit testing a weather-forecast agent using `FunctionModel`, you can follow these steps as illustrated in the relevant documentation:\n\n1. **Define a Mock Function**: Create a function that simulates the LLM call. This function will process incoming messages, extract necessary information (like location and date), and return mock responses.\n\n2. **Use `FunctionModel`**: Utilize `FunctionModel` to replace the agent's default model with your custom function. This allows you to control how the function behaves during the test, making it return predictable results.\n\nHere's an example implementation based on the documentation:\n\n```python\nimport re\nimport pytest\nfrom pydantic_ai import models\nfrom pydantic_ai import ModelMessage, ModelResponse, TextPart, ToolCallPart\nfrom pydantic_ai.models.function import FunctionModel, AgentInfo\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\nmodels.ALLOW_MODEL_REQUESTS = False\n\ndef call_weather_forecast(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n    if len(messages) == 1:\n        # First call, extract the location and date from the user prompt\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'location': 'London', 'forecast_date': m.group()}  \n        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # Assuming this is the response from the weather forecast tool\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\nasync def test_forecast_future():\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):\n        prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n```\n\n### Key Components Explained:\n- **`call_weather_forecast` Function**: This mock function simulates the behavior of the weather forecasting tool by determining if it is the first call (where it extracts the location and date) or a response call (where it returns a pre-defined message).\n- **`FunctionModel` Override**: The agent\u2019s model is temporarily replaced with `FunctionModel(call_weather_forecast)` which ensures that during the test, the function you defined will handle the calls instead of the actual LLM.\n  \nThis approach allows for comprehensive testing of the weather forecasting functionality without invoking any real LLM calls, facilitating isolated unit testing. \n\nFor more details, see the documentation [here](https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 2999,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 649,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T18:10:59+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMeX5lyUuDfjwkXTgZuoCDrEp6hEq",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}