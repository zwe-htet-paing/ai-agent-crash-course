{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b0212b",
   "metadata": {},
   "source": [
    "# Day 3: Add Search\n",
    "\n",
    "https://docs.google.com/document/d/1aieUiF2oN8q3FCvAV7KEyC9HoPFxFIAHvUEnhALz8Og/edit?tab=t.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071e685",
   "metadata": {},
   "source": [
    "#### 1. Text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f303f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ingest import read_repo_data\n",
    "from utils.chunk import  sliding_window\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e80e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077b67a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'chunk': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       " 'title': 'Introduction',\n",
       " 'description': 'Example section for showcasing API endpoints',\n",
       " 'filename': 'docs-main/api-reference/introduction.mdx'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e77de8",
   "metadata": {},
   "source": [
    "Simple in-memory text search \n",
    "\n",
    "- [minisearch](https://github.com/alexeygrigorev/minsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e2caeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x77d62c7ec290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4720b7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'docs-main/api-reference/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Update label=\"2025-07-18\" description=\"Evidently v0.7.11\">\\n  ## **Evidently 0.7.11**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\\n\\nExample notebooks:\\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\\n\\n</Update>\\n\\n<Update label=\"2025-07-09\" description=\"Evidently v0.7.10\">\\n  ## **Evidently 0.7.10**\\n    Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\\n  \\nNEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/r',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'docs-main/changelog/changelog.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'ntly/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'docs-main/changelog/changelog.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'eleases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-0',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'docs-main/changelog/changelog.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool).\\n</Update>\\n\\n<Update label=\"2025-02-07\" description=\"Evidently v0.6.2\">\\n  ## **Evidently 0.6.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm` , so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations.\\n</Update>\\n\\n<Update label=\"2025-01-31\" description=\"Evidently v0.6.1\">\\n  ## **Evidently 0.6.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'docs-main/changelog/changelog.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '2-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool).\\n</Update>\\n\\n<Update label=\"2025-02-07\" description=\"Evidently v0.6.2\">\\n  ## **Evidently 0.6.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm` , so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations.\\n</Update>\\n\\n<Update label=\"2025-01-31\" description=\"Evidently v0.6.1\">\\n  ## **Evidently 0.6.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.1).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently v0.6\">\\n  ## **New API release**\\n\\n  The new API is available when you import modules from `evidently.future`. Read more in [Migration guide](/faq/migration). Release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.0).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently Cloud\">\\n  ## **Editable datasets**\\n\\n  You can now hit \"edit\" on any existing dataset, create a copy and add / delete rows and columns. Use it while working on your evaluation datasets or to leave comments on outputs.\\n\\n  ![](/images/changelog/editable_dataset-min.png)\\n</Update>\\n\\n<Update label=\"2025-01-10\" description=\"Docs\">\\n  ## **New Docs**\\n\\n  We are creating a new Docs website in anticipation of API change. You can still access old docs for information on earlier API and examples.\\n</Update>',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'docs-main/changelog/changelog.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to ',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'on\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/d',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated M',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'escriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping r',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'apping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<In',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'equired for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descripto',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'fo>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'rs=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classifi',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': '=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'cation=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': ' the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs-main/docs/library/data_definition.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'To evaluate text data, like LLM inputs and outputs, you create **Descriptors**. This is a universal interface for all evals - from text statistics to LLM judges.\\n\\nEach descriptor computes a score or label per row of your dataset. You can combine multiple descriptors and set optional pass/fail conditions. You can use built-in descriptors or create custom ones using LLM prompts or Python.\\n\\nFor a general introduction, check [Core Concepts](/docs/library/overview). You can also refer to the [LLM quickstart](quickstart_llm) for a minimal example.\\n\\n## Basic flow\\n\\n<Accordion title=\"Generate toy data\" defaultOpen={false}>\\n  Use this code snippet to create sample data for testing:\\n\\n  ```python\\n  import pandas as pd\\n  \\n  data = [\\n      [\"What is the chemical symbol for gold?\", \"The chemical symbol for gold is Au.\"],\\n      [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n      [\"Tell me a joke.\", \"Why don\\'t programmers like nature? It has too many bugs!\"],\\n      [\"What is the boiling point of water?\", \"The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit).\"],\\n      [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n      [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n      [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework. You might want to consult your teacher for help.\"],\\n      [\"How many states are there in the USA?\", \"There are 50 states in the USA.\"],\\n      [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n      [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n  ]\\n  \\n  # Columns\\n  columns = [\"question\", \"answer\"]\\n  \\n  # Creating the DataFrame\\n  df = pd.DataFrame(data, column',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'boiling point of water?\", \"The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit).\"],\\n      [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n      [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n      [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework. You might want to consult your teacher for help.\"],\\n      [\"How many states are there in the USA?\", \"There are 50 states in the USA.\"],\\n      [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n      [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n  ]\\n  \\n  # Columns\\n  columns = [\"question\", \"answer\"]\\n  \\n  # Creating the DataFrame\\n  df = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\n\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals\\n```\\n\\n**Note**. Some Descriptors (like `OOVWordsPercentage()` may require `nltk` dictionaries:\\n\\n```python\\nnltk.download(\\'words\\')\\nnltk.download(\\'wordnet\\')\\nnltk.download(\\'omw-1.4\\')\\nnltk.download(\\'vader_lexicon\\')\\n```\\n\\n**Step 2. Add descriptors** via the Dataset object. There are two ways to do this:\\n\\n- **Option A.** Simultaneously create the `Dataset` object and add descriptors to the selected columns (in this case, \"answer\" column).\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\'',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 's=columns)\\n  ```\\n</Accordion>\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\n\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals\\n```\\n\\n**Note**. Some Descriptors (like `OOVWordsPercentage()` may require `nltk` dictionaries:\\n\\n```python\\nnltk.download(\\'words\\')\\nnltk.download(\\'wordnet\\')\\nnltk.download(\\'omw-1.4\\')\\nnltk.download(\\'vader_lexicon\\')\\n```\\n\\n**Step 2. Add descriptors** via the Dataset object. There are two ways to do this:\\n\\n- **Option A.** Simultaneously create the `Dataset` object and add descriptors to the selected columns (in this case, \"answer\" column).\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n<Info>\\nRead more on how how to [create the Dataset and Data Definition](/docs/library/data_definition)\\n</Info>\\n\\n- **Option B.** Add descriptors to the existing Dataset using `add_descriptors`.\\n\\nFor example, first create the Dataset.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n)\\n```\\n\\nThen, add the scores to this Dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Sentiment(\"answer\", alias=\"Sentiment\"),\\n    TextLength(\"answer\", alias=\"Length\"),\\n    IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n])\\n```\\n\\n**Step 3. (Optional). Export results**. You can preview the DataFrame with results: \\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/metrics/descriptors-min.png)\\n\\n**Step 4. Get the Report**. This will summarize the results, capturing stats and distributions for all descriptors. The easiest way to get the Repor',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n<Info>\\nRead more on how how to [create the Dataset and Data Definition](/docs/library/data_definition)\\n</Info>\\n\\n- **Option B.** Add descriptors to the existing Dataset using `add_descriptors`.\\n\\nFor example, first create the Dataset.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n)\\n```\\n\\nThen, add the scores to this Dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Sentiment(\"answer\", alias=\"Sentiment\"),\\n    TextLength(\"answer\", alias=\"Length\"),\\n    IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n])\\n```\\n\\n**Step 3. (Optional). Export results**. You can preview the DataFrame with results: \\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/metrics/descriptors-min.png)\\n\\n**Step 4. Get the Report**. This will summarize the results, capturing stats and distributions for all descriptors. The easiest way to get the Report is through `TextEvals` Preset.\\n\\nTo configure and run the Report for the `eval_dataset`:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(eval_dataset)\\nmy_eval\\n\\n# my_eval.json()\\n# ws.add_report(project.id, my_eval, include_data=True)\\n```\\n\\nYou can view the Report in Python, export the outputs (HTML, JSON, Python dictionary) or upload it to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n\\n![](/images/metrics/descriptors-report.png)\\n\\n## Customizing descriptors\\n\\n<Tip>\\n  **All descriptors and parameters**. Evidently has multiple implemented descriptors, both deterministic and LLM-based. See a [reference table](/metrics/all_descriptors) with all descriptors and parameters.\\n</Tip>\\n\\n**Alias**. It is best to add an `alias` to each Descriptor to make it easier to reference. This name shows up in visualizations and column headers. It’s especially handy if you’re using checks like regular expressions with word lists, where the auto-generat',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 't is through `TextEvals` Preset.\\n\\nTo configure and run the Report for the `eval_dataset`:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(eval_dataset)\\nmy_eval\\n\\n# my_eval.json()\\n# ws.add_report(project.id, my_eval, include_data=True)\\n```\\n\\nYou can view the Report in Python, export the outputs (HTML, JSON, Python dictionary) or upload it to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n\\n![](/images/metrics/descriptors-report.png)\\n\\n## Customizing descriptors\\n\\n<Tip>\\n  **All descriptors and parameters**. Evidently has multiple implemented descriptors, both deterministic and LLM-based. See a [reference table](/metrics/all_descriptors) with all descriptors and parameters.\\n</Tip>\\n\\n**Alias**. It is best to add an `alias` to each Descriptor to make it easier to reference. This name shows up in visualizations and column headers. It’s especially handy if you’re using checks like regular expressions with word lists, where the auto-generated title could get very long.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    WordCount(\"answer\", alias=\"Words\"),\\n])\\n```\\n\\n**Descriptor parameters**. Some Descriptors have required parameters. For example, if you’re testing for competitor mentions using the `Contains` Descriptor, add the list of `items`:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Contains(\"answer\", items=[\"AcmeCorp\", \"YetAnotherCorp\"], alias=\"Competitors\")\\n])\\n```\\n\\nThese parameters are specific to each descriptors. Check the [reference table](/metrics/all_descriptors).\\n\\n**Multi-column descriptors**. Some evals use more than one column. For example, to match a new answer against reference, or measure semantic similarity. Pass both columns using parameters:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    SemanticSimilarity(columns=[\"question\", \"answer\"], alias=\"Semantic_Match\")\\n])\\n```\\n\\n**LLM-as-a-judge**. There are also built-in descriptors that prompt an external LLM to return an evaluat',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'ed title could get very long.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    WordCount(\"answer\", alias=\"Words\"),\\n])\\n```\\n\\n**Descriptor parameters**. Some Descriptors have required parameters. For example, if you’re testing for competitor mentions using the `Contains` Descriptor, add the list of `items`:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Contains(\"answer\", items=[\"AcmeCorp\", \"YetAnotherCorp\"], alias=\"Competitors\")\\n])\\n```\\n\\nThese parameters are specific to each descriptors. Check the [reference table](/metrics/all_descriptors).\\n\\n**Multi-column descriptors**. Some evals use more than one column. For example, to match a new answer against reference, or measure semantic similarity. Pass both columns using parameters:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    SemanticSimilarity(columns=[\"question\", \"answer\"], alias=\"Semantic_Match\")\\n])\\n```\\n\\n**LLM-as-a-judge**. There are also built-in descriptors that prompt an external LLM to return an evaluation score. You can add them like any other descriptor, but you must also provide an API key to use the corresponding LLM.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"answer\", alias=\"Contains_Denial\")\\n])\\n```\\n\\n<Tip>\\n  **Using and customizing LLM judge**. Check the [in-depth LLM judge guide](/metrics/customize_llm_judge) on using built-in and custom LLM-based evaluators.\\n</Tip>\\n\\n**Custom evals**. Beyond custom LLM judges, you can also implement your own programmatic evals as Python functions. Check the [custom descriptor guide](/metrics/customize_descriptor).\\n\\n## Adding Descriptor Tests\\n\\nDescriptor Tests let you define pass/fail checks for each row in your dataset. Instead of just calculating values (like “How long is this text?”), you can ask:\\n\\n- Is the text under 100 characters?\\n- Is the sentiment positive?\\n\\nYou can also combine multiple tests into a single summary result per row.\\n\\n**Step 1. Imports**. Run imports:\\n\\n```python\\nfrom evidently.descriptors impo',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'ion score. You can add them like any other descriptor, but you must also provide an API key to use the corresponding LLM.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"answer\", alias=\"Contains_Denial\")\\n])\\n```\\n\\n<Tip>\\n  **Using and customizing LLM judge**. Check the [in-depth LLM judge guide](/metrics/customize_llm_judge) on using built-in and custom LLM-based evaluators.\\n</Tip>\\n\\n**Custom evals**. Beyond custom LLM judges, you can also implement your own programmatic evals as Python functions. Check the [custom descriptor guide](/metrics/customize_descriptor).\\n\\n## Adding Descriptor Tests\\n\\nDescriptor Tests let you define pass/fail checks for each row in your dataset. Instead of just calculating values (like “How long is this text?”), you can ask:\\n\\n- Is the text under 100 characters?\\n- Is the sentiment positive?\\n\\nYou can also combine multiple tests into a single summary result per row.\\n\\n**Step 1. Imports**. Run imports:\\n\\n```python\\nfrom evidently.descriptors import ColumnTest, TestSummary\\nfrom evidently.tests import *\\n```\\n\\n**Step 2. Add tests to a descriptor**. When creating a descriptor (like `TextLength` or `Sentiment`), use the tests argument to set conditions. Each test adds a new column with a True/False result.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n    ]\\n)\\n```\\n\\nUse test parameters like `gte` (greater than or equal), `lte` (less than or equal), eq (equal). Check the [full list here](docs/library/tests#test-parameters).\\n\\nYou can preview the results with: `eval_dataset.as_dataframe()`:\\n![](/images/metrics/descriptors_tests-min.png)\\n\\n**Step 3. Add a Test Summary**. Use `TestSummary` to combine multiple tests into one or m',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'rt ColumnTest, TestSummary\\nfrom evidently.tests import *\\n```\\n\\n**Step 2. Add tests to a descriptor**. When creating a descriptor (like `TextLength` or `Sentiment`), use the tests argument to set conditions. Each test adds a new column with a True/False result.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n    ]\\n)\\n```\\n\\nUse test parameters like `gte` (greater than or equal), `lte` (less than or equal), eq (equal). Check the [full list here](docs/library/tests#test-parameters).\\n\\nYou can preview the results with: `eval_dataset.as_dataframe()`:\\n![](/images/metrics/descriptors_tests-min.png)\\n\\n**Step 3. Add a Test Summary**. Use `TestSummary` to combine multiple tests into one or more summary columns. For example, the following returns True if all tests pass:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\", tests=[\\n            eq(\"OK\", column=\"Denials\", alias=\"Is not a refusal\")]),\\n        TestSummary(success_all=True, alias=\"Test result\"), #returns True if all conditions are satisfied\\n    ]\\n)\\n```\\n\\n<Info>\\n  `TestSummary` will only consider tests added **before it** in the list of descriptors.\\n</Info>\\n\\n<Info>\\nFor LLM judge descriptors returning multiple columns (e.g., label and reasoning), you must specify the target column for the test — see `DeclineLLMEval` in the example.\\n</Info>\\n\\nYou can aggregate Tes',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'ore summary columns. For example, the following returns True if all tests pass:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\", tests=[\\n            eq(\"OK\", column=\"Denials\", alias=\"Is not a refusal\")]),\\n        TestSummary(success_all=True, alias=\"Test result\"), #returns True if all conditions are satisfied\\n    ]\\n)\\n```\\n\\n<Info>\\n  `TestSummary` will only consider tests added **before it** in the list of descriptors.\\n</Info>\\n\\n<Info>\\nFor LLM judge descriptors returning multiple columns (e.g., label and reasoning), you must specify the target column for the test — see `DeclineLLMEval` in the example.\\n</Info>\\n\\nYou can aggregate Test results differently and include multiple summary columns, such as total count, pass rate, or weighted score:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    TestSummary(\\n        success_all=True,     # True if all tests pass\\n        success_any=True,     # True if any test passes\\n        success_count=True,   # Total number of tests passed\\n        success_rate=True,    # Share of passed tests\\n        score=True,           # Weighted score\\n        score_weights={\\n            \"Sentiment is non-negative\": 0.9,\\n            \"Length is under 100\": 0.1,\\n        },\\n    )\\n])\\n```\\n\\n**Testing existing columns**. Use `ColumnTest` to apply checks to any column, even ones not generated by descriptors. This is useful for working with metadata or precomputed values:\\n\\n```python\\ndataset = Dataset.from_pandas(pd.DataFrame(data), descriptors=[\\n    ColumnTest(\"Feedback\", eq(\"Positive\")),\\n])\\n```\\n\\n## Summary Reports\\n\\nYou\\'ve already seen how to generate a report using the `TextEvals` preset. It\\'s t',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 't results differently and include multiple summary columns, such as total count, pass rate, or weighted score:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    TestSummary(\\n        success_all=True,     # True if all tests pass\\n        success_any=True,     # True if any test passes\\n        success_count=True,   # Total number of tests passed\\n        success_rate=True,    # Share of passed tests\\n        score=True,           # Weighted score\\n        score_weights={\\n            \"Sentiment is non-negative\": 0.9,\\n            \"Length is under 100\": 0.1,\\n        },\\n    )\\n])\\n```\\n\\n**Testing existing columns**. Use `ColumnTest` to apply checks to any column, even ones not generated by descriptors. This is useful for working with metadata or precomputed values:\\n\\n```python\\ndataset = Dataset.from_pandas(pd.DataFrame(data), descriptors=[\\n    ColumnTest(\"Feedback\", eq(\"Positive\")),\\n])\\n```\\n\\n## Summary Reports\\n\\nYou\\'ve already seen how to generate a report using the `TextEvals` preset. It\\'s the simplest and useful way to summarize evaluation results. However, you can also create custom reports using different metric combinations for more control.\\n\\n**Imports**. Import the components you\\'ll need:\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n```\\n\\n**Selecting a list of columns**. You can apply `TextEvals` to specific descriptors in your dataset. This makes your report more focused and lightweight.\\n\\n```python\\nreport = Report([\\n    TextEvals(columns=[\"Sentiment\", \"Length\", \"Test result\"])\\n])\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n**Custom Report with different Metrics**. Each Evidently Report is built from individual Metrics. For example, `TextEvals` internally uses `ValueStats` Metric for each descriptor. To customize the Report, you can reference specific descriptors and use metrics like `MeanValue`, `MaxValue`, etc:\\n\\n```python\\ncustom_report = Report([\\n    MeanValue(',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'he simplest and useful way to summarize evaluation results. However, you can also create custom reports using different metric combinations for more control.\\n\\n**Imports**. Import the components you\\'ll need:\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n```\\n\\n**Selecting a list of columns**. You can apply `TextEvals` to specific descriptors in your dataset. This makes your report more focused and lightweight.\\n\\n```python\\nreport = Report([\\n    TextEvals(columns=[\"Sentiment\", \"Length\", \"Test result\"])\\n])\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n**Custom Report with different Metrics**. Each Evidently Report is built from individual Metrics. For example, `TextEvals` internally uses `ValueStats` Metric for each descriptor. To customize the Report, you can reference specific descriptors and use metrics like `MeanValue`, `MaxValue`, etc:\\n\\n```python\\ncustom_report = Report([\\n    MeanValue(column=\"Length\"),\\n    MeanValue(column=\"Sentiment\")\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, None)\\nmy_custom_eval\\n```\\n\\n<Note>\\n  **List of all Metrics**. Check the [Reference table](/metrics/all_metrics). Consider using column-level Metrics like `MeanValue`, `MeanValue`, `MaxValue`, `QuantileValue`, `OutRangeValueCount` and `CategoryCount`.\\n</Note>\\n\\n**Drift detection**. You can also run advanced checks, like comparing distributions between two datasets, for example, to detect text length drift:\\n\\n```python\\ncustom_report = Report([\\n    ValueDrift(column=\"Length\"),\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, eval_dataset)\\nmy_custom_eval\\n```\\n\\n## Dataset-level Test Suites\\n\\nYou can also attach Tests to your Metrics to get pass/fail results at the **dataset** Report level. Example tests:\\n\\n- No response has sentiment \\\\< 0\\n- No response exceeds 150 characters\\n- No more than 10% of rows fail the summary test\\n\\n```python\\ntests = Report([\\n    MinValue(column=\"Sentiment\", tests=[',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'column=\"Length\"),\\n    MeanValue(column=\"Sentiment\")\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, None)\\nmy_custom_eval\\n```\\n\\n<Note>\\n  **List of all Metrics**. Check the [Reference table](/metrics/all_metrics). Consider using column-level Metrics like `MeanValue`, `MeanValue`, `MaxValue`, `QuantileValue`, `OutRangeValueCount` and `CategoryCount`.\\n</Note>\\n\\n**Drift detection**. You can also run advanced checks, like comparing distributions between two datasets, for example, to detect text length drift:\\n\\n```python\\ncustom_report = Report([\\n    ValueDrift(column=\"Length\"),\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, eval_dataset)\\nmy_custom_eval\\n```\\n\\n## Dataset-level Test Suites\\n\\nYou can also attach Tests to your Metrics to get pass/fail results at the **dataset** Report level. Example tests:\\n\\n- No response has sentiment \\\\< 0\\n- No response exceeds 150 characters\\n- No more than 10% of rows fail the summary test\\n\\n```python\\ntests = Report([\\n    MinValue(column=\"Sentiment\", tests=[gte(0)]),\\n    MaxValue(column=\"Length\", tests=[lte(150)]),\\n    CategoryCount(column=\"Test result\", category=False, share_tests=[lte(0.1)])\\n])\\n\\nmy_test_eval = tests.run(eval_dataset, None)\\nmy_test_eval\\n# my_test_eval.json()\\n```\\n\\nThis produces a Test Suite that shows clear pass/fail results for the overall dataset. This is useful for automated checks and regression testing.\\n\\n![](/images/metrics/descriptors-report-test.png)\\n\\n<Note>\\n  **Report and Tests API**. Check separate guides on [generating Reports](/docs/library/report) and setting [Test conditions](/docs/library/tests).\\n</Note>',\n",
       "  'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'filename': 'docs-main/docs/library/descriptors.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'This page shows the core eval workflow with the Evidently library and links to guides.\\n\\n## Define and run the eval\\n\\n<Tip>\\n  To log the evaluation results to the Evidently Platform, first connect to [Evidently Cloud](/docs/setup/cloud) or your [local workspace](/docs/setup/self-hosting) and [create a Project](/docs/platform/projects_manage). It\\'s optional: you can also run evals locally.\\n</Tip>\\n\\n<Steps>\\n  <Step title=\"Prepare the input data\">\\n    Get your data in a table like a `pandas.DataFrame`. More on [data requirements](/docs/library/overview#dataset). You can also [load data](/docs/platform/datasets_workflow) from Evidently Platform, like tracing or synthetic datasets.\\n  </Step>\\n\\n  <Step title=\"Create a Dataset object\">\\n    Create a Dataset object with `DataDefinition()` that specifies column role and types. You can also use default type detection. [How to set Data Definition](/docs/library/data_definition).\\n\\n    ```python\\n    eval_data = Dataset.from_pandas(\\n        source_df,\\n        data_definition=DataDefinition()\\n    )\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add descriptors\">\\n    For text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\\n\\n    ```python\\n    eval_data.add_descriptors(descriptors=[\\n        TextLength(\"Question\", alias=\"Length\"),\\n        Sentiment(\"Answer\", alias=\"Sentiment\")\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"Configure Report\">\\n    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset()\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Test conditions\">\\n    Add dataset-level Pass/Fail conditions, like to check if all texts are in \\\\< 100 symbols length. How to [configure Tests](/docs/library/tests).\\n\\n    ```pytho',\n",
       "  'title': 'Overview',\n",
       "  'description': 'End-to-end evaluation workflow.',\n",
       "  'filename': 'docs-main/docs/library/evaluations_overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '      data_definition=DataDefinition()\\n    )\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add descriptors\">\\n    For text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\\n\\n    ```python\\n    eval_data.add_descriptors(descriptors=[\\n        TextLength(\"Question\", alias=\"Length\"),\\n        Sentiment(\"Answer\", alias=\"Sentiment\")\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"Configure Report\">\\n    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset()\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Test conditions\">\\n    Add dataset-level Pass/Fail conditions, like to check if all texts are in \\\\< 100 symbols length. How to [configure Tests](/docs/library/tests).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset(),\\n        MaxValue(column=\"Length\", tests=[lt(100)]),\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Tags and Timestamps\">\\n    Add `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp `. [How to add metadata](/docs/library/tags_metadata).\\n  </Step>\\n\\n  <Step title=\"Run the Report\">\\n    To execute the eval, `run`the Report on the `Dataset` (or two).\\n\\n    ```python\\n    my_eval = report.run(eval_data, None)\\n    ```\\n  </Step>\\n\\n  <Step title=\"Explore the results\">\\n    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).\\n\\n    ```python\\n    ws.add_run(project.id, my_eval, include_data=True)\\n    ```\\n\\n    * To view locally. [All output formats](/docs/library/output_formats).\\n\\n    ```python\\n    my_eval\\n    ##my_eval.json()\\n    ```\\n  </Step>\\n</Steps>\\n\\n## Quickstarts\\n\\nCheck for end-to-end examples:\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM quickstart\" ',\n",
       "  'title': 'Overview',\n",
       "  'description': 'End-to-end evaluation workflow.',\n",
       "  'filename': 'docs-main/docs/library/evaluations_overview.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'n\\n    report = Report([\\n        DataSummaryPreset(),\\n        MaxValue(column=\"Length\", tests=[lt(100)]),\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Tags and Timestamps\">\\n    Add `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp `. [How to add metadata](/docs/library/tags_metadata).\\n  </Step>\\n\\n  <Step title=\"Run the Report\">\\n    To execute the eval, `run`the Report on the `Dataset` (or two).\\n\\n    ```python\\n    my_eval = report.run(eval_data, None)\\n    ```\\n  </Step>\\n\\n  <Step title=\"Explore the results\">\\n    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).\\n\\n    ```python\\n    ws.add_run(project.id, my_eval, include_data=True)\\n    ```\\n\\n    * To view locally. [All output formats](/docs/library/output_formats).\\n\\n    ```python\\n    my_eval\\n    ##my_eval.json()\\n    ```\\n  </Step>\\n</Steps>\\n\\n## Quickstarts\\n\\nCheck for end-to-end examples:\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM quickstart\" icon=\"comment-text\" href=\"/quickstart_llm\">\\n    Evaluate the quality of text outputs.\\n  </Card>\\n\\n  <Card title=\"ML quickstart\" icon=\"table\" href=\"/quickstart_ml\">\\n    Test tabular data quality and data drift.\\n  </Card>\\n</CardGroup>',\n",
       "  'title': 'Overview',\n",
       "  'description': 'End-to-end evaluation workflow.',\n",
       "  'filename': 'docs-main/docs/library/evaluations_overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Use for relevant pages after features are implemented.\\n\\n# Metrics\\n\\n## Correlations\\n\\nUse for exploratory data analysis, drift monitoring (correlation changes) or to check alignment between scores (e.g. LLM-based descriptors against human labels).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types.\\n</Info>\\n\\nColumn data quality\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------\\n| **RowsWithMissingValuesCount()**  (Coming soon) | <ul><li> Dataset-level.</li><li>Counts rows with missing values.</li><li>Metric result: `value`.</li></ul>                                                                             | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one row with missing values.</li><li>**With reference**: Fails if share differs by',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------\\n| **RowsWithMissingValuesCount()**  (Coming soon) | <ul><li> Dataset-level.</li><li>Counts rows with missing values.</li><li>Metric result: `value`.</li></ul>                                                                             | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one row with missing values.</li><li>**With reference**: Fails if share differs by >10% (+/-)</li></ul>           |\\n| **AlmostEmptyColumnCount()**  (Coming soon)     | <ul><li> Dataset-level.</li><li>Counts almost empty columns (95% empty).</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>        |\\n| **NewCategoriesCount()** (Coming soon)                                                                                         | <ul><li>Column-level.</li><li>Counts new categories compared to reference (reference required).</li><li>Metric result: `count`, `share`.</li></ul>   | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.          ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': ' >10% (+/-)</li></ul>           |\\n| **AlmostEmptyColumnCount()**  (Coming soon)     | <ul><li> Dataset-level.</li><li>Counts almost empty columns (95% empty).</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>        |\\n| **NewCategoriesCount()** (Coming soon)                                                                                         | <ul><li>Column-level.</li><li>Counts new categories compared to reference (reference required).</li><li>Metric result: `count`, `share`.</li></ul>   | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |\\n| **MissingCategoriesCount()**  (Coming soon)                                                                                    | <ul><li>Column-level.</li><li>Counts missing categories compared to reference.</li><li>Metric result: `count`, `share`.</li></ul>                    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |\\n| **MostCommonValueCount()** (Coming soon)                                                                                       | <ul><li>Column-level.</li><li>Identifies the most common value and provides its count/share.</li><li>Metric result: `value: count, ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': '                                                                                                                                                |\\n| **MissingCategoriesCount()**  (Coming soon)                                                                                    | <ul><li>Column-level.</li><li>Counts missing categories compared to reference.</li><li>Metric result: `count`, `share`.</li></ul>                    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |\\n| **MostCommonValueCount()** (Coming soon)                                                                                       | <ul><li>Column-level.</li><li>Identifies the most common value and provides its count/share.</li><li>Metric result: `value: count, share`.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if most common value share is ≥80%.</li><li>**With reference**:  Fails if most common value share differs by >10% (+/-).</li></ul> |\\n\\nDrift\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | ---------------------------------------------',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'share`.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if most common value share is ≥80%.</li><li>**With reference**:  Fails if most common value share differs by >10% (+/-).</li></ul> |\\n\\nDrift\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- \\n| **EmbeddingDrift()** (Coming soon)    | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for embeddings.</li><li>Requires embedding columns set in data definition.</li><li>Metric result: `value`.</li></ul>                                                                                                                                      | **Required**: <ul><li>`embeddings`</li><li>`method`</li></ul> See [embedding drift options](/metrics/customize_embedding_drift).              ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': '------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- \\n| **EmbeddingDrift()** (Coming soon)    | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for embeddings.</li><li>Requires embedding columns set in data definition.</li><li>Metric result: `value`.</li></ul>                                                                                                                                      | **Required**: <ul><li>`embeddings`</li><li>`method`</li></ul> See [embedding drift options](/metrics/customize_embedding_drift).                                                                                                                                                       | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_embedding_drift).</li></ul>                            |\\n| **MultivariateDrift()** (Coming soon) | <ul><li>Dataset-level.</li><li> Requires reference.</li><li>Computes a single dataset drift score.</li><li>Default method: share of drifted columns.</li><li>Metric result: `value`.</li></ul>                                                                                                                                             | **Optional**: <ul><li>`columns`</li><li>`method`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                                                          | <ul><li>**With reference**: Defaults for method. See [methods](/me',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': '                                                                                                                                         | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_embedding_drift).</li></ul>                            |\\n| **MultivariateDrift()** (Coming soon) | <ul><li>Dataset-level.</li><li> Requires reference.</li><li>Computes a single dataset drift score.</li><li>Default method: share of drifted columns.</li><li>Metric result: `value`.</li></ul>                                                                                                                                             | **Optional**: <ul><li>`columns`</li><li>`method`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                                                          | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_data_drift).             </li></ul>                    |\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'trics/customize_data_drift).             </li></ul>                    |\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\\n| **DatasetCorrelations()** (Coming soon) | <ul><li>Calculates the correlations between all or set columns in the dataset.</li><li>Supported methods: Pearson, Spearman, Kendall, Cramer\\\\_V.</li></ul>                                                | **Optional**: <ul><li>`columns`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                          | N/A                                                                                            |\\n| **Correlation()** (Coming soon)         | <ul><li>Calculates the correlation between two defined columns.</li></ul>                                                                                          ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': '------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- |\\n| **DatasetCorrelations()** (Coming soon) | <ul><li>Calculates the correlations between all or set columns in the dataset.</li><li>Supported methods: Pearson, Spearman, Kendall, Cramer\\\\_V.</li></ul>                                                | **Optional**: <ul><li>`columns`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                          | N/A                                                                                            |\\n| **Correlation()** (Coming soon)         | <ul><li>Calculates the correlation between two defined columns.</li></ul>                                                                                                                                 | **Required**: <ul><li>`column_x`</li><li>`column_y`</li></ul>**Optional**:<ul><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>[Test conditions](/docs/library/tests)</li></ul> | N/A                                                                                            |\\n| **CorrelationChanges()** (Coming soon)  | <ul><li>Dataset-level.</li><li>Reference required.</li><li>Checks the number of correlation violations (significant changes in correlation strength between columns) across all or set columns.</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>`corr_diff` (default: 0.25)</li><li>[Test conditions](/docs/library/tests)</li></ul>       | <ul><li>**With reference**: Fails if at least one correlation violation is detected.</li></ul> |\\n\\n\\nClassification\\n\\n| Metric                              | Des',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '                                       | **Required**: <ul><li>`column_x`</li><li>`column_y`</li></ul>**Optional**:<ul><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>[Test conditions](/docs/library/tests)</li></ul> | N/A                                                                                            |\\n| **CorrelationChanges()** (Coming soon)  | <ul><li>Dataset-level.</li><li>Reference required.</li><li>Checks the number of correlation violations (significant changes in correlation strength between columns) across all or set columns.</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>`corr_diff` (default: 0.25)</li><li>[Test conditions](/docs/library/tests)</li></ul>       | <ul><li>**With reference**: Fails if at least one correlation violation is detected.</li></ul> |\\n\\n\\nClassification\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n|',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'cription                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **LabelCount()** (Coming soon) | <ul><li>Distribution of predicted classes.</li><li>Can visualize class balance and/or probability distribution.</li></ul>                                                   | **Required**: <ul><li>Set at least one visualization: `class_balance`, `prob_distribution`.</li></ul>  **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                    | N/A                                                                                                                                                                                  |\\n| **Lift()**  (Coming soon)      | <ul><li>Calculates lift.</li><li>Can visualize lift curve or table.</li><li>Metric result: `value`.</li></ul>                                                               | **Required**: <ul><li>Set at least one visualization: `lift_table`, `lift_curve`.</li></ul> **Optional**: <ul><li>`probas_thr',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': ' **LabelCount()** (Coming soon) | <ul><li>Distribution of predicted classes.</li><li>Can visualize class balance and/or probability distribution.</li></ul>                                                   | **Required**: <ul><li>Set at least one visualization: `class_balance`, `prob_distribution`.</li></ul>  **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                    | N/A                                                                                                                                                                                  |\\n| **Lift()**  (Coming soon)      | <ul><li>Calculates lift.</li><li>Can visualize lift curve or table.</li><li>Metric result: `value`.</li></ul>                                                               | **Required**: <ul><li>Set at least one visualization: `lift_table`, `lift_curve`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                    | N/A                                                                                                                                                                                  |\\n\\nRecsys\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type. Some metrics require additional training data.\\n</Info>\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                               ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': 'eshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                    | N/A                                                                                                                                                                                  |\\n\\nRecsys\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type. Some metrics require additional training data.\\n</Info>\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RecSysPreset()**                  | <ul><li>Larget Preset. </li><li>Includes a range of recommendation system metrics.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_recsys).</li></ul> | None.                                                                                                                                                          | As in individual metrics.                         ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': '                                            |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RecSysPreset()**                  | <ul><li>Larget Preset. </li><li>Includes a range of recommendation system metrics.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_recsys).</li></ul> | None.                                                                                                                                                          | As in individual metrics.                                                                                                               |\\n| **Personalization()** (Coming soon) | <ul><li>Calculates Personalization score at the top K recommendations.</li><li>Metric result: `value`.</li></ul>                                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Personalization > 0.</li><li>**With reference**: Fails if Personalization differs by >10%.</li></ul> |\\n| **ARP()** (Coming soon)             | <ul><li>Computes Average Recommendation Popularity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`normalize_arp` (default: `False`)</li><li>[Test conditions](/docs/library/tests)</li></ul>          | <ul><li>',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': '                                                                                      |\\n| **Personalization()** (Coming soon) | <ul><li>Calculates Personalization score at the top K recommendations.</li><li>Metric result: `value`.</li></ul>                                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Personalization > 0.</li><li>**With reference**: Fails if Personalization differs by >10%.</li></ul> |\\n| **ARP()** (Coming soon)             | <ul><li>Computes Average Recommendation Popularity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`normalize_arp` (default: `False`)</li><li>[Test conditions](/docs/library/tests)</li></ul>          | <ul><li>**No reference**: Tests if ARP > 0.</li><li>**With reference**: Fails if ARP differs by >10%.</li></ul>                         |\\n| **Coverage()**(Coming soon)         | <ul><li>Calculates Coverage at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                            | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Coverage > 0.</li><li>**With reference**: Fails if Coverage differs by >10%.</li></ul>               |\\n| **GiniIndex()**(Coming soon)        | <ul><li>Calculates Gini Index at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                          | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                     ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': '**No reference**: Tests if ARP > 0.</li><li>**With reference**: Fails if ARP differs by >10%.</li></ul>                         |\\n| **Coverage()**(Coming soon)         | <ul><li>Calculates Coverage at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                            | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Coverage > 0.</li><li>**With reference**: Fails if Coverage differs by >10%.</li></ul>               |\\n| **GiniIndex()**(Coming soon)        | <ul><li>Calculates Gini Index at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                          | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Gini Index \\\\< 1.</li><li>**With reference**: Fails if Gini Index differs by >10%.</li></ul>          |\\n| **Diversity()**  (Coming soon)      | <ul><li>Calculates Diversity at the top K recommendations.</li><li>Requires item features.</li><li>Metric result: `value`.</li></ul>                                                | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                             | <ul><li>**No reference**: Tests if Diversity > 0.</li><li>**With reference**: Fails if Diversity differs by >10%.</li></ul>             |\\n| **Serendipity()**(Coming soon)      | <ul><li>Calculates Serendipity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                         | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>`mi',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': '                                | <ul><li>**No reference**: Tests if Gini Index \\\\< 1.</li><li>**With reference**: Fails if Gini Index differs by >10%.</li></ul>          |\\n| **Diversity()**  (Coming soon)      | <ul><li>Calculates Diversity at the top K recommendations.</li><li>Requires item features.</li><li>Metric result: `value`.</li></ul>                                                | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                             | <ul><li>**No reference**: Tests if Diversity > 0.</li><li>**With reference**: Fails if Diversity differs by >10%.</li></ul>             |\\n| **Serendipity()**(Coming soon)      | <ul><li>Calculates Serendipity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                         | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul>     | <ul><li>**No reference**: Tests if Serendipity > 0.</li><li>**With reference**: Fails if Serendipity differs by >10%.</li></ul>         |\\n| **Novelty()**  (Coming soon)        | <ul><li>Calculates Novelty at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Novelty > 0.</li><li>**With reference**: Fails if Novelty differs by >10%.</li></ul>                 |\\n\\nRelevant for RecSys metrics:\\n\\n* `no_feedback_user: bool = False`. Specifies whether to include the users who did not select any of the items, when computing the quality metric. Default: False.\\n\\n* `min_rel_score: Optional[int] = None`. Specifies the minimum relevan',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': \"n_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul>     | <ul><li>**No reference**: Tests if Serendipity > 0.</li><li>**With reference**: Fails if Serendipity differs by >10%.</li></ul>         |\\n| **Novelty()**  (Coming soon)        | <ul><li>Calculates Novelty at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Novelty > 0.</li><li>**With reference**: Fails if Novelty differs by >10%.</li></ul>                 |\\n\\nRelevant for RecSys metrics:\\n\\n* `no_feedback_user: bool = False`. Specifies whether to include the users who did not select any of the items, when computing the quality metric. Default: False.\\n\\n* `min_rel_score: Optional[int] = None`. Specifies the minimum relevance score to consider relevant when calculating the quality metrics for non-binary targets (e.g., if a target is a rating or a custom score).\\n\\n# Ranking metrics explainers\\n\\n### Diversity\\n\\n**Evidently Metric**: `Diversity`\\n\\n**Recommendation diversity**: this metric measures the average intra-list diversity at K. It reflects the variety of items within the same user's recommendation list, averaged by all users. \\n\\n**Implemented method**:\\n* **Measure the difference between recommended items**. Calculate the Cosine distance for each pair of recommendations inside the top-K in each user's list. The cosine distance serves as a measure of diversity between vectors representing recommended items, and is computed as:\\n\\n$$\\\\text{Cosine distance} = 1 - \\\\text{Cosine Similarity}$$\\n\\nLink: [Cosine Similarity on Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity). \\n\\n* **Intra-list diversity**. Calculate intra-list diversity for each user by averaging the Cosine Distance between each pair of items \",\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': \"ce score to consider relevant when calculating the quality metrics for non-binary targets (e.g., if a target is a rating or a custom score).\\n\\n# Ranking metrics explainers\\n\\n### Diversity\\n\\n**Evidently Metric**: `Diversity`\\n\\n**Recommendation diversity**: this metric measures the average intra-list diversity at K. It reflects the variety of items within the same user's recommendation list, averaged by all users. \\n\\n**Implemented method**:\\n* **Measure the difference between recommended items**. Calculate the Cosine distance for each pair of recommendations inside the top-K in each user's list. The cosine distance serves as a measure of diversity between vectors representing recommended items, and is computed as:\\n\\n$$\\\\text{Cosine distance} = 1 - \\\\text{Cosine Similarity}$$\\n\\nLink: [Cosine Similarity on Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity). \\n\\n* **Intra-list diversity**. Calculate intra-list diversity for each user by averaging the Cosine Distance between each pair of items in the user's top-K list.\\n* **Overall diversity**. Calculate the overall diversity by averaging the intra-list diversity across all users.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n**0:** identical recommendations in top-K.\\n**2:** very diverse recommendations in top-K.\\n\\n**Interpretation**: the higher the value, the more varied items are shown to each user (e.g. inside a single recommendation block).\\n\\n**Requirements**: You must pass the `item_features` list to point to numerical columns or embeddings that describe the recommended items. For example, these could be encoded genres that represent each movie. This makes it possible to compare the degree of similarity between different items. \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing varied but irrelevant items will have high diversity.\\n* This method performs many pairwise calculations between items and can take some time to compute.\\n  \\n### Novelty\\n\\n**Evi\",\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': \"in the user's top-K list.\\n* **Overall diversity**. Calculate the overall diversity by averaging the intra-list diversity across all users.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n**0:** identical recommendations in top-K.\\n**2:** very diverse recommendations in top-K.\\n\\n**Interpretation**: the higher the value, the more varied items are shown to each user (e.g. inside a single recommendation block).\\n\\n**Requirements**: You must pass the `item_features` list to point to numerical columns or embeddings that describe the recommended items. For example, these could be encoded genres that represent each movie. This makes it possible to compare the degree of similarity between different items. \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing varied but irrelevant items will have high diversity.\\n* This method performs many pairwise calculations between items and can take some time to compute.\\n  \\n### Novelty\\n\\n**Evidently Metric**: `Novelty`\\n\\n**Recommendation novelty**: this metric measures the average novelty of recommendations at K. It reflects how unusual top-K items are shown to each user, averaged by all users. \\n\\n**Implemented method**:\\n* Measure **novelty of recommended items**. The novelty of an item can be defined based on its popularity in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2(p_i)$$\\nwhere *p* represents the probability that item *i* is observed. It is calculated as the share of users that interacted with an item in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2\\\\left(\\\\frac{\\\\text{users who interacted with } i}{\\\\text{number of users}}\\\\right)$$\\n\\nHigh novelty corresponds to long-tail items that few users interacted with, and low novelty values correspond to popular items. If all users had interacted with an item, novelty is 0.\\n* Measure **novelty by user**. For each user, compute the average item novelty at K, by summing up the novelty of all items and dividing by K.\\n* **Overall n\",\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': 'dently Metric**: `Novelty`\\n\\n**Recommendation novelty**: this metric measures the average novelty of recommendations at K. It reflects how unusual top-K items are shown to each user, averaged by all users. \\n\\n**Implemented method**:\\n* Measure **novelty of recommended items**. The novelty of an item can be defined based on its popularity in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2(p_i)$$\\nwhere *p* represents the probability that item *i* is observed. It is calculated as the share of users that interacted with an item in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2\\\\left(\\\\frac{\\\\text{users who interacted with } i}{\\\\text{number of users}}\\\\right)$$\\n\\nHigh novelty corresponds to long-tail items that few users interacted with, and low novelty values correspond to popular items. If all users had interacted with an item, novelty is 0.\\n* Measure **novelty by user**. For each user, compute the average item novelty at K, by summing up the novelty of all items and dividing by K.\\n* **Overall novelty**. Average the novelty by user across all users.\\n\\n**Range**: 0 to infinity. \\n\\n**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the recommended items are well-known.   \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing many irrelevant but unexpected (long tail) items will have high novelty. \\n* It is not possible to define the novelty of an item absent in the training set. The evaluation only considers items that are present in training. \\n\\nFurther reading: [Castells, P., Vargas, S., & Wang, J. (2011). Novelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance](https://repositorio.uam.es/bitstream/handle/10486/666094/novelty_castells_DDR_2011.pdf)\\n\\n### Serendipity\\n\\n**Evidently Metric**: `Serendipity`\\n\\nRecommendation serendipity: this metric measures how unusual the relevant recommendations are in K, averaged for all users. \\n\\nSerendipity combines unexpectedness',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 21000,\n",
       "  'chunk': 'ovelty**. Average the novelty by user across all users.\\n\\n**Range**: 0 to infinity. \\n\\n**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the recommended items are well-known.   \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing many irrelevant but unexpected (long tail) items will have high novelty. \\n* It is not possible to define the novelty of an item absent in the training set. The evaluation only considers items that are present in training. \\n\\nFurther reading: [Castells, P., Vargas, S., & Wang, J. (2011). Novelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance](https://repositorio.uam.es/bitstream/handle/10486/666094/novelty_castells_DDR_2011.pdf)\\n\\n### Serendipity\\n\\n**Evidently Metric**: `Serendipity`\\n\\nRecommendation serendipity: this metric measures how unusual the relevant recommendations are in K, averaged for all users. \\n\\nSerendipity combines unexpectedness and relevance. It reflects the ability of a recommender system to show relevant items (that get a positive ranking or action) that are unexpected in the context of the user history (= are not similar to previous interactions). For example, a user who usually likes comedies gets recommended and upvotes a thriller.\\n\\n**Implemented method**. \\n* Measure the **unexpectedness** of relevant recommendations. The “unexpectedness” is measured using Cosine distance. For every relevant recommendation in top-K, we compute the distance between this item and the previous user interactions in the training set. Higher cosine distance indicates higher unexpectedness.\\n\\n$$\\\\text{serendipity}_i = \\\\text{unexpectedness}_i\\\\times\\\\text{relevance}_i$$\\n\\nWhere *relevance(i)* is equal to 1 if the item is relevant, and is 0 otherwise.\\n* **Serendipity by user**. Calculate the average of the resulting distances for all relevant recommendations in the user list.  \\n* **Overall serendipity**. Calculate the overall recomme',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 22000,\n",
       "  'chunk': ' and relevance. It reflects the ability of a recommender system to show relevant items (that get a positive ranking or action) that are unexpected in the context of the user history (= are not similar to previous interactions). For example, a user who usually likes comedies gets recommended and upvotes a thriller.\\n\\n**Implemented method**. \\n* Measure the **unexpectedness** of relevant recommendations. The “unexpectedness” is measured using Cosine distance. For every relevant recommendation in top-K, we compute the distance between this item and the previous user interactions in the training set. Higher cosine distance indicates higher unexpectedness.\\n\\n$$\\\\text{serendipity}_i = \\\\text{unexpectedness}_i\\\\times\\\\text{relevance}_i$$\\n\\nWhere *relevance(i)* is equal to 1 if the item is relevant, and is 0 otherwise.\\n* **Serendipity by user**. Calculate the average of the resulting distances for all relevant recommendations in the user list.  \\n* **Overall serendipity**. Calculate the overall recommendation serendipity by averaging the results across all users.\\n\\n$$\\\\text{Serendipity} = 1 - \\\\sum_{u \\\\in S} \\\\frac{1}{|S| |H_u|} \\\\sum_{h \\\\in H_u} \\\\sum_{i \\\\in R_{u,k}} \\\\frac{\\\\text{CosSim}(i, h)}{k}$$\\n\\nWhere\\n* *S* is the set of all users.\\n* *H(u)* is the item history of user *u*.\\n* *R(u)* Top-K function, where *R(u,k)* gives the top *k* recommended items for user *u*.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n* **0**: only popular, expected relevant recommendations.\\n* **2**: completely unexpected relevant recommendations.\\n \\n**Interpretation**: the higher the value, the better the ability of the system to “positively surprise” the user. \\n\\n**Requirements**: You must pass the `item_features` list to point to the numerical columns or embeddings that describe the recommended items. This allows comparing the degree of similarity between recommended items.\\n\\n**Notes**: \\n* This metric is only computed for the users that are present in the training set. If ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 23000,\n",
       "  'chunk': \"ndation serendipity by averaging the results across all users.\\n\\n$$\\\\text{Serendipity} = 1 - \\\\sum_{u \\\\in S} \\\\frac{1}{|S| |H_u|} \\\\sum_{h \\\\in H_u} \\\\sum_{i \\\\in R_{u,k}} \\\\frac{\\\\text{CosSim}(i, h)}{k}$$\\n\\nWhere\\n* *S* is the set of all users.\\n* *H(u)* is the item history of user *u*.\\n* *R(u)* Top-K function, where *R(u,k)* gives the top *k* recommended items for user *u*.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n* **0**: only popular, expected relevant recommendations.\\n* **2**: completely unexpected relevant recommendations.\\n \\n**Interpretation**: the higher the value, the better the ability of the system to “positively surprise” the user. \\n\\n**Requirements**: You must pass the `item_features` list to point to the numerical columns or embeddings that describe the recommended items. This allows comparing the degree of similarity between recommended items.\\n\\n**Notes**: \\n* This metric is only computed for the users that are present in the training set. If there is no previous recommendation history, these users will be ignored. \\n* This metric only considers the unexpectedness of relevant items in top-K. Irrelevant recommendations, and their share, are not taken into account.\\n\\nFurther reading: [Zhang, Y., Séaghdha, D., Quercia, D., Jambor, T. (2011). Auralist: introducing serendipity into music recommendation.](http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf)\\n\\n### Personalization\\n\\n**Evidently Metric**: `Personalization`\\n\\nPersonalization of recommendations: this metric measures the average uniqueness of each user's recommendations in top-K.\\n\\n**Implemented method**:\\n* For every two users, compute the **overlap between top-K recommended items**. (The number of common items in top-K between two lists, divided by K).\\n* Calculate the **average overlap** across all pairs of users.\\n* Calculate personalization as: \\n\\n$$\\\\text{Personalization} = 1 - \\\\text{average overlap}$$\\n\\nThe resulting metric reflects the average sha\",\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 24000,\n",
       "  'chunk': \"there is no previous recommendation history, these users will be ignored. \\n* This metric only considers the unexpectedness of relevant items in top-K. Irrelevant recommendations, and their share, are not taken into account.\\n\\nFurther reading: [Zhang, Y., Séaghdha, D., Quercia, D., Jambor, T. (2011). Auralist: introducing serendipity into music recommendation.](http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf)\\n\\n### Personalization\\n\\n**Evidently Metric**: `Personalization`\\n\\nPersonalization of recommendations: this metric measures the average uniqueness of each user's recommendations in top-K.\\n\\n**Implemented method**:\\n* For every two users, compute the **overlap between top-K recommended items**. (The number of common items in top-K between two lists, divided by K).\\n* Calculate the **average overlap** across all pairs of users.\\n* Calculate personalization as: \\n\\n$$\\\\text{Personalization} = 1 - \\\\text{average overlap}$$\\n\\nThe resulting metric reflects the average share of unique recommendations in each user’s list.\\n\\n**Range**: 0 to 1.\\n* **0**: Identical recommendations for each user in top-K. \\n* **1**: Each user’s recommendations in top-K are unique.   \\n\\n**Interpretation**: the higher the value, the more personalized (= different from others) is each user’s list. The metric visualization also shows the top-10 most popular items.\\n\\n### Average Recommendation Popularity (ARP)\\n\\n**Evidently Metric**: `ARP`\\n\\nThe recommendation popularity bias is a tendency to favor a few popular items. \\n\\nARP reflects the average popularity of the items recommended to the users. \\n\\n**Implementation**.\\n* Compute the item popularity as the number of times each item was seen in training. \\n* Compute the average popularity for each user’s list as a sum of all items’ popularity divided by the number of recommended items.\\n* Compute the average popularity for all users by averaging the results across all users.\\n\\n$$ARP = \\\\frac{1}{|U|} \\\\sum_{u \\\\in U} \\\\frac{1}{|L_u|} \\\\sum_{i \\\\in L_u\",\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 25000,\n",
       "  'chunk': 're of unique recommendations in each user’s list.\\n\\n**Range**: 0 to 1.\\n* **0**: Identical recommendations for each user in top-K. \\n* **1**: Each user’s recommendations in top-K are unique.   \\n\\n**Interpretation**: the higher the value, the more personalized (= different from others) is each user’s list. The metric visualization also shows the top-10 most popular items.\\n\\n### Average Recommendation Popularity (ARP)\\n\\n**Evidently Metric**: `ARP`\\n\\nThe recommendation popularity bias is a tendency to favor a few popular items. \\n\\nARP reflects the average popularity of the items recommended to the users. \\n\\n**Implementation**.\\n* Compute the item popularity as the number of times each item was seen in training. \\n* Compute the average popularity for each user’s list as a sum of all items’ popularity divided by the number of recommended items.\\n* Compute the average popularity for all users by averaging the results across all users.\\n\\n$$ARP = \\\\frac{1}{|U|} \\\\sum_{u \\\\in U} \\\\frac{1}{|L_u|} \\\\sum_{i \\\\in L_u} \\\\phi(i)$$\\n\\nWhere:\\n* *U* is the total number of users.\\n* *L(u)* is the list of items recommended for the user *u*.\\n* *ϕ(i)* is the number of times item *i* was rated in the training set (popularity of item *i*)\\n\\n**Range**: 0 to infinity \\n\\n**Interpretation**: the higher the value, the more popular on average the recommendations are in top-K.  \\n\\n**Note**: This metric is not normalized and depends on the number of recommendations in the training set.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)\\n\\n### Coverage\\n\\n**Evidently Metric**: `Coverage`\\n\\nCoverage reflects the item coverage as a proportion of items that has been recommended by the system.\\n\\n**Implementation**: compute the share of items recommended to the users out of the total number of potential items (as seen in the training dataset).\\n\\n$$\\\\text{Coverage} ',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 26000,\n",
       "  'chunk': '} \\\\phi(i)$$\\n\\nWhere:\\n* *U* is the total number of users.\\n* *L(u)* is the list of items recommended for the user *u*.\\n* *ϕ(i)* is the number of times item *i* was rated in the training set (popularity of item *i*)\\n\\n**Range**: 0 to infinity \\n\\n**Interpretation**: the higher the value, the more popular on average the recommendations are in top-K.  \\n\\n**Note**: This metric is not normalized and depends on the number of recommendations in the training set.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)\\n\\n### Coverage\\n\\n**Evidently Metric**: `Coverage`\\n\\nCoverage reflects the item coverage as a proportion of items that has been recommended by the system.\\n\\n**Implementation**: compute the share of items recommended to the users out of the total number of potential items (as seen in the training dataset).\\n\\n$$\\\\text{Coverage} = \\\\frac{\\\\text{Number of unique items recommended} K}{\\\\text{Total number of unique items}}$$\\n\\n**Range**: 0 to 1, where 1 means that 100% of items have been recommended to users. \\n\\n**Interpretation**: the higher the value (usually preferable), the larger the share of items represented in the recommendations. Popularity-based recommenders that only recommend a limited number of popular items will have low coverage.\\n\\n### Gini index \\n\\n**Evidently Metric**: `GiniIndex`\\n\\nGini index: reflects the inequality in the distribution of recommended items shown to different users, as compared to a perfectly equal distribution. \\n\\n**Implementation**:  \\n\\n$$ Gini(L) = 1 - \\\\frac{1}{|I| - 1} \\\\sum_{k=1}^{|I|} (2k - |I| - 1) p(i_k | L)$$\\n\\nWhere \\n* *L* is the combined list of all recommendation lists given to different users (note that an item may appear multiple times in L, if recommended for more than one user).\\n* *p(i|L)* is the ratio of occurrence of item *i* in *L*.\\n* *I* is the set of all items in the ca',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 27000,\n",
       "  'chunk': '= \\\\frac{\\\\text{Number of unique items recommended} K}{\\\\text{Total number of unique items}}$$\\n\\n**Range**: 0 to 1, where 1 means that 100% of items have been recommended to users. \\n\\n**Interpretation**: the higher the value (usually preferable), the larger the share of items represented in the recommendations. Popularity-based recommenders that only recommend a limited number of popular items will have low coverage.\\n\\n### Gini index \\n\\n**Evidently Metric**: `GiniIndex`\\n\\nGini index: reflects the inequality in the distribution of recommended items shown to different users, as compared to a perfectly equal distribution. \\n\\n**Implementation**:  \\n\\n$$ Gini(L) = 1 - \\\\frac{1}{|I| - 1} \\\\sum_{k=1}^{|I|} (2k - |I| - 1) p(i_k | L)$$\\n\\nWhere \\n* *L* is the combined list of all recommendation lists given to different users (note that an item may appear multiple times in L, if recommended for more than one user).\\n* *p(i|L)* is the ratio of occurrence of item *i* in *L*.\\n* *I* is the set of all items in the catalog.\\n\\n**Range**: 0 to 1, where 0 represents the perfect equality (recommended items are evenly distributed among users), and 1 is complete inequality (the recommendations are concentrated on a single item).\\n\\n**Interpretation**: the lower the value (usually preferable), the more equal the item distribution in recommendations. If the value is high, a few items are frequently recommended to many users while others are ignored.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)',\n",
       "  'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'filename': 'docs-main/docs/library/leftover_content.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Sometimes you need to generate multiple column-level Tests or Metrics. To simplify this, you can use metric generator helper functions.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Imports\\n\\n<Accordion title=\"Generate data\" defaultOpen={false}>\\nUse the following code to generate toy data for this guide.\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n\\nnp.random.seed(42)\\n\\ndata = {\\n    \"Age\": np.random.randint(18, 60, size=30),\\n    \"Salary\": np.random.randint(30000, 120000, size=30),\\n    \"Department\": np.random.choice([\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"], size=30),\\n    \"YearsExperience\": np.random.randint(1, 15, size=30),  \\n    \"EducationLevel\": np.random.choice([\"High School\", \"Bachelor\", \"Master\", \"PhD\"], size=30)  \\n}\\n\\ndummy_df = pd.DataFrame(data)\\n\\neval_data_1 = Dataset.from_pandas(\\n    dummy_df.iloc[:15],\\n    data_definition=DataDefinition()\\n)\\neval_data_2 = Dataset.from_pandas(\\n    dummy_df.iloc[15:],\\n    data_definition=DataDefinition()\\n)\\n```\\n</Accordion>\\n\\nImports\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.generators import ColumnMetricGenerator\\n```\\n\\n## Metric generators\\n\\n**Example 1**. Apply the selected metric (`ValueDrift`) to all columns in the dataset. \\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift)\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 2**. Apply the selected metric (`ValueDrift`) to the listed columns in the dataset. Use `metric_kwargs` to pass any applicable metric parameters.\\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift, \\n                          columns=[\"EducationLevel\", \"Salary\"],\\n                          metric_kwargs={\"method\":\"psi\"}), # metric parameters\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 3**. Apply the selected metric (`ValueDrift`) only to the categorical (`cat`) col',\n",
       "  'title': 'Metric generators',\n",
       "  'description': 'How to generate multiple metrics at once.',\n",
       "  'filename': 'docs-main/docs/library/metric_generator.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'et.from_pandas(\\n    dummy_df.iloc[15:],\\n    data_definition=DataDefinition()\\n)\\n```\\n</Accordion>\\n\\nImports\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.generators import ColumnMetricGenerator\\n```\\n\\n## Metric generators\\n\\n**Example 1**. Apply the selected metric (`ValueDrift`) to all columns in the dataset. \\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift)\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 2**. Apply the selected metric (`ValueDrift`) to the listed columns in the dataset. Use `metric_kwargs` to pass any applicable metric parameters.\\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift, \\n                          columns=[\"EducationLevel\", \"Salary\"],\\n                          metric_kwargs={\"method\":\"psi\"}), # metric parameters\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 3**. Apply the selected metric (`ValueDrift`) only to the categorical (`cat`) columns in the dataset.\\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(UniqueValueCount, \\n                          column_types=\\'cat\\'),  #apply to categorical columns only \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\nAvailable: \\n* `num` - numerical\\n* `cat` - categorical\\n* `all` - all \\n\\n## Test generators\\n\\nYou can use the same approach to generate Tests. Use `metric_kwargs` to pass test conditions.\\n\\n**Example.** Generate the same Test for all the columns in the dataset. It will use defaults if you do not specify the test condition.\\n\\n```python\\nfrom evidently.future.tests import *\\n\\nreport = Report([\\n    ColumnMetricGenerator(MinValue, \\n                          column_types=\\'num\\',\\n                          metric_kwargs={\"tests\":[gt(0)]}), \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\nThis will apply the minimum value test to all numerical columns in the dataset and check that they are above 0.',\n",
       "  'title': 'Metric generators',\n",
       "  'description': 'How to generate multiple metrics at once.',\n",
       "  'filename': 'docs-main/docs/library/metric_generator.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesn’t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```',\n",
       "  'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'The Evidently Python library is an open-source tool designed to evaluate, test and monitor the quality of AI systems, from experimentation to production. You can use the evaluation library on its own, or as part of the [Monitoring Platform](/docs/platform/overview) (self-hosted or Evidently Cloud).\\n\\nThis page provides a conceptual overview of the Evidently library.\\n\\n# At a glance\\n\\nEvidently library covers 4 core workflows. You can these features together or standalone.\\n\\n**1. AI/ML Evaluations**\\n\\n<Check>\\n  **TL;DR**: Lots of useful AI/ML/data metrics out of the box. Exportable as scores or visual reports.\\n</Check>\\n\\nEvidently’s core capability is running evaluations on AI system inputs and outputs. It includes 100\\\\+ built-in metrics and checks, and also useful configurable templates for custom evaluations.\\n\\nYou can get raw either metrics or pass/fail test results.\\n\\nWe support metrics that make sense both for predictive ML tasks and generative LLM system outputs. Example built-in checks:\\n\\n| **Type**                  | **Example checks**                                                        |\\n| ------------------------- | ------------------------------------------------------------------------- |\\n| **🔡 Text qualities**     | Length, sentiment, special symbols, pattern  matches, etc.                |\\n| **📝 LLM output quality** | Semantic similarity, relevance, RAG faithfulness, custom LLM judges, etc. |\\n| **🛢 Data quality**       | Missing values, duplicates, min-max ranges, correlations, etc.            |\\n| **📊 Data drift**         | 20\\\\+ tests and distance metrics to detect distribution drift.             |\\n| **🎯 Classification**     | Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.        |\\n| **📈 Regression**         | MAE, ME, RMSE, error distribution, error normality, error bias, etc.      |\\n| **🗂 Ranking (inc. RAG)** | NDCG, MAP, MRR, Hit Rate, etc.                                            |\\n\\nYou can get evaluation results in multiple formats:',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '\\n| **Type**                  | **Example checks**                                                        |\\n| ------------------------- | ------------------------------------------------------------------------- |\\n| **🔡 Text qualities**     | Length, sentiment, special symbols, pattern  matches, etc.                |\\n| **📝 LLM output quality** | Semantic similarity, relevance, RAG faithfulness, custom LLM judges, etc. |\\n| **🛢 Data quality**       | Missing values, duplicates, min-max ranges, correlations, etc.            |\\n| **📊 Data drift**         | 20\\\\+ tests and distance metrics to detect distribution drift.             |\\n| **🎯 Classification**     | Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.        |\\n| **📈 Regression**         | MAE, ME, RMSE, error distribution, error normality, error bias, etc.      |\\n| **🗂 Ranking (inc. RAG)** | NDCG, MAP, MRR, Hit Rate, etc.                                            |\\n\\nYou can get evaluation results in multiple formats:\\n\\n- **Export scores** as JSON or Python dictionary.\\n- **As a DataFrame**, either as a raw metrics table or by attaching scores to existing data rows.\\n- **Generate visual reports** in Jupyter, Colab, or export as HTML\\n- **Upload to Evidently Platform** to track evaluations over time\\n\\nThis exportability makes it easy to integrate Evidently into your existing workflows and pipelines –\\xa0even if you are not using the Evidently Platform.\\n\\nHere is an example visual report showing various data quality metrics and test results. Other evaluations can be presented in the same way, or exported as raw scores:\\n\\n![](/images/concepts/report_test_preview.gif)\\n\\n**📌 Links:**\\n\\n- Quickstart for [LLM evaluation](/quickstart_llm) \\n- Quickstart for [ML evaluation](/quickstart_ml)\\n\\nOr read on through this page for conceptual introduction.\\n\\n**2. Synthetic data generation [NEW]**\\n\\n<Check>\\n  **TL;DR**: We have a nice config for structured synthetic data generation using LLMs.\\n</Check>\\n\\nPrimarily designed for LLM u',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '\\n\\n- **Export scores** as JSON or Python dictionary.\\n- **As a DataFrame**, either as a raw metrics table or by attaching scores to existing data rows.\\n- **Generate visual reports** in Jupyter, Colab, or export as HTML\\n- **Upload to Evidently Platform** to track evaluations over time\\n\\nThis exportability makes it easy to integrate Evidently into your existing workflows and pipelines –\\xa0even if you are not using the Evidently Platform.\\n\\nHere is an example visual report showing various data quality metrics and test results. Other evaluations can be presented in the same way, or exported as raw scores:\\n\\n![](/images/concepts/report_test_preview.gif)\\n\\n**📌 Links:**\\n\\n- Quickstart for [LLM evaluation](/quickstart_llm) \\n- Quickstart for [ML evaluation](/quickstart_ml)\\n\\nOr read on through this page for conceptual introduction.\\n\\n**2. Synthetic data generation [NEW]**\\n\\n<Check>\\n  **TL;DR**: We have a nice config for structured synthetic data generation using LLMs.\\n</Check>\\n\\nPrimarily designed for LLM use cases, Evidently also helps you generate synthetic test datasets - such as RAG-style question-answer pairs from a knowledge base or synthetic inputs to cold-start your AI app testing.\\n\\n**📌 Links:** \\n- [Synthetic data](docs/library/synthetic_data_api) \\n\\n**3. Prompt optimization [NEW]**\\n\\n<Check>\\n  **TL;DR**: We help write prompts using labeled or annotated data as a target.\\n</Check>\\n\\nEvidently also includes tools for automated prompt writing. This features uses built-in evaluation capabilities to score prompt variations, optimizing them based on a target dataset and/or free-form user feedback.\\n\\nThis feature also help automatically generate LLM judge prompts to streamline the creation of custom evaluations.\\n\\n**📌 Links:** \\n- [Prompt optimization](docs/library/prompt_optimization)\\n\\n4. **Tracking and Visualization UI**\\n\\n<Check>\\n  **TL;DR**: There is also a minimal UI to store and track evaluation results.\\n</Check>\\n\\nThe Evidently library also includes a lightweight self-hostable UI for sto',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': \"se cases, Evidently also helps you generate synthetic test datasets - such as RAG-style question-answer pairs from a knowledge base or synthetic inputs to cold-start your AI app testing.\\n\\n**📌 Links:** \\n- [Synthetic data](docs/library/synthetic_data_api) \\n\\n**3. Prompt optimization [NEW]**\\n\\n<Check>\\n  **TL;DR**: We help write prompts using labeled or annotated data as a target.\\n</Check>\\n\\nEvidently also includes tools for automated prompt writing. This features uses built-in evaluation capabilities to score prompt variations, optimizing them based on a target dataset and/or free-form user feedback.\\n\\nThis feature also help automatically generate LLM judge prompts to streamline the creation of custom evaluations.\\n\\n**📌 Links:** \\n- [Prompt optimization](docs/library/prompt_optimization)\\n\\n4. **Tracking and Visualization UI**\\n\\n<Check>\\n  **TL;DR**: There is also a minimal UI to store and track evaluation results.\\n</Check>\\n\\nThe Evidently library also includes a lightweight self-hostable UI for storing, comparing, and visualizing evaluation results over time.\\n\\nWhile visual reports provide a snapshot of an evaluation for a specific period, dataset, or prompt version, the UI allows you to store multiple evaluations and track changes over time.\\n\\n![](/images/concepts/evidently_oss_ui-min.png)\\n\\n**📌 Links:**\\n\\n- See live demo: [https://demo.evidentlyai.com](https://demo.evidentlyai.com/). \\n- [Self-hosting guide](/docs/setup/self-hosting)\\n\\n<Note>\\n  The open-source UI is different from the Evidently Cloud / Enterprise platform version which has muliple additional features. Explore the [Evidently Platform capabailities](/docs/platform/overview). \\n</Note>\\n\\n# Core evaluation concepts\\n\\nLet's take a look at the end-to-end evaluation process. It can be adapted to different metrics or data types, following the same worklows. \\n\\n## Dataset\\n\\nTo run an evaluation, you first need to prepare the data. For example, generate and trace outputs from your ML or LLM system.\\n\\n1. **Prepare your data as a pan\",\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'ring, comparing, and visualizing evaluation results over time.\\n\\nWhile visual reports provide a snapshot of an evaluation for a specific period, dataset, or prompt version, the UI allows you to store multiple evaluations and track changes over time.\\n\\n![](/images/concepts/evidently_oss_ui-min.png)\\n\\n**📌 Links:**\\n\\n- See live demo: [https://demo.evidentlyai.com](https://demo.evidentlyai.com/). \\n- [Self-hosting guide](/docs/setup/self-hosting)\\n\\n<Note>\\n  The open-source UI is different from the Evidently Cloud / Enterprise platform version which has muliple additional features. Explore the [Evidently Platform capabailities](/docs/platform/overview). \\n</Note>\\n\\n# Core evaluation concepts\\n\\nLet\\'s take a look at the end-to-end evaluation process. It can be adapted to different metrics or data types, following the same worklows. \\n\\n## Dataset\\n\\nTo run an evaluation, you first need to prepare the data. For example, generate and trace outputs from your ML or LLM system.\\n\\n1. **Prepare your data as a pandas DataFrame**. The table can include any combination of numerical, categorical, text, metadata (including timestamps or IDs), and embedding columns.\\n\\nHere are a few examples of data inputs Evidently can handle:\\n\\n<Tabs>\\n  <Tab title=\"LLM logs\">\\n    **LLM logs**. Pass any text columns with inputs/outputs, context or ground truth.\\n\\n    | Question                             | Context                                                                                                   | Answer                          |\\n    | ------------------------------------ | --------------------------------------------------------------------------------------------------------- | ------------------------------- |\\n    | How old is the universe?             | The universe is believed to have originated from the Big Bang that occurred 13.8 billion years ago.       | 13.8 billion years old.         |\\n    | What’s the lifespan of Baobab trees? | Baobab trees can live up to 2,500 years. They are often calle',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'das DataFrame**. The table can include any combination of numerical, categorical, text, metadata (including timestamps or IDs), and embedding columns.\\n\\nHere are a few examples of data inputs Evidently can handle:\\n\\n<Tabs>\\n  <Tab title=\"LLM logs\">\\n    **LLM logs**. Pass any text columns with inputs/outputs, context or ground truth.\\n\\n    | Question                             | Context                                                                                                   | Answer                          |\\n    | ------------------------------------ | --------------------------------------------------------------------------------------------------------- | ------------------------------- |\\n    | How old is the universe?             | The universe is believed to have originated from the Big Bang that occurred 13.8 billion years ago.       | 13.8 billion years old.         |\\n    | What’s the lifespan of Baobab trees? | Baobab trees can live up to 2,500 years. They are often called the “Tree of Life”.                        | Up to 2,500 years.              |\\n    | What is the speed of light?          | The speed of light in a vacuum is approximately 299,792 kilometers per second (186,282 miles per second). | Close to 299,792 km per second. |\\n  </Tab>\\n  <Tab title=\"Data table\">\\n    **Data table**. You can pass any dataset to run run data quality and data drift checks. Use this when evaluating ML model performance without ground truth: include input features and predictions.\\n\\n    | Order ID | Product                | Category    | Quantity | Price  | Payment Method | Shipping Status |\\n    | -------- | ---------------------- | ----------- | -------- | ------ | -------------- | --------------- |\\n    | ORD001   | Wireless Headphones    | Electronics | 1        | 120.00 | Credit Card    | Shipped         |\\n    | ORD002   | Yoga Mat               | Sports      | 2        | 45.00  | PayPal         | In Transit      |\\n    | ORD003   | Stainless Steel Bottle | Kitchen  ',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'd the “Tree of Life”.                        | Up to 2,500 years.              |\\n    | What is the speed of light?          | The speed of light in a vacuum is approximately 299,792 kilometers per second (186,282 miles per second). | Close to 299,792 km per second. |\\n  </Tab>\\n  <Tab title=\"Data table\">\\n    **Data table**. You can pass any dataset to run run data quality and data drift checks. Use this when evaluating ML model performance without ground truth: include input features and predictions.\\n\\n    | Order ID | Product                | Category    | Quantity | Price  | Payment Method | Shipping Status |\\n    | -------- | ---------------------- | ----------- | -------- | ------ | -------------- | --------------- |\\n    | ORD001   | Wireless Headphones    | Electronics | 1        | 120.00 | Credit Card    | Shipped         |\\n    | ORD002   | Yoga Mat               | Sports      | 2        | 45.00  | PayPal         | In Transit      |\\n    | ORD003   | Stainless Steel Bottle | Kitchen     | 3        | 30.00  | Debit Card     | Delivered       |\\n  </Tab>\\n  <Tab title=\"Classification\">\\n    **Classification logs**. To evaluate classification quality, pass a table that contains columns with predicted and actual labels. Input features are optional but useful for some evals.\\n\\n    | Timestamp           | Transaction ID | Amount  | Location      | Device Type | Fraud Label | Target |\\n    | ------------------- | -------------- | ------- | ------------- | ----------- | ----------- | ------ |\\n    | 2023-12-01 10:15:23 | TXN001         | 250.00  | New York, USA | Mobile      | 0           | 0      |\\n    | 2023-12-01 10:17:45 | TXN002         | 5000.00 | London, UK    | Desktop     | 1           | 1      |\\n    | 2023-12-01 10:20:10 | TXN003         | 1200.00 | Sydney, AUS   | Tablet      | 0           | 0      |\\n  </Tab>\\n  <Tab title=\"Regression\">\\n    **Regression logs**. To evaluate regression quality, pass a table that contains columns with predicted and actual values. Input fe',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': '   | 3        | 30.00  | Debit Card     | Delivered       |\\n  </Tab>\\n  <Tab title=\"Classification\">\\n    **Classification logs**. To evaluate classification quality, pass a table that contains columns with predicted and actual labels. Input features are optional but useful for some evals.\\n\\n    | Timestamp           | Transaction ID | Amount  | Location      | Device Type | Fraud Label | Target |\\n    | ------------------- | -------------- | ------- | ------------- | ----------- | ----------- | ------ |\\n    | 2023-12-01 10:15:23 | TXN001         | 250.00  | New York, USA | Mobile      | 0           | 0      |\\n    | 2023-12-01 10:17:45 | TXN002         | 5000.00 | London, UK    | Desktop     | 1           | 1      |\\n    | 2023-12-01 10:20:10 | TXN003         | 1200.00 | Sydney, AUS   | Tablet      | 0           | 0      |\\n  </Tab>\\n  <Tab title=\"Regression\">\\n    **Regression logs**. To evaluate regression quality, pass a table that contains columns with predicted and actual values. Input features are optional but useful for some evals.\\n\\n    | Prop ID | Location      | Sq ft | Type      | Bedrooms | Has Garden | Predicted  (\\\\$) | Actual (\\\\$) |\\n    | ------- | ------------- | ----- | --------- | -------- | ---------- | --------------- | ----------- |\\n    | P01     | New York, USA | 850   | Apartment | 2        | No         | 850,000         | 870,000     |\\n    | P02     | New York, USA | 1200  | House     | 3        | Yes        | 1,250,000       | 1,300,000   |\\n    | P03     | London, UK    | 950   | Flat      | 2        | No         | 700,000         | 720,000     |\\n  </Tab>\\n  <Tab title=\"Ranking\">\\n    **Ranking logs**. To evaluate ranking or recommendations, pass data that contains columns with rank/score and interaction result. Features are optional but useful for some evals.\\n\\n    | User ID | Movie ID | Title        | Genre         | Avg Rating | Watched (%) | Predicted Rank |\\n    | ------- | -------- | ------------ | ------------- | ---------- | ----------- | --------',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'atures are optional but useful for some evals.\\n\\n    | Prop ID | Location      | Sq ft | Type      | Bedrooms | Has Garden | Predicted  (\\\\$) | Actual (\\\\$) |\\n    | ------- | ------------- | ----- | --------- | -------- | ---------- | --------------- | ----------- |\\n    | P01     | New York, USA | 850   | Apartment | 2        | No         | 850,000         | 870,000     |\\n    | P02     | New York, USA | 1200  | House     | 3        | Yes        | 1,250,000       | 1,300,000   |\\n    | P03     | London, UK    | 950   | Flat      | 2        | No         | 700,000         | 720,000     |\\n  </Tab>\\n  <Tab title=\"Ranking\">\\n    **Ranking logs**. To evaluate ranking or recommendations, pass data that contains columns with rank/score and interaction result. Features are optional but useful for some evals.\\n\\n    | User ID | Movie ID | Title        | Genre         | Avg Rating | Watched (%) | Predicted Rank |\\n    | ------- | -------- | ------------ | ------------- | ---------- | ----------- | -------------- |\\n    | U001    | M001     | The Matrix   | Sci-Fi        | 4.8        | 100         | 1              |\\n    | U002    | M002     | Titanic      | Romance/Drama | 4.5        | 80          | 2              |\\n    | U001    | M003     | Interstellar | Sci-Fi        | 4.7        | 90          | 2              |\\n  </Tab>\\n  <Tab title=\"Embeddings\">\\n    **Embeddings**. To evaluate embeddings drift, pass embeddings as numerical columns.\\n\\n    | col_0    | col_1    | col_2    | col_3    | col_4    | ... | col_98   | col_99   | col_100  |\\n    | -------- | -------- | -------- | -------- | -------- | --- | -------- | -------- | -------- |\\n    | 0.171242 | 0.149020 | 0.122876 | 0.121569 | 0.137255 | ... | 0.614379 | 0.613072 | 0.612000 |\\n    | 0.619608 | 0.628758 | 0.670588 | 0.661438 | 0.636601 | ... | 0.525490 | 0.509804 | 0.500000 |\\n  </Tab>\\n</Tabs>\\n\\nThese are examples: you data can have other structure.\\n\\n2. **Create a Dataset object**. Once you have the data, you must create an Evidently `',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '------ |\\n    | U001    | M001     | The Matrix   | Sci-Fi        | 4.8        | 100         | 1              |\\n    | U002    | M002     | Titanic      | Romance/Drama | 4.5        | 80          | 2              |\\n    | U001    | M003     | Interstellar | Sci-Fi        | 4.7        | 90          | 2              |\\n  </Tab>\\n  <Tab title=\"Embeddings\">\\n    **Embeddings**. To evaluate embeddings drift, pass embeddings as numerical columns.\\n\\n    | col_0    | col_1    | col_2    | col_3    | col_4    | ... | col_98   | col_99   | col_100  |\\n    | -------- | -------- | -------- | -------- | -------- | --- | -------- | -------- | -------- |\\n    | 0.171242 | 0.149020 | 0.122876 | 0.121569 | 0.137255 | ... | 0.614379 | 0.613072 | 0.612000 |\\n    | 0.619608 | 0.628758 | 0.670588 | 0.661438 | 0.636601 | ... | 0.525490 | 0.509804 | 0.500000 |\\n  </Tab>\\n</Tabs>\\n\\nThese are examples: you data can have other structure.\\n\\n2. **Create a Dataset object**. Once you have the data, you must create an Evidently `Dataset` object. This allows attaching extra meta-information so that your data is processed correctly.\\n\\nThis is needed because some evaluations may require specific columns or data types present. For example, to evaluate classification quality, you need both predictions and actual labels. To specify where they are located in your table, you can map the data schema using [Data Definition](/docs/library/data_definition).\\n\\n3. **[Optional] Preparing two datasets**. Typically you evaluate a single (`current` ) dataset. Optionally, you can prepare a second (`reference`) dataset that will be used during the evaluation. Both must have identical structures.\\n\\n![](/images/datasets_input_data_two.png)\\n\\nWhen to use two datasets:\\n\\n- **Side-by-side comparison**. This lets you compare outputs or data quality across two periods, prompt/model versions, etc. in a single Report.\\n- **Data drift detection. (Required)**. You can detect distribution shifts by comparing datasets, such as this week’s data to t',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'Dataset` object. This allows attaching extra meta-information so that your data is processed correctly.\\n\\nThis is needed because some evaluations may require specific columns or data types present. For example, to evaluate classification quality, you need both predictions and actual labels. To specify where they are located in your table, you can map the data schema using [Data Definition](/docs/library/data_definition).\\n\\n3. **[Optional] Preparing two datasets**. Typically you evaluate a single (`current` ) dataset. Optionally, you can prepare a second (`reference`) dataset that will be used during the evaluation. Both must have identical structures.\\n\\n![](/images/datasets_input_data_two.png)\\n\\nWhen to use two datasets:\\n\\n- **Side-by-side comparison**. This lets you compare outputs or data quality across two periods, prompt/model versions, etc. in a single Report.\\n- **Data drift detection. (Required)**. You can detect distribution shifts by comparing datasets, such as this week’s data to the previous one.\\n- **Simplify test setup**. You can automatically generate test conditions (e.g., min-max ranges) from the reference dataset without manual configuration.\\n\\n<Info>\\n  **Data sampling**. For large datasets (millions of rows), evals can take some time. The depends on:\\n\\n  - the specific evaluation: some are more computationally intensive than others\\n  - your dataset: e.g., if you run column-level evals and have lots of columns\\n  - your infrastructure: data is processed in-memory.\\n\\n  If the computation takes too long, it’s often more efficient to use samples. For example, in data drift detection, you can apply random or stratified sampling.\\n</Info>\\n\\nOnce your `Dataset` is ready, you can run evaluations. You can either:\\n\\n- Add `descriptors` to your dataset, and then compute a summary Report.\\n- Compute a Report directly over raw data.\\n\\n## Descriptors\\n\\nTo evaluate text data and LLM outputs, you need `Descriptors`.\\n\\nA **Descriptor** is a _row-level_ score or label that assesses a',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'he previous one.\\n- **Simplify test setup**. You can automatically generate test conditions (e.g., min-max ranges) from the reference dataset without manual configuration.\\n\\n<Info>\\n  **Data sampling**. For large datasets (millions of rows), evals can take some time. The depends on:\\n\\n  - the specific evaluation: some are more computationally intensive than others\\n  - your dataset: e.g., if you run column-level evals and have lots of columns\\n  - your infrastructure: data is processed in-memory.\\n\\n  If the computation takes too long, it’s often more efficient to use samples. For example, in data drift detection, you can apply random or stratified sampling.\\n</Info>\\n\\nOnce your `Dataset` is ready, you can run evaluations. You can either:\\n\\n- Add `descriptors` to your dataset, and then compute a summary Report.\\n- Compute a Report directly over raw data.\\n\\n## Descriptors\\n\\nTo evaluate text data and LLM outputs, you need `Descriptors`.\\n\\nA **Descriptor** is a _row-level_ score or label that assesses a specific quality of a given text. It’s different from metrics (like accuracy or precision) that give a score for an entire _dataset_. You can use descriptors to assess LLM outputs in summarization, Q&A, chatbots, agents, RAGs, etc.\\n\\nDescriptors range from deterministic to complex ML- or LLM-based checks.\\n\\nA simple example of a descriptor is `TextLength`.  A more complex example is a customizable `LLMEval` descriptor: where you prompt an LLM to act as a judge and, for example, label responses as \"relevant\" or \"not relevant\".\\n\\nDescriptors can also use two texts at once, like checking `SemanticSimilarity` between two columns to compare new response to the reference one.\\n\\nYou can use [built-in descriptors](/metrics/all_descriptors), configure templates (like LLM judges or regular expressions) or add custom checks in Python. Each Descriptor returns a result that can be:\\n\\n- **Numerical**. Any scores like symbol count or sentiment score.\\n- **Categorical**. Labels or binary “true”/“false” res',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': ' specific quality of a given text. It’s different from metrics (like accuracy or precision) that give a score for an entire _dataset_. You can use descriptors to assess LLM outputs in summarization, Q&A, chatbots, agents, RAGs, etc.\\n\\nDescriptors range from deterministic to complex ML- or LLM-based checks.\\n\\nA simple example of a descriptor is `TextLength`.  A more complex example is a customizable `LLMEval` descriptor: where you prompt an LLM to act as a judge and, for example, label responses as \"relevant\" or \"not relevant\".\\n\\nDescriptors can also use two texts at once, like checking `SemanticSimilarity` between two columns to compare new response to the reference one.\\n\\nYou can use [built-in descriptors](/metrics/all_descriptors), configure templates (like LLM judges or regular expressions) or add custom checks in Python. Each Descriptor returns a result that can be:\\n\\n- **Numerical**. Any scores like symbol count or sentiment score.\\n- **Categorical**. Labels or binary “true”/“false” results for pattern matches.\\n- **Text string**. Like explanations generated by LLM.\\n\\nEvidently adds the computed descriptor values directly to the dataset.\\n\\n![](/images/concepts/overview_descriptors_export.png)\\n\\nThis helps with debugging: for example, you can sort to find the negative responses. You can view the results as a Pandas DataFrame or on the Evidently Platform.\\n\\n**Descriptor tests**. Additionally, you can add a pass/fail condition on top of computed descriptors. For example, consider output a \"pass\" only when both conditions are true: it has expected length and is labeled \"correct\" by the LLM judge.\\n\\nAfter you get the row-level Descriptors, you can also compute Metrics and Tests on the dataset level – using Reports.\\n\\n## Reports\\n\\nA **Report** lets you structure and run evals on the dataset or column-level.\\n\\nYou can generate Reports after you get the descriptors, or for any existing dataset like a table with ML model logs. Use Reports to:\\n\\n- summarize the computed text descriptors',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': 'ults for pattern matches.\\n- **Text string**. Like explanations generated by LLM.\\n\\nEvidently adds the computed descriptor values directly to the dataset.\\n\\n![](/images/concepts/overview_descriptors_export.png)\\n\\nThis helps with debugging: for example, you can sort to find the negative responses. You can view the results as a Pandas DataFrame or on the Evidently Platform.\\n\\n**Descriptor tests**. Additionally, you can add a pass/fail condition on top of computed descriptors. For example, consider output a \"pass\" only when both conditions are true: it has expected length and is labeled \"correct\" by the LLM judge.\\n\\nAfter you get the row-level Descriptors, you can also compute Metrics and Tests on the dataset level – using Reports.\\n\\n## Reports\\n\\nA **Report** lets you structure and run evals on the dataset or column-level.\\n\\nYou can generate Reports after you get the descriptors, or for any existing dataset like a table with ML model logs. Use Reports to:\\n\\n- summarize the computed text descriptors across all inputs\\n- analyze any tabular dataset (descriptive stats, quality, drift)\\n- evaluate AI system performance (regression, classification, ranking, etc.)\\n\\nEach Report runs a computation and visualizes a set of **Metrics** and conditional **Tests.** If you pass two datasets, you get a side-by-side comparison.\\xa0\\n\\nThe easiest way to start is by using **Presets**.\\n\\n### Metric Presets\\n\\nPresets are pre-configured evaluation templates.\\n\\nThey help compute multiple related Metrics using a single line of code. Evidently has a number of **comprehensive Presets** ([see all](/metrics/all_presets)) for specific evaluation scenarios: from exploratory data analysis to AI quality assessments. For example:\\n\\n<Tabs>\\n  <Tab title=\"TextEvals\">\\n    `TextEvals` summarizes the scores from all text descriptors.\\n\\n    ![](/images/examples/llm_quickstart_report.png)\\n  </Tab>\\n  <Tab title=\"Data Drift\">\\n    `DataDriftPreset` identifies shifts in data distribution for all dataset columns.\\n\\n    ![](/images/conc',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': ' across all inputs\\n- analyze any tabular dataset (descriptive stats, quality, drift)\\n- evaluate AI system performance (regression, classification, ranking, etc.)\\n\\nEach Report runs a computation and visualizes a set of **Metrics** and conditional **Tests.** If you pass two datasets, you get a side-by-side comparison.\\xa0\\n\\nThe easiest way to start is by using **Presets**.\\n\\n### Metric Presets\\n\\nPresets are pre-configured evaluation templates.\\n\\nThey help compute multiple related Metrics using a single line of code. Evidently has a number of **comprehensive Presets** ([see all](/metrics/all_presets)) for specific evaluation scenarios: from exploratory data analysis to AI quality assessments. For example:\\n\\n<Tabs>\\n  <Tab title=\"TextEvals\">\\n    `TextEvals` summarizes the scores from all text descriptors.\\n\\n    ![](/images/examples/llm_quickstart_report.png)\\n  </Tab>\\n  <Tab title=\"Data Drift\">\\n    `DataDriftPreset` identifies shifts in data distribution for all dataset columns.\\n\\n    ![](/images/concepts/overview_drift_report-min.png)\\n  </Tab>\\n  <Tab title=\"Data Summary\">\\n    `DataSummaryPreset` summarizes all dataset columns, generating statistics and profiles for each.\\n\\n    ![](/images/metrics/preset_datasummary_example-min.png)\\n  </Tab>\\n  <Tab title=\"Classification\">\\n    `ClassificationPreset` breaks down classification metrics and includes debugging plots.\\n\\n    ![](/images/metrics/preset_classification_example-min.png)\\n  </Tab>\\n</Tabs>\\n\\n### Metrics\\n\\nEach Preset is made of individual Metrics. You can also create your own **custom Report** by listing the `Metrics` you want to include.\\n\\n- You can combine multiple Metrics and Presets in a Report.\\xa0\\n- You can include both built-in Metrics and custom Metrics.\\n\\nBuilt-in Metrics range from simple statistics like `MeanValue` or `MissingValueCount` to complex algorithmic evals like `DriftedColumnsCount`.\\n\\nEach **Metric** computes a single value and has an optional visual representation (or several to choose from). For convenience, there ',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': 'epts/overview_drift_report-min.png)\\n  </Tab>\\n  <Tab title=\"Data Summary\">\\n    `DataSummaryPreset` summarizes all dataset columns, generating statistics and profiles for each.\\n\\n    ![](/images/metrics/preset_datasummary_example-min.png)\\n  </Tab>\\n  <Tab title=\"Classification\">\\n    `ClassificationPreset` breaks down classification metrics and includes debugging plots.\\n\\n    ![](/images/metrics/preset_classification_example-min.png)\\n  </Tab>\\n</Tabs>\\n\\n### Metrics\\n\\nEach Preset is made of individual Metrics. You can also create your own **custom Report** by listing the `Metrics` you want to include.\\n\\n- You can combine multiple Metrics and Presets in a Report.\\xa0\\n- You can include both built-in Metrics and custom Metrics.\\n\\nBuilt-in Metrics range from simple statistics like `MeanValue` or `MissingValueCount` to complex algorithmic evals like `DriftedColumnsCount`.\\n\\nEach **Metric** computes a single value and has an optional visual representation (or several to choose from). For convenience, there are also **small Presets** that combine a handful of scores in a single widget, like `ValueStats` that shows many relevant descriptive value statistics at once.\\n\\n![](/images/concepts/overview_small_preset_cat_value_compare_example.png)\\n\\nSimilarly `DatasetStats` give quick overview of all dataset-level stats, `ClassificationQuality` computes multiple metrics like Precision, Recall, Accuracy, ROC AUC, etc.\\n\\n<Info>\\n  Explore all [**Built-in Metrics**](/metrics/all_metrics).\\n</Info>\\n\\n## Test Suites\\n\\nReports are great for analysis and debugging, or logging metrics during monitoring. However, in many cases, you don’t want to review all the scores but run a **conditional check** to confirm that nothing is off. In this case, **Tests** are a great option.\\n\\n### Tests\\n\\n**Tests** let you validate your results against specific expectations. You create a Test by adding a **condition** parameter to a Metric.\\xa0Each Test will calculate a given value, check it against the rule, and report a pass/fail res',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': 'are also **small Presets** that combine a handful of scores in a single widget, like `ValueStats` that shows many relevant descriptive value statistics at once.\\n\\n![](/images/concepts/overview_small_preset_cat_value_compare_example.png)\\n\\nSimilarly `DatasetStats` give quick overview of all dataset-level stats, `ClassificationQuality` computes multiple metrics like Precision, Recall, Accuracy, ROC AUC, etc.\\n\\n<Info>\\n  Explore all [**Built-in Metrics**](/metrics/all_metrics).\\n</Info>\\n\\n## Test Suites\\n\\nReports are great for analysis and debugging, or logging metrics during monitoring. However, in many cases, you don’t want to review all the scores but run a **conditional check** to confirm that nothing is off. In this case, **Tests** are a great option.\\n\\n### Tests\\n\\n**Tests** let you validate your results against specific expectations. You create a Test by adding a **condition** parameter to a Metric.\\xa0Each Test will calculate a given value, check it against the rule, and report a pass/fail result.\\n\\n- You can run multiple Tests in one go.\\n- You can create Tests on the dataset or column level.\\n- You can formulate custom conditions or use defaults.\\n\\nA **Test Suite** is a collection of individual Tests. It works as an extension to a Report. Once you configure Tests, your Report will get an **additional tab** that shows a summary of outcomes.;\\n\\nYou can navigate the results by test outcome.\\n\\n![](/images/concepts/overview_test_suite_example-min.png)\\n\\nEach Test results in one of the following statuses:\\n\\n- **Pass:** The condition was met.\\n- **Fail:** The condition wasn’t met.\\n- **Warning:** The condition wasn’t met, but the check is marked as non-critical.\\n- **Error:** Something went wrong with the Test itself, such as an execution error.\\n\\nYou can view extra details to debug. For example, if you run a Test to check that less than 5% of LLM responses fall outside the approved length, you can see the corresponding distribution:\\n\\n![](/images/concepts/overview_descriptor_test_example-mi',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': 'ult.\\n\\n- You can run multiple Tests in one go.\\n- You can create Tests on the dataset or column level.\\n- You can formulate custom conditions or use defaults.\\n\\nA **Test Suite** is a collection of individual Tests. It works as an extension to a Report. Once you configure Tests, your Report will get an **additional tab** that shows a summary of outcomes.;\\n\\nYou can navigate the results by test outcome.\\n\\n![](/images/concepts/overview_test_suite_example-min.png)\\n\\nEach Test results in one of the following statuses:\\n\\n- **Pass:** The condition was met.\\n- **Fail:** The condition wasn’t met.\\n- **Warning:** The condition wasn’t met, but the check is marked as non-critical.\\n- **Error:** Something went wrong with the Test itself, such as an execution error.\\n\\nYou can view extra details to debug. For example, if you run a Test to check that less than 5% of LLM responses fall outside the approved length, you can see the corresponding distribution:\\n\\n![](/images/concepts/overview_descriptor_test_example-min.png)\\n\\n### Test Conditions\\n\\nEvidently has a powerful API to [set up Test conditions](/docs/library/tests).\\n\\n- **Manual setup.** You can add thresholds to Metrics one by one, using simple syntax like **`greater than (gt)`** or **`less than (lt)`**.\\xa0By picking different Metrics to test against, you can formulate fine-grained conditions like \"less than 10% of texts can fall outside 10–100 character length.\"\\n- **Manual setup with reference.** If you have a reference dataset (like a previous data batch), you can set conditions **relative** to it. For example, you can check if the min-max value range stays within ±5% of the reference range without setting exact thresholds.\\n- **Automatic setup.** You can run any Test using built-in defaults. These are either:\\n  - **Heuristics**. For example, the Test on missing values assumes none should be preset.\\n  - **Heuristics relative to reference.** Here, conditions adjust to a reference. For instance, the Test on missing values assumes their share sh',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': 'n.png)\\n\\n### Test Conditions\\n\\nEvidently has a powerful API to [set up Test conditions](/docs/library/tests).\\n\\n- **Manual setup.** You can add thresholds to Metrics one by one, using simple syntax like **`greater than (gt)`** or **`less than (lt)`**.\\xa0By picking different Metrics to test against, you can formulate fine-grained conditions like \"less than 10% of texts can fall outside 10–100 character length.\"\\n- **Manual setup with reference.** If you have a reference dataset (like a previous data batch), you can set conditions **relative** to it. For example, you can check if the min-max value range stays within ±5% of the reference range without setting exact thresholds.\\n- **Automatic setup.** You can run any Test using built-in defaults. These are either:\\n  - **Heuristics**. For example, the Test on missing values assumes none should be preset.\\n  - **Heuristics relative to reference.** Here, conditions adjust to a reference. For instance, the Test on missing values assumes their share should stay within ±10% of the reference.\\n\\n### Test Presets\\n\\nFor even faster setup, there are **Test Presets**. Each Metric Preset has a corresponding Test Preset that you can enable as an add-on.\\xa0When you do this:\\n\\n- Evidently adds a predefined set of Tests to your Report.\\n- These Tests use default conditions, either static or inferred from the reference dataset.\\n\\nFor example:\\xa0\\n\\n- **Data Summary**. The Metric Preset gives an overview and stats for all columns. The Test Suite checks for quality issues like missing values, duplicates, etc. across all values.\\n- **Classification.** The Metric Preset shows quality metrics like precision or recall. The Test Suite verifies these metrics against a baseline, like a dummy baseline calculated by Evidently or previous model performance.\\n\\n## Building your workflow\\n\\nYou can use Evidently Reports and Test Suites on their own or as part of a monitoring system.\\xa0\\n\\n### Independent use\\n\\nReports are great for exploratory evals:\\n\\n- **Ad hoc evals.** Run one-',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': 'ould stay within ±10% of the reference.\\n\\n### Test Presets\\n\\nFor even faster setup, there are **Test Presets**. Each Metric Preset has a corresponding Test Preset that you can enable as an add-on.\\xa0When you do this:\\n\\n- Evidently adds a predefined set of Tests to your Report.\\n- These Tests use default conditions, either static or inferred from the reference dataset.\\n\\nFor example:\\xa0\\n\\n- **Data Summary**. The Metric Preset gives an overview and stats for all columns. The Test Suite checks for quality issues like missing values, duplicates, etc. across all values.\\n- **Classification.** The Metric Preset shows quality metrics like precision or recall. The Test Suite verifies these metrics against a baseline, like a dummy baseline calculated by Evidently or previous model performance.\\n\\n## Building your workflow\\n\\nYou can use Evidently Reports and Test Suites on their own or as part of a monitoring system.\\xa0\\n\\n### Independent use\\n\\nReports are great for exploratory evals:\\n\\n- **Ad hoc evals.** Run one-time analyses on your data, models or LLM outputs.\\n- **Experiments.** Compare models, prompts, or datasets side by side.\\n- **Debugging.** Investigate data or model issues.\\n\\nTest Suites are great for automated checks like:\\n\\n- **Data validation.** Test inputs and outputs in prediction pipelines.\\xa0\\n- **CI/CD and regression testing.** Check AI system performance after updates.\\n- **Safety testing**. Run structured behavioral tests like adversarial testing.\\xa0\\n\\nFor automation, you can integrate Evidently with tools like Airflow. You can trigger actions based on Test results, such as sending alerts or halting a pipeline.\\n\\n### As part of platform\\n\\nYou can use **Reports** together with the **Evidently Platform** in production workflows:\\n\\n- **Reports** serve as a metric computation layer, running evaluations on your data.\\n- The **Platform** lets you store, compare, track and alert on evaluation results.\\n\\nReports are stored as JSON files, which can be natively parsed to visualize metrics on a Dashbo',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': 'time analyses on your data, models or LLM outputs.\\n- **Experiments.** Compare models, prompts, or datasets side by side.\\n- **Debugging.** Investigate data or model issues.\\n\\nTest Suites are great for automated checks like:\\n\\n- **Data validation.** Test inputs and outputs in prediction pipelines.\\xa0\\n- **CI/CD and regression testing.** Check AI system performance after updates.\\n- **Safety testing**. Run structured behavioral tests like adversarial testing.\\xa0\\n\\nFor automation, you can integrate Evidently with tools like Airflow. You can trigger actions based on Test results, such as sending alerts or halting a pipeline.\\n\\n### As part of platform\\n\\nYou can use **Reports** together with the **Evidently Platform** in production workflows:\\n\\n- **Reports** serve as a metric computation layer, running evaluations on your data.\\n- The **Platform** lets you store, compare, track and alert on evaluation results.\\n\\nReports are stored as JSON files, which can be natively parsed to visualize metrics on a Dashboard.\\n\\n![](/images/evals_flow_python.png)\\n\\nThis setup works for both experiments and production monitoring. For example:\\n\\n- **Experiments.** Log evaluations while experimenting with prompts or model versions. Use the Platform to compare runs and track progress.\\n- **Regression Tests.** Use Test Suites to validate updates on your golden dataset. Debug failures and maintain a history of results on the Platform.\\n- **Batch Monitoring.** Integrate Reports into your data pipelines to compute Metrics for data batches. Use the Platform for performance tracking and alerting.\\n\\n**Evidently Cloud** also offers managed evaluations to generate Reports directly on the platform, and other features such as synthetic data and test generation.\\n\\n**Platform deployment options.** You can choose:\\n\\n- Self-host the open-source platform version.\\xa0\\n- Sign up for [Evidently Cloud](https://www.evidentlyai.com/register) (Recommended).\\n\\nThe Evidently Platform has additional features beyond evaluation: from synthetic da',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 21000,\n",
       "  'chunk': 'ard.\\n\\n![](/images/evals_flow_python.png)\\n\\nThis setup works for both experiments and production monitoring. For example:\\n\\n- **Experiments.** Log evaluations while experimenting with prompts or model versions. Use the Platform to compare runs and track progress.\\n- **Regression Tests.** Use Test Suites to validate updates on your golden dataset. Debug failures and maintain a history of results on the Platform.\\n- **Batch Monitoring.** Integrate Reports into your data pipelines to compute Metrics for data batches. Use the Platform for performance tracking and alerting.\\n\\n**Evidently Cloud** also offers managed evaluations to generate Reports directly on the platform, and other features such as synthetic data and test generation.\\n\\n**Platform deployment options.** You can choose:\\n\\n- Self-host the open-source platform version.\\xa0\\n- Sign up for [Evidently Cloud](https://www.evidentlyai.com/register) (Recommended).\\n\\nThe Evidently Platform has additional features beyond evaluation: from synthetic data to tracing.\\n\\n[Read more on the platform](/docs/platform/overview).',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'More detailed documentation coming soon. \\n\\nRead the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)',\n",
       "  'title': 'Prompt optimization',\n",
       "  'description': '[NEW] Automated prompt optimization.',\n",
       "  'filename': 'docs-main/docs/library/prompt_optimization.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Reports perform evaluations on the Dataset level and/or summarize results of the row-level evaluations. For a general introduction, check [Core Concepts](/docs/library/overview).\\n\\n**Pre-requisites**:\\n\\n* You [installed Evidently](/docs/setup/installation).\\n\\n* You created a Dataset with the [Data Definition](/docs/library/data_definition).\\n\\n* (Optional) for text data, you added Descriptors.\\n\\n<Note>\\n  For a quick end-to-end example of generating Reports, check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n</Note>\\n\\n## Imports\\n\\nImport the Metrics and Presets you plan to use.\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.presets import *\\n```\\n\\nYou can use Metric Presets, which are pre-built Reports that work out of the box, or create a custom Report selecting Metrics one by one.\\n\\n## Presets\\n\\n<Tip>\\n  **Available Presets**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\nTo generate a template Report, simply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = re',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs-main/docs/library/report.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'mply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\"target\", \"prediction\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs-main/docs/library/report.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'port.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\"target\", \"prediction\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\"target\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\"target\", method=\"psi\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, ',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs-main/docs/library/report.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': '**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\"target\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\"target\", method=\"psi\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, you must always pass the `k` parameter (Required).\\n\\n```python\\nreport = Report([\\n   PrecisionTopK(k=10)\\n])\\n```\\n\\n## Compare results\\n\\nIf you computed multiple snapshots, you can quickly compare the resulting metrics side-by-side in a dataframe:\\n\\n```python\\nfrom evidently import compare\\n\\ncompare_dataframe = compare(my_eval_1, my_eval_2, my_eval_3)\\n```\\n\\n## Group by\\n\\nYou can calculate metrics separately for different groups in your data, using a column with categories to split by. Use the `GroupyBy` metric as shown below.\\n\\n**Example**. This will compute the maximum value of salaries by each label in the \"Department\" column.\\n\\n```python\\nfrom evidently.metrics.group_by import GroupBy\\n\\nreport = Report([\\n    GroupBy(MaxValue(column=\"Salary\"), \"Department\"),\\n])\\nmy_eval = report.run(data, None)\\nmy_eval.dict()\\n```\\n\\nNote: you cannot use auto-generated Test conditions when you use GroupBy.\\n\\n## What\\'s next?\\n\\nYou can also add conditions to Metrics: check the [Tests guide](/docs/library/tests).',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs-main/docs/library/report.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'You can generate synthetic test data from RAG knowledge base or using a simple config.\\n\\nMore detailed documentation coming soon.\\n\\nExample notebooks:\\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)',\n",
       "  'title': 'Synthetic data generation',\n",
       "  'description': '[NEW] Code-first synthetic data generation.',\n",
       "  'filename': 'docs-main/docs/library/synthetic_data_api.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'This is relevant when you logging Reports to the Platform. Tags help you associate each Report with a specific model / prompt version, time period, or other context.\\n\\n## Add timestamp\\n\\nEach Report run has a single timestamp. By default, Evidently assigns `datetime.now()` as the run time based on the user\\'s time zone.\\n\\nYou can also specify a custom timestamp by passing it to the `run()` method:\\n\\n```python\\nfrom datetime import datetime\\n\\nmy_eval_4 = report.run(eval_data_1,\\n                       eval_data_2,\\n                       timestamp=datetime(2024, 1, 29))\\n```\\n\\nBecause timestamps are fully customizable, you can log Reports asynchronously or with a delay. For example, make an evaluation after receiving ground truth and backdate Reports to the relevant time period.\\n\\n## Add tags and metadata\\n\\nYou can add `tags` and `metadata` to Reports to support search and ease of filtering. Tags also let you visualize data from specific subsets of Reports on monitoring Panels.\\n\\nUse tags in the following scenarios:\\n* Mark evaluation runs by model version, prompt version, or test scenario.\\n* Indicate status: production, shadow, champion/challenger, A/B versions.\\n* Identify Reports by geography, use case, user segment, or role.\\n* Tag based on reference dataset windows (for example, weekly vs. monthly drift comparisons)\\n* Highlight Reports with a specific role, such as datasheet or model card.\\n\\n**Custom tags**. You can add tags to the Report. Pass any custom Tags as a list:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\ntags=[\"classification\", \"production\"])\\n```\\n\\n**Custom metadata**. Pass metadata as a Python dictionary in key:value pairs:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\nmetadata = {\\n\\t\"deployment\": \"shadow\",\\n\\t\"status\": \"production\",\\n\\t})\\n```\\n\\n**Default metadata**. Use built-in metadata fields `model_id`, `reference_id`, `batch_size`, `dataset_id`:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\n  model_id=\"model_id\",\\n\\treference_id=\"re',\n",
       "  'title': 'Add tags and metadata',\n",
       "  'description': 'How to add metadata to evaluations.',\n",
       "  'filename': 'docs-main/docs/library/tags_metadata.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'owing scenarios:\\n* Mark evaluation runs by model version, prompt version, or test scenario.\\n* Indicate status: production, shadow, champion/challenger, A/B versions.\\n* Identify Reports by geography, use case, user segment, or role.\\n* Tag based on reference dataset windows (for example, weekly vs. monthly drift comparisons)\\n* Highlight Reports with a specific role, such as datasheet or model card.\\n\\n**Custom tags**. You can add tags to the Report. Pass any custom Tags as a list:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\ntags=[\"classification\", \"production\"])\\n```\\n\\n**Custom metadata**. Pass metadata as a Python dictionary in key:value pairs:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\nmetadata = {\\n\\t\"deployment\": \"shadow\",\\n\\t\"status\": \"production\",\\n\\t})\\n```\\n\\n**Default metadata**. Use built-in metadata fields `model_id`, `reference_id`, `batch_size`, `dataset_id`:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\n  model_id=\"model_id\",\\n\\treference_id=\"reference_id\",\\n\\tbatch_size=\"batch_size\",\\n\\tdataset_id=\"dataset_id\"\\n)\\n```\\n\\n**Add tags to run**: You can also tag individual Report runs. This is useful for experiments where you re-run the same Report with different prompts or hyperparameter settings.\\n\\n\\n```python\\nmy_eval = report.run(eval_data_1, eval_data_2, tags=[\"prompt_v1\", \"claude\"])\\n```',\n",
       "  'title': 'Add tags and metadata',\n",
       "  'description': 'How to add metadata to evaluations.',\n",
       "  'filename': 'docs-main/docs/library/tags_metadata.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"Tests let you validate specific conditions and get Pass/Fail results on the dataset level. Tests are an add-on to the Report and appear in a separate tab.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports and select Metrics](/docs/library/report).\\n\\n<Note>\\nFor a quick end-to-end example of generating Tests, сheck the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n</Note>\\n\\n## Imports\\n\\nTo use Tests, import the following modules:\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.presets import *\\nfrom evidently.tests import *\\n```\\n\\n## Auto-generated conditions\\n\\nThere are 3 ways to run conditional checks:\\n\\n* **Tests Presets**. Get a suite of pre-selected Tests with auto-generated conditions. \\n\\n* **Tests with defaults**. Pick Tests one by one, with auto-generate conditions.\\n\\n* **Custom Tests**. Choose all Tests and set conditions manually.\\n\\nLet's first cover the automatic Tests.\\n\\n### Test Presets\\n\\nTest Presets automatically generate a set of Tests to evaluate your data or AI system. Each Report Preset has this option. \\n\\nEnable it by setting `include_tests=True` on the Report level. (Default: False).\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset(),\\n],\\ninclude_tests=True)\\n```\\n\\nFor example, while the `DataSummaryPreset()` Report simply shows descriptive stats of your data, adding the Tests will additionally run multiple checks on data quality and expected column statistics.\\n\\nThe automatic Test conditions can either\\n* be derived from a reference dataset, or\\n* use built-in heuristics.\\n\\n**Using reference**. When you provide a reference dataset, Tests compare the new data against it:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, eval_data_2) # eval_data_2 is reference\\n```\\n\\nFor example, the check on missing values will validate if the current share of missing values is within +/-10% of the reference.\\n\\n<Note>\\nNote that in this case the order matters: the first `eval_data_1` is the current data you evaluate, the second \",\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"a set of Tests to evaluate your data or AI system. Each Report Preset has this option. \\n\\nEnable it by setting `include_tests=True` on the Report level. (Default: False).\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset(),\\n],\\ninclude_tests=True)\\n```\\n\\nFor example, while the `DataSummaryPreset()` Report simply shows descriptive stats of your data, adding the Tests will additionally run multiple checks on data quality and expected column statistics.\\n\\nThe automatic Test conditions can either\\n* be derived from a reference dataset, or\\n* use built-in heuristics.\\n\\n**Using reference**. When you provide a reference dataset, Tests compare the new data against it:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, eval_data_2) # eval_data_2 is reference\\n```\\n\\nFor example, the check on missing values will validate if the current share of missing values is within +/-10% of the reference.\\n\\n<Note>\\nNote that in this case the order matters: the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline and use to generate test conditions.\\n</Note>\\n\\n**Using heuristics**. Without reference, Tests use predefined rules:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, None) # no reference data\\n```\\n\\nIn this case, the missing values Test simply expects 0% missing values. Similarly, classification accuracy Test will compare the performance against a dummy model, etc. Some metrics (like min/max/mean values) don't have default heuristics.\\n\\n<Info>\\n  **How to check Test defaults?** Consult the [All Metrics](/metrics/all_metrics) reference table.\\n</Info>\\n\\n### Individual Tests with defaults\\n\\nPresets are great for a start or quick sanity checks, but often you'd want to select specific Tests. For example, instead of running checks on all value statistics, validate only mean or max.\\n\\nYou can pick the Tests while still using default conditions.\\n\\n**Select Tests**. List the individual Metrics, and choose the the `include_Tests` option:\\n\\n```Py\",\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '`eval_data_2` is the reference dataset you consider as a baseline and use to generate test conditions.\\n</Note>\\n\\n**Using heuristics**. Without reference, Tests use predefined rules:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, None) # no reference data\\n```\\n\\nIn this case, the missing values Test simply expects 0% missing values. Similarly, classification accuracy Test will compare the performance against a dummy model, etc. Some metrics (like min/max/mean values) don\\'t have default heuristics.\\n\\n<Info>\\n  **How to check Test defaults?** Consult the [All Metrics](/metrics/all_metrics) reference table.\\n</Info>\\n\\n### Individual Tests with defaults\\n\\nPresets are great for a start or quick sanity checks, but often you\\'d want to select specific Tests. For example, instead of running checks on all value statistics, validate only mean or max.\\n\\nYou can pick the Tests while still using default conditions.\\n\\n**Select Tests**. List the individual Metrics, and choose the the `include_Tests` option:\\n\\n```Python\\nreport = Report([\\n    MissingValueCount(column=\"Age\"),\\n    MinValue(column=\"Age\"),\\n], \\ninclude_tests=True)\\n```\\n\\nThe Report will use reference conditions with two datasets, or heuristics with one dataset.\\n\\n**Exclude some Tests**. To prevent Test generation for certain Metrics/Presets, set the list of `tests` to `None` or leave empty:\\n\\n```Python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", tests=[]),\\n    MinValue(column=\"Age\"),\\n], \\ninclude_tests=True)\\n```\\n\\nThis Report will include only the Test for `MinValue()` with auto-generated conditions.\\n\\n## Custom Test conditions\\n\\nYou can define specific pass/fail conditions for each Test. \\n\\nFor example, set minimum expected precision or share of a certain category. Tests fail when conditions aren\\'t met.\\n\\n**Setting conditions**. For each Metric you want to validate, define a list of `tests` and set expected behavior using parameters like `gt` (greater than), `lt` (less than), `eq` (equal).\\n\\nFor example, to verify that there are no mis',\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'thon\\nreport = Report([\\n    MissingValueCount(column=\"Age\"),\\n    MinValue(column=\"Age\"),\\n], \\ninclude_tests=True)\\n```\\n\\nThe Report will use reference conditions with two datasets, or heuristics with one dataset.\\n\\n**Exclude some Tests**. To prevent Test generation for certain Metrics/Presets, set the list of `tests` to `None` or leave empty:\\n\\n```Python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", tests=[]),\\n    MinValue(column=\"Age\"),\\n], \\ninclude_tests=True)\\n```\\n\\nThis Report will include only the Test for `MinValue()` with auto-generated conditions.\\n\\n## Custom Test conditions\\n\\nYou can define specific pass/fail conditions for each Test. \\n\\nFor example, set minimum expected precision or share of a certain category. Tests fail when conditions aren\\'t met.\\n\\n**Setting conditions**. For each Metric you want to validate, define a list of `tests` and set expected behavior using parameters like `gt` (greater than), `lt` (less than), `eq` (equal).\\n\\nFor example, to verify that there are no missing values and no values below 18 in the \"Age\" column:\\n\\n```Python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", tests=[eq(0)]),\\n    MinValue(column=\"Age\", tests=[gte(18)]),\\n])\\n```\\n\\nNote that you don\\'t need to use `include_tests` when setting Tests manually.\\n\\n<Info>\\n**Sometimes you may need to use other parameters to set test conditions**. The `tests` parameter applies when a metric returns a single value, or to test `count` for metrics that return both `count` and `share`. For metrics with multiple outputs (e.g. MAE returns `mean` and `std`), you may need to use specific test parameters like `mean_tests` and `std_tests`. You can check metric outputs at the [All Metric page](/metrics/all_metrics).\\n</Info>\\n\\n### Test parameters\\n\\nHere are the conditions you can set:\\n\\n| Condition      | Explanation                                       | Example                                                |\\n| -------------- | ------------------------------------------------- | ------------------',\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'sing values and no values below 18 in the \"Age\" column:\\n\\n```Python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", tests=[eq(0)]),\\n    MinValue(column=\"Age\", tests=[gte(18)]),\\n])\\n```\\n\\nNote that you don\\'t need to use `include_tests` when setting Tests manually.\\n\\n<Info>\\n**Sometimes you may need to use other parameters to set test conditions**. The `tests` parameter applies when a metric returns a single value, or to test `count` for metrics that return both `count` and `share`. For metrics with multiple outputs (e.g. MAE returns `mean` and `std`), you may need to use specific test parameters like `mean_tests` and `std_tests`. You can check metric outputs at the [All Metric page](/metrics/all_metrics).\\n</Info>\\n\\n### Test parameters\\n\\nHere are the conditions you can set:\\n\\n| Condition      | Explanation                                       | Example                                                |\\n| -------------- | ------------------------------------------------- | ------------------------------------------------------ |\\n| `eq(val)`      | equal to <br /> `test_result == val` <br /><br />             | `MinValue(column=\"Age\", tests=[eq(18)])`         |\\n| `not_eq(val)`  | not equal <br /> `test_result != val`             | `MinValue(column=\"Age\", tests=[not_eq(18)])`           |\\n| `gt(val)`      | greater than  <br /> `test_result > val`          | `MinValue(column=\"Age\", tests=[gt(18)])`               |\\n| `gte(val)`     | greater than or equal <br /> `test_result >= val` | `MinValue(column=\"Age\", tests=[gte(18)])`              |\\n| `lt(val)`      | less than <br /> `test_result < val`              | `MinValue(column=\"Age\", tests=[lt(18)])`               |\\n| `lte(val)`     | less than or equal <br /> `test_result <= val`    | `MinValue(column=\"Age\", tests=[lte(18)])`              |\\n| `is_in: list`  | `test_result ==` one of the values                | `MinValue(column=\"Age\", tests=[is_in([18, 21, 30])])`  |\\n| `not_in: list` | `test_result !=` any of the values      ',\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': '------------------------------------ |\\n| `eq(val)`      | equal to <br /> `test_result == val` <br /><br />             | `MinValue(column=\"Age\", tests=[eq(18)])`         |\\n| `not_eq(val)`  | not equal <br /> `test_result != val`             | `MinValue(column=\"Age\", tests=[not_eq(18)])`           |\\n| `gt(val)`      | greater than  <br /> `test_result > val`          | `MinValue(column=\"Age\", tests=[gt(18)])`               |\\n| `gte(val)`     | greater than or equal <br /> `test_result >= val` | `MinValue(column=\"Age\", tests=[gte(18)])`              |\\n| `lt(val)`      | less than <br /> `test_result < val`              | `MinValue(column=\"Age\", tests=[lt(18)])`               |\\n| `lte(val)`     | less than or equal <br /> `test_result <= val`    | `MinValue(column=\"Age\", tests=[lte(18)])`              |\\n| `is_in: list`  | `test_result ==` one of the values                | `MinValue(column=\"Age\", tests=[is_in([18, 21, 30])])`  |\\n| `not_in: list` | `test_result !=` any of the values                | `MinValue(column=\"Age\", tests=[not_in([16, 17, 18])])` |\\n\\n**Additional parameters**. Some Metrics need extra parameters. For example, to check for values outside fixed range, you must set this range. To test that no value is out of 18-80 range:\\n\\n```python\\nreport = Report([\\n    OutRangeValueCount(column=\"Age\", left=18, right=80, tests=[eq(0)]),\\n])  \\n```\\n\\n<Info>\\n  **How to check available parameters?** Consult the [All Metrics](/metrics/all_metrics) reference table.\\n</Info>\\n\\n**Combine custom and default conditions**. You can use both default and custom conditions across the Report by setting `include_tests=True` and adding custom conditions where needed. \\n\\n```Python\\nreport = Report([\\n    RowCount(tests=[gt(10)]),\\n    MissingValueCount(column=\"Age\"),\\n],\\ninclude_tests=True) \\n```\\n\\nYour custom conditions override the defaults for those specific Tests where you add them.\\n\\n**Multiple conditions**. You can add multiple checks to the same Metric at once:\\n\\n```python\\nreport = Report([\\n',\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': '          | `MinValue(column=\"Age\", tests=[not_in([16, 17, 18])])` |\\n\\n**Additional parameters**. Some Metrics need extra parameters. For example, to check for values outside fixed range, you must set this range. To test that no value is out of 18-80 range:\\n\\n```python\\nreport = Report([\\n    OutRangeValueCount(column=\"Age\", left=18, right=80, tests=[eq(0)]),\\n])  \\n```\\n\\n<Info>\\n  **How to check available parameters?** Consult the [All Metrics](/metrics/all_metrics) reference table.\\n</Info>\\n\\n**Combine custom and default conditions**. You can use both default and custom conditions across the Report by setting `include_tests=True` and adding custom conditions where needed. \\n\\n```Python\\nreport = Report([\\n    RowCount(tests=[gt(10)]),\\n    MissingValueCount(column=\"Age\"),\\n],\\ninclude_tests=True) \\n```\\n\\nYour custom conditions override the defaults for those specific Tests where you add them.\\n\\n**Multiple conditions**. You can add multiple checks to the same Metric at once:\\n\\n```python\\nreport = Report([\\n    MinValue(column=\"Age\", tests=[gte(17), lte(19)]),\\n])\\n```\\n\\nThis creates two separate Tests for the Min value.\\n\\n**Testing count vs. share**. Some Metrics like `MissingValueCount` or `CategoryCount` return both absolute counts and percentage. The default `tests` parameter lets you set condition against the absolute value. To test the relative value, use `share_tests` parameter.\\n\\nTo test for fewer than 5 missing values (absolute):\\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", tests=[lte(5)])\\n])\\n```\\n\\nTo test for less than 10% missing values (relative):\\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", share_tests=[lte(0.1)]),\\n])\\n```\\n\\n### Tests relative to reference\\n\\n**Testing against reference**. If you pass a reference dataset, you can set conditions relative to the reference values. For example, to Test that the number of rows in the current dataset is equal or greater than the reference number of rows +/- 10%:\\n\\n```python\\nfrom evidently.future.tests i',\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': '    MinValue(column=\"Age\", tests=[gte(17), lte(19)]),\\n])\\n```\\n\\nThis creates two separate Tests for the Min value.\\n\\n**Testing count vs. share**. Some Metrics like `MissingValueCount` or `CategoryCount` return both absolute counts and percentage. The default `tests` parameter lets you set condition against the absolute value. To test the relative value, use `share_tests` parameter.\\n\\nTo test for fewer than 5 missing values (absolute):\\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", tests=[lte(5)])\\n])\\n```\\n\\nTo test for less than 10% missing values (relative):\\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", share_tests=[lte(0.1)]),\\n])\\n```\\n\\n### Tests relative to reference\\n\\n**Testing against reference**. If you pass a reference dataset, you can set conditions relative to the reference values. For example, to Test that the number of rows in the current dataset is equal or greater than the reference number of rows +/- 10%:\\n\\n```python\\nfrom evidently.future.tests import Reference\\n\\nreport = Report([\\n   RowCount(tests=[gte(Reference(relative=0.1))]),\\n])\\nmy_eval = report.run(eval_data_1, eval_data_2)\\n```\\n\\nYou can also define the absolute difference from reference:\\n\\n```python\\nreport = Report([\\n   RowCount(tests=[gte(Reference(absolute=5))]),\\n])\\n```\\n\\nThis checks that the the number of rows is greater or equal to reference +/-5.\\n\\n### Set Test criticality\\n\\nBy default, failed Tests return Fail. To get a Warning instead, set `is_critical=False`: \\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", share_tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nThis helps manage alert fatigue and prioritize Tests. If you [set alerts](/docs/platform/alerts) on failed Tests, the \"Warning\" result won\\'t trigger an alert. Warnings are labeled yellow.\\n\\nYou can also use this to set \"layered\" conditions. For example, get a Warning for any missing values, Fail if over 10%:\\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", \\n                      share_',\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'mport Reference\\n\\nreport = Report([\\n   RowCount(tests=[gte(Reference(relative=0.1))]),\\n])\\nmy_eval = report.run(eval_data_1, eval_data_2)\\n```\\n\\nYou can also define the absolute difference from reference:\\n\\n```python\\nreport = Report([\\n   RowCount(tests=[gte(Reference(absolute=5))]),\\n])\\n```\\n\\nThis checks that the the number of rows is greater or equal to reference +/-5.\\n\\n### Set Test criticality\\n\\nBy default, failed Tests return Fail. To get a Warning instead, set `is_critical=False`: \\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", share_tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nThis helps manage alert fatigue and prioritize Tests. If you [set alerts](/docs/platform/alerts) on failed Tests, the \"Warning\" result won\\'t trigger an alert. Warnings are labeled yellow.\\n\\nYou can also use this to set \"layered\" conditions. For example, get a Warning for any missing values, Fail if over 10%:\\n\\n```python\\nreport = Report([\\n    MissingValueCount(column=\"Age\", \\n                      share_tests=[eq(0, is_critical=False), \\n                                   lte(0.1, is_critical=True)]),\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n```',\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Check>\\n  Built-in alerting is a Pro feature available in the **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\n![](/images/alerts.png)\\n\\nTo enable alerts, open the Project and navigate to the \"Alerts\" in the left menu. You must set:\\n\\n* A notification channel.\\n\\n* An alert condition.\\n\\n## Notification channels\\n\\nYou can choose between the following options:\\n\\n* **Email**. Add email addresses to send alerts to.\\n\\n* **Slack**. Add a Slack webhook.\\n\\n* **Discord**. Add a Discord webhook.\\n\\n## Alert conditions\\n\\n### Failed tests\\n\\nIf you use Tests (conditional checks) in your Project, you can tie alerting to the failed Tests in a Test Suite. Toggle this option on the Alerts page. Evidently will set an alert to the defined channel if any of the Tests fail.\\n\\n<Tip>\\n  **How to avoid alert fatigue?** Use the `is_critical` parameter to mark non-critical Test as Warnings. Setting it to `False` prevent alerts for those checks even if they fail.\\n</Tip>\\n\\n### Custom conditions\\n\\nYou can also set alerts on individual Metric values. For example, you can generate Alerts when the share of drifting features is above a certain threshold.\\n\\nClick on the plus sign below the “Add new Metric alert” and follow the prompts to set an alert condition.\\n\\n![](../.gitbook/assets/cloud/alerts.png)',\n",
       "  'title': 'Alerts',\n",
       "  'description': 'How to set up alerts.',\n",
       "  'filename': 'docs-main/docs/platform/alerts.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'You can add Panels in the user interface or using Python API. This pages describes the Python API. Check how to [add panels in the UI](dashboard_add_panels_ui).\\n\\n## Dashboard Management\\n\\n<Check>\\n  Dashboards as code are available in Evidently OSS, Cloud, Enterprise.\\n</Check>\\n\\n<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).\\n</Tip>\\n\\n**Adding Tabs**. To add a new Tab:\\n\\n```python\\nproject.dashboard.add_tab(\"Another Tab\")\\n```\\n\\nYou can also create a new Tab while adding a Panel as shown below. If the destination Tab doesn\\'t exist, it will be created. If it does, the Panel will be added below existing ones in that Tab.\\n\\n**Deleting Tabs**. To delete a Tab:\\n\\n```python\\nproject.dashboard.delete_tab(\"Another Tab\")\\n```\\n\\n**Deleting Panels**. To delete a specific Panel:\\n\\n```python\\nproject.dashboard.delete_panel(\"Dashboard title\", \"My new tab\")\\n```\\n\\n(First list the Panel name, then the Tab name).\\n\\n**[DANGER]. Delete Dashboard**. To delete all Tabs and Panels on the Dashboard:\\n\\n```python\\nproject.dashboard.clear_dashboard()\\n```\\n\\nNote: This does **not** delete the underlying Reports or dataset; it only clears the Panels.\\n\\n## Adding Panels\\n\\nImports:\\n\\n```\\nfrom evidently.sdk.models import PanelMetric\\nfrom evidently.sdk.panels import DashboardPanelPlot\\n```\\n\\nYou can add multiple Panels at once: they will appear in the listed order.\\n\\n### Text\\n\\nText-only panels are perfect for titles.\\n\\n**Add a text panel**. Add a new text panel to the specified Tab.\\n\\n```python\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Dashboard title\",\\n        size=\"full\", \\n        values=[], #leave empty\\n        plot_params={\"plot_type\": \"text\"},\\n    ),\\n    tab=\"My new tab\", #will create a Tab if there is no Tab with this name\\n)\\n```\\n\\n### Counters\\n\\nCounter panels show a value with optional supporting text.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Text counter\" img=\"/images/dashboard/panel_counter_example-min.png\">\\n    Shows th',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'd**. To delete all Tabs and Panels on the Dashboard:\\n\\n```python\\nproject.dashboard.clear_dashboard()\\n```\\n\\nNote: This does **not** delete the underlying Reports or dataset; it only clears the Panels.\\n\\n## Adding Panels\\n\\nImports:\\n\\n```\\nfrom evidently.sdk.models import PanelMetric\\nfrom evidently.sdk.panels import DashboardPanelPlot\\n```\\n\\nYou can add multiple Panels at once: they will appear in the listed order.\\n\\n### Text\\n\\nText-only panels are perfect for titles.\\n\\n**Add a text panel**. Add a new text panel to the specified Tab.\\n\\n```python\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Dashboard title\",\\n        size=\"full\", \\n        values=[], #leave empty\\n        plot_params={\"plot_type\": \"text\"},\\n    ),\\n    tab=\"My new tab\", #will create a Tab if there is no Tab with this name\\n)\\n```\\n\\n### Counters\\n\\nCounter panels show a value with optional supporting text.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Text counter\" img=\"/images/dashboard/panel_counter_example-min.png\">\\n    Shows the specified value(s) and optional text.\\n  </Card>\\n  <Card title=\"Pie chart\" img=\"/images/dashboard/panel_pie_chart.png\">\\n    Shows the specified value(s) in a pie chart.\\n  </Card>\\n</CardGroup>\\n\\n**Add Counters**. To add panels for the `RowCount` metric with different aggregations:\\n\\n```python\\n# Sum\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Total number of evaluations over time.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"sum\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n\\n# Average\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Average number of evaluations per Report.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"avg\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n\\n# Last\\nproject.dashboard.add_panel(\\n   ',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'e specified value(s) and optional text.\\n  </Card>\\n  <Card title=\"Pie chart\" img=\"/images/dashboard/panel_pie_chart.png\">\\n    Shows the specified value(s) in a pie chart.\\n  </Card>\\n</CardGroup>\\n\\n**Add Counters**. To add panels for the `RowCount` metric with different aggregations:\\n\\n```python\\n# Sum\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Total number of evaluations over time.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"sum\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n\\n# Average\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Average number of evaluations per Report.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"avg\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n\\n# Last\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Latest number of evaluations.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"last\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n```\\n\\n**Add pie charts**. You can use the same aggregation params (`sum`, `last`, `avg`).\\n\\n```python\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Total number of evaluations over time.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"pie\", \"aggregation\": \"sum\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n```\\n\\n### Plots\\n\\nThese Panels display values as bar or line plots.\\n\\n<CardGroup cols={3}>\\n  <Card title=\"Line chart\" img=\"/images/dashboard/panel_line_chart.png\">\\n    Shows the selected values over time. You can add multiple series to the same chart as multiple lines.\\n  </Card>\\n  <Card title=\"Bar ',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Latest number of evaluations.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"last\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n```\\n\\n**Add pie charts**. You can use the same aggregation params (`sum`, `last`, `avg`).\\n\\n```python\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Row count\",\\n        subtitle=\"Total number of evaluations over time.\",\\n        size=\"half\",\\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\\n        plot_params={\"plot_type\": \"pie\", \"aggregation\": \"sum\"},\\n    ),\\n    tab=\"My tab\",\\n)\\n```\\n\\n### Plots\\n\\nThese Panels display values as bar or line plots.\\n\\n<CardGroup cols={3}>\\n  <Card title=\"Line chart\" img=\"/images/dashboard/panel_line_chart.png\">\\n    Shows the selected values over time. You can add multiple series to the same chart as multiple lines.\\n  </Card>\\n  <Card title=\"Bar chart (stacked)\" img=\"/images/dashboard/panel_dist_stacked_2-min.png\">\\n    Shows selected values or distributions over time (if stored in each Report). Stacked in a single bar.\\n  </Card>\\n  <Card title=\"Bar chart (grouped)\" img=\"/images/dashboard/panel_dist_group_2-min.png\">\\n    Shows selected values or distributions over time (if stored in each Report). Multiple bars.\\n  </Card>\\n</CardGroup>\\n\\n**Add Plots**. To add time series panels for the `RowCount` metric.\\n\\n```python\\n# line chart\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Row count\",\\n                subtitle = \"Number of evaluations over time.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Row count\",\\n                        metric=\"RowCount\",\\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"line\"},\\n            ),\\n            tab=\"My tab\",\\n        )\\n        \\n# bar chart\\nproject.dashbo',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'chart (stacked)\" img=\"/images/dashboard/panel_dist_stacked_2-min.png\">\\n    Shows selected values or distributions over time (if stored in each Report). Stacked in a single bar.\\n  </Card>\\n  <Card title=\"Bar chart (grouped)\" img=\"/images/dashboard/panel_dist_group_2-min.png\">\\n    Shows selected values or distributions over time (if stored in each Report). Multiple bars.\\n  </Card>\\n</CardGroup>\\n\\n**Add Plots**. To add time series panels for the `RowCount` metric.\\n\\n```python\\n# line chart\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Row count\",\\n                subtitle = \"Number of evaluations over time.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Row count\",\\n                        metric=\"RowCount\",\\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"line\"},\\n            ),\\n            tab=\"My tab\",\\n        )\\n        \\n# bar chart\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Row count\",\\n                subtitle = \"Number of evaluations over time.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Row count\",\\n                        metric=\"RowCount\",\\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"bar\", \"is_stacked\": False}, #default False, set as True to get stacked bars\\n            ),\\n            tab=\"My tab\",\\n        )\\n```\\n\\n**Multiple values**. A single Panel can show multiple values. For example, this will add multiple lines on a Line chart:\\n\\n```python\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Text Length\",\\n        subtitle=\"Text length stats (symbols).\",\\n        size=\"full\",\\n        values=[\\n            PanelMetric(legend=\"max\", metric=\"MaxValue\", metric_labels={\"column\": \"length\"}),\\n            PanelMetric(legend=\"mean\", metric=\"MeanValue\", metric_la',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'ard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Row count\",\\n                subtitle = \"Number of evaluations over time.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Row count\",\\n                        metric=\"RowCount\",\\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"bar\", \"is_stacked\": False}, #default False, set as True to get stacked bars\\n            ),\\n            tab=\"My tab\",\\n        )\\n```\\n\\n**Multiple values**. A single Panel can show multiple values. For example, this will add multiple lines on a Line chart:\\n\\n```python\\nproject.dashboard.add_panel(\\n    DashboardPanelPlot(\\n        title=\"Text Length\",\\n        subtitle=\"Text length stats (symbols).\",\\n        size=\"full\",\\n        values=[\\n            PanelMetric(legend=\"max\", metric=\"MaxValue\", metric_labels={\"column\": \"length\"}),\\n            PanelMetric(legend=\"mean\", metric=\"MeanValue\", metric_labels={\"column\": \"length\"}),\\n            PanelMetric(legend=\"min\", metric=\"MinValue\", metric_labels={\"column\": \"length\"}),\\n        ]\\n    )\\n)\\n```\\n\\n### Dashboard Panel options\\n\\nA summary of all parameters:\\n\\n| Parameter              | Type   | Required | Default  | Description                                                                               |\\n| ---------------------- | ------ | -------- | -------- | ----------------------------------------------------------------------------------------- |\\n| `title`                | `str`  | ❌        | `None`   | Title of the panel.                                                                       |\\n| `description`          | `str`  | ❌        | `None`   | Optional panel description shown as a subtitle.                                           |\\n| `size`                 | `str`  | ❌        | `\"full\"` | Panel size: `\"full\"` (100% width) or `\"half\"` (50%).                                      |\\n| `values`               | `list` | ✅        |',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'bels={\"column\": \"length\"}),\\n            PanelMetric(legend=\"min\", metric=\"MinValue\", metric_labels={\"column\": \"length\"}),\\n        ]\\n    )\\n)\\n```\\n\\n### Dashboard Panel options\\n\\nA summary of all parameters:\\n\\n| Parameter              | Type   | Required | Default  | Description                                                                               |\\n| ---------------------- | ------ | -------- | -------- | ----------------------------------------------------------------------------------------- |\\n| `title`                | `str`  | ❌        | `None`   | Title of the panel.                                                                       |\\n| `description`          | `str`  | ❌        | `None`   | Optional panel description shown as a subtitle.                                           |\\n| `size`                 | `str`  | ❌        | `\"full\"` | Panel size: `\"full\"` (100% width) or `\"half\"` (50%).                                      |\\n| `values`               | `list` | ✅        | —        | List of `PanelMetric` objects to display.                                                 |\\n| `tab`                  | `str`  | ❌        | `None`   | Dashboard tab name. If not set, defaults to the first tab or creates a new \"General\" tab. |\\n| `create_if_not_exists` | `bool` | ❌        | `True`   | If `True`, creates the tab if it doesn\\'t exist. Throws exception if `False`.              |\\n| `plot_params`          | `dict` | ❌        | `{}`     | Panel visualization settings like `\"plot_type\"`: `\"text\"`, `\"line\"`, `\"counter\"`.         |\\n\\n## Configuring Panel values\\n\\n### Metric\\n\\nTo define which value the Panel displays, you must reference the name of the corresponding Evidently Metric. This metric must be present in the Reports logged to your Project. If the metric isn\\'t present, the Panel will appear empty.\\n\\n**Dataset-level Metrics**: pass the Metric name directly to `PanelMetric`, e.g., `\"RowCount\"`.\\n\\nExample:\\n\\n```python\\nproject.dashboard.add_panel(\\n             DashboardPa',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': ' —        | List of `PanelMetric` objects to display.                                                 |\\n| `tab`                  | `str`  | ❌        | `None`   | Dashboard tab name. If not set, defaults to the first tab or creates a new \"General\" tab. |\\n| `create_if_not_exists` | `bool` | ❌        | `True`   | If `True`, creates the tab if it doesn\\'t exist. Throws exception if `False`.              |\\n| `plot_params`          | `dict` | ❌        | `{}`     | Panel visualization settings like `\"plot_type\"`: `\"text\"`, `\"line\"`, `\"counter\"`.         |\\n\\n## Configuring Panel values\\n\\n### Metric\\n\\nTo define which value the Panel displays, you must reference the name of the corresponding Evidently Metric. This metric must be present in the Reports logged to your Project. If the metric isn\\'t present, the Panel will appear empty.\\n\\n**Dataset-level Metrics**: pass the Metric name directly to `PanelMetric`, e.g., `\"RowCount\"`.\\n\\nExample:\\n\\n```python\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Row count\",\\n                subtitle = \"Number of evaluations over time.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Row count\",\\n                        metric=\"RowCount\", ## <- metric name\\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"line\"},\\n            ),\\n            tab=\"My tab\",\\n        )\\n```\\n\\n**Presets** (like `TextEvals`, `ClassificationPreset`, `DataDriftPreset`) contain multiple sub-metrics. When logging Reports using a Preset, you must reference the specific **metric** inside it, such as `Accuracy`, `Recall`, etc.\\n\\n<Info>\\n  **Need help finding metric names?** See the [All Metrics Reference Table](/metrics/all_metrics) for a full list of Metrics.\\n</Info>\\n\\n### Metric labels\\n\\nSome Metrics require additional context. This applies when the metrics:\\n\\n- Operate at the column level\\n- Return multiple values (metric results)\\n- Have user-def',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'nelPlot(\\n                title=\"Row count\",\\n                subtitle = \"Number of evaluations over time.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Row count\",\\n                        metric=\"RowCount\", ## <- metric name\\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"line\"},\\n            ),\\n            tab=\"My tab\",\\n        )\\n```\\n\\n**Presets** (like `TextEvals`, `ClassificationPreset`, `DataDriftPreset`) contain multiple sub-metrics. When logging Reports using a Preset, you must reference the specific **metric** inside it, such as `Accuracy`, `Recall`, etc.\\n\\n<Info>\\n  **Need help finding metric names?** See the [All Metrics Reference Table](/metrics/all_metrics) for a full list of Metrics.\\n</Info>\\n\\n### Metric labels\\n\\nSome Metrics require additional context. This applies when the metrics:\\n\\n- Operate at the column level\\n- Return multiple values (metric results)\\n- Have user-defined custom parameters\\n\\nIn these cases, use `metric_labels` to specify what exactly you want to plot.\\n\\n**Example**. To plot the share of categories inside \"Denials\" column:\\n\\n```python\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Denials\",\\n                subtitle = \"Number of denials.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"\"\"{{label}}\"\"\",\\n                        metric=\"UniqueValueCount\", # <- metric from TextEvals Preset that computes distinct values\\n                        metric_labels={\"column\": \"denials\", #column name\\n                                       \"value_type\": \"share\" #metric result\\n                                       } \\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"bar\", \"is_stacked\": True},\\n            ),\\n            tab=\"My tab\",\\n        )\\n```\\n\\n**Column / Descriptor**. When you compute a text descriptor or ',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'ined custom parameters\\n\\nIn these cases, use `metric_labels` to specify what exactly you want to plot.\\n\\n**Example**. To plot the share of categories inside \"Denials\" column:\\n\\n```python\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Denials\",\\n                subtitle = \"Number of denials.\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"\"\"{{label}}\"\"\",\\n                        metric=\"UniqueValueCount\", # <- metric from TextEvals Preset that computes distinct values\\n                        metric_labels={\"column\": \"denials\", #column name\\n                                       \"value_type\": \"share\" #metric result\\n                                       } \\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"bar\", \"is_stacked\": True},\\n            ),\\n            tab=\"My tab\",\\n        )\\n```\\n\\n**Column / Descriptor**. When you compute a text descriptor or any metric that operates at the column level, use the `column` label to specify which column or descriptor it refers to.\\n\\nFor example, in a `TextEvals` Report, each text descriptor (e.g., text length, LLM judged \"denials\", etc.) is treated as a column. These descriptors are summarized with various statistics. To plot one of these values, you need to:\\n\\n- Choose a summary Metric like `UniqueValueCount`, `MissingValueCount`, `MaxValue`, etc.\\n- Use the `column` label to point the specific descriptor.\\n\\n**Example**. To plot the min value from the \"Text Length\" column:\\n\\n```python\\nvalues=[\\n    PanelMetric(\\n        legend=\"Min text length\",\\n        metric=\"MinValue\", # <- metric from TextEvals Preset that computes min value\\n        metric_labels={\\n            \"column\": \"TextLength\",  # <- target column name \\n        }\\n    )\\n]\\n```\\n\\n**Value type**. Most Evidently Metrics return a single `value`. For example, `Accuracy` returns the corresponding accuracy `value`. So listing just the `Metric` name ',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'any metric that operates at the column level, use the `column` label to specify which column or descriptor it refers to.\\n\\nFor example, in a `TextEvals` Report, each text descriptor (e.g., text length, LLM judged \"denials\", etc.) is treated as a column. These descriptors are summarized with various statistics. To plot one of these values, you need to:\\n\\n- Choose a summary Metric like `UniqueValueCount`, `MissingValueCount`, `MaxValue`, etc.\\n- Use the `column` label to point the specific descriptor.\\n\\n**Example**. To plot the min value from the \"Text Length\" column:\\n\\n```python\\nvalues=[\\n    PanelMetric(\\n        legend=\"Min text length\",\\n        metric=\"MinValue\", # <- metric from TextEvals Preset that computes min value\\n        metric_labels={\\n            \"column\": \"TextLength\",  # <- target column name \\n        }\\n    )\\n]\\n```\\n\\n**Value type**. Most Evidently Metrics return a single `value`. For example, `Accuracy` returns the corresponding accuracy `value`. So listing just the `Metric` name is enough to specify what exactly you want to plot.\\n\\nHowever, some metrics produce more than one metric result, like:\\n\\n- `CategoryCount`: returns both `share` and `count`\\n- `MAE`: returns both `mean` and `std`\\n\\nIn this case, you must point to which value you want using the `value_type` key, e.g. `{\"value_type\": \"share\"}`\\n\\n```python\\nvalues = [\\n    PanelMetric(\\n        legend=\"Share\",\\n        metric=\"DriftedColumnsCount\",  # <- metric from Data Drift Preset that returns `count` or `share` of drifting columns\\n        metric_labels={\"value_type\": \"share\"}  # <- plot relative share\\n    ),\\n]\\n```\\n\\n<Info>\\n  **How to verify the metric result for a specific metric?**\\n\\n  - Look up the expected outputs in the [All Metrics Table](/metrics/all_metrics).\\n  - Or, generate a Report with the target `metric` and inspect its structure via`report.dict()` or `report.json()`.\\n</Info>\\n\\n**Metrics with extra parameters**. If a metric has configurable options (like drift method), you must also include those in `',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'is enough to specify what exactly you want to plot.\\n\\nHowever, some metrics produce more than one metric result, like:\\n\\n- `CategoryCount`: returns both `share` and `count`\\n- `MAE`: returns both `mean` and `std`\\n\\nIn this case, you must point to which value you want using the `value_type` key, e.g. `{\"value_type\": \"share\"}`\\n\\n```python\\nvalues = [\\n    PanelMetric(\\n        legend=\"Share\",\\n        metric=\"DriftedColumnsCount\",  # <- metric from Data Drift Preset that returns `count` or `share` of drifting columns\\n        metric_labels={\"value_type\": \"share\"}  # <- plot relative share\\n    ),\\n]\\n```\\n\\n<Info>\\n  **How to verify the metric result for a specific metric?**\\n\\n  - Look up the expected outputs in the [All Metrics Table](/metrics/all_metrics).\\n  - Or, generate a Report with the target `metric` and inspect its structure via`report.dict()` or `report.json()`.\\n</Info>\\n\\n**Metrics with extra parameters**. If a metric has configurable options (like drift method), you must also include those in `metric_labels`.\\n\\n### `PanelMetric` options\\n\\nA summary of all parameters:\\n\\n| Parameter       | Type   | Required | Default | Description                                                                      |\\n| --------------- | ------ | -------- | ------- | -------------------------------------------------------------------------------- |\\n| `legend`        | `str`  | ❌        | `None`  | Legend name in the panel. If `None`, one is auto-generated.                      |\\n| `tags`          | `list` | ❌        | `[]`    | Optional tags to select values only from a subset of Reports in the Project.     |\\n| `metadata`      | `dict` | ❌        | `{}`    | Optional metadata to select values only from a subset of Reports in the Project. |\\n| `metric`        | `str`  | ✅        | —       | Metric name (e.g., `\"RowCount\"`).                                                |\\n| `metric_labels` | `dict` | ❌        | `{}`    | Parameters like `column` names (applies to descriptors too) or `value_type`.  ',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': 'metric_labels`.\\n\\n### `PanelMetric` options\\n\\nA summary of all parameters:\\n\\n| Parameter       | Type   | Required | Default | Description                                                                      |\\n| --------------- | ------ | -------- | ------- | -------------------------------------------------------------------------------- |\\n| `legend`        | `str`  | ❌        | `None`  | Legend name in the panel. If `None`, one is auto-generated.                      |\\n| `tags`          | `list` | ❌        | `[]`    | Optional tags to select values only from a subset of Reports in the Project.     |\\n| `metadata`      | `dict` | ❌        | `{}`    | Optional metadata to select values only from a subset of Reports in the Project. |\\n| `metric`        | `str`  | ✅        | —       | Metric name (e.g., `\"RowCount\"`).                                                |\\n| `metric_labels` | `dict` | ❌        | `{}`    | Parameters like `column` names (applies to descriptors too) or `value_type`.     |',\n",
       "  'title': 'Add dashboard panels (API)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Dashboards let you create Panels to visualize evaluation results over time. Note that to be able to populate the panels, you must first add Reports with evaluation results to the Project.\\n\\n<Check>\\n  No-code Dashboards are available in the Evidently Cloud and Enterprise.\\n</Check>\\n\\n## Adding Tabs\\n\\nBy default, new Panels appear on a single Dashboard. You can add multiple Tabs to organize them.\\n\\n**To add a Tab**:\\n\\n- Enter \"Edit\" mode on the Dashboard (top right corner).\\n- Click the plus sign with \"add Tab\" on the left.\\n- To create a custom Tab, select \"empty\" and enter a name.\\n\\nTo simplify setup, you can start with pre-built Tabs. These are dashboard templates with preset Panel combinations:\\n\\n![Add Dashboard Tab](/images/dashboard/add_dashboard_tab_v2.gif)\\n\\n**Pre-built Tabs** rely on having related Metrics (or Presets that include the specific Metrics) within the Project. If the necessary data is not available, the Panels will appear empty until you add Reports that contain those Metrics.\\n\\nAvailable Tabs:\\n\\n| Template    | Description                                                                                                                    | Data source                                                                 |\\n| ----------- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- |\\n| **Columns** | Shows the results of text evaluations over time OR plots column distributions over time for categorical and numerical columns. | `TextEvals()`, `DataSumaryPreset()`or`ValueStats()` for individual columns. |\\n\\n**To delete a Tab**: enter the \"Edit\" mode again, choose \"edit Tabs\" sign next to the Tab names on the left, and choose which one to delete.\\n\\n## Adding Panels\\n\\nYou can add any number of Panels to your Dashboard, including text panels, counters, pie charts, line plots, and bar plots (grouped and stacked). When y',\n",
       "  'title': 'Add dashboard panels (UI)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '\\nAvailable Tabs:\\n\\n| Template    | Description                                                                                                                    | Data source                                                                 |\\n| ----------- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- |\\n| **Columns** | Shows the results of text evaluations over time OR plots column distributions over time for categorical and numerical columns. | `TextEvals()`, `DataSumaryPreset()`or`ValueStats()` for individual columns. |\\n\\n**To delete a Tab**: enter the \"Edit\" mode again, choose \"edit Tabs\" sign next to the Tab names on the left, and choose which one to delete.\\n\\n## Adding Panels\\n\\nYou can add any number of Panels to your Dashboard, including text panels, counters, pie charts, line plots, and bar plots (grouped and stacked). When you create a Panel, you pull the corresponding value from multiple Reports and show it over time or using the specified aggregation (sum, average, last).\\n\\n<Info>\\n  Check the preview and description of each Panel here: [How to add panels via Python API](dashboard_add_panels).\\n</Info>\\n\\n**How to add a Panel:**\\n\\n- Enter \"Edit\" mode on the Dashboard (top right corner).\\n- Click on the \"Add Panel\" button next to it.\\n- Follow the prompts to configure the panel.\\n- Use the preview to review your setup.\\n- Click \"Save\" and select the Tab where you want to add the Panel.\\n\\nHere is an example of the panel configuration view:\\n\\n![](/images/dashboard/add_panel_ui.png)\\n\\n- **Select Metrics**. To point to a specific Metric, you must choose the Metric name that matches the name of the Evidently Metric logged inside the Reports in the given Project.\\n- **Filter by Tag.** By default, the metrics will be parsed from all Reports in the Project. Use the \"From\" field to filter by Tags. (You must first attach these ',\n",
       "  'title': 'Add dashboard panels (UI)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ou create a Panel, you pull the corresponding value from multiple Reports and show it over time or using the specified aggregation (sum, average, last).\\n\\n<Info>\\n  Check the preview and description of each Panel here: [How to add panels via Python API](dashboard_add_panels).\\n</Info>\\n\\n**How to add a Panel:**\\n\\n- Enter \"Edit\" mode on the Dashboard (top right corner).\\n- Click on the \"Add Panel\" button next to it.\\n- Follow the prompts to configure the panel.\\n- Use the preview to review your setup.\\n- Click \"Save\" and select the Tab where you want to add the Panel.\\n\\nHere is an example of the panel configuration view:\\n\\n![](/images/dashboard/add_panel_ui.png)\\n\\n- **Select Metrics**. To point to a specific Metric, you must choose the Metric name that matches the name of the Evidently Metric logged inside the Reports in the given Project.\\n- **Filter by Tag.** By default, the metrics will be parsed from all Reports in the Project. Use the \"From\" field to filter by Tags. (You must first attach these tags to the corresponding Reports).\\n- **Filter by Metric label.** If you have a single Metric of that type in the Project (like `RowCount`), it may be enough to just specify the metric name. However, if you have multiple instances of the same metric - as is often the case for column-level Metrics like `UniqueValueCount` - you need to specify additional parameters. Use the \"Where\" selector to specify further keys like:\\n  - **Column**: Use this to select the name of the column or descriptor.\\n  - **Value type**: Choose whether to plot value or count for metrics that return both.\\n\\n  You can see all keys available for a given Metric in the dropdown menu. You can add multiple keys, depending on the metric type, like metric-specific parameters.\\n- **Set Legend**. You can use the \"Label\" field to modify what appears on the legend.\\n- **Set Panel Type**. You can also specify the plot type and aggregation level.\\n\\nFor example, you can switch the view for the same metric as on the screenshot above t',\n",
       "  'title': 'Add dashboard panels (UI)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'tags to the corresponding Reports).\\n- **Filter by Metric label.** If you have a single Metric of that type in the Project (like `RowCount`), it may be enough to just specify the metric name. However, if you have multiple instances of the same metric - as is often the case for column-level Metrics like `UniqueValueCount` - you need to specify additional parameters. Use the \"Where\" selector to specify further keys like:\\n  - **Column**: Use this to select the name of the column or descriptor.\\n  - **Value type**: Choose whether to plot value or count for metrics that return both.\\n\\n  You can see all keys available for a given Metric in the dropdown menu. You can add multiple keys, depending on the metric type, like metric-specific parameters.\\n- **Set Legend**. You can use the \"Label\" field to modify what appears on the legend.\\n- **Set Panel Type**. You can also specify the plot type and aggregation level.\\n\\nFor example, you can switch the view for the same metric as on the screenshot above to a pie chart and set the view to show only the last value instead of all values over time:\\n\\n![](/images/dashboard/add_panel_ui_pie.png)\\n\\n## **Deleting/Editing**\\n\\nTo delete or edit a Panel, enter Edit mode and hover over a specific Panel to choose an action.',\n",
       "  'title': 'Add dashboard panels (UI)',\n",
       "  'description': 'How to design your Dashboard with custom Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"<Check>\\n  Dashboard is available in **Evidently OSS**, **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\n## What is a Dashboard?\\n\\nA Dashboard provides a clear view of your AI application performance. You can use it:\\n\\n- to track evaluation results across multiple experiments;\\n- to track live production quality over time.\\n\\nEach Project has its own Dashboard. It's empty at first.\\n\\nTo populate it, you need to run an evaluation and **save at least one Report** to the Project. You can then choose values from Reports to plot.\\n\\n![](/images/dashboard_llm_light.png)\\n\\n## Dashboard Tabs\\n\\n<Check>\\n  Multiple Tabs are available in **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\nYou can logically organize Panels within the same Dashboard into different Tabs.\\n\\n![](/images/dashboard_llm_tabs.gif)\\n\\n## **Dashboard Panels**\\n\\nA Panel is a visual element in the Dashboard that displays specified values in a single widget. Panels can be counters, line plots, bar plots, etc. You can add multiple Panels to the Dashboard and customize their type and values shown.\\n\\nYou can add Panels in two ways:\\n- Using the Python API – define your Dashboard as code.\\n- Through the UI – add Panels directly from the interface (Cloud and Enterprise only).\\n  \\nTo create a Panel, you need to specify:\\n- Value – choose an individual metric to plot.\\n- Parameters – such as title, panel type, and size.\\n- Tags (optional) – use to filter and visualize subsets of your data.\\n\\n## From Dashboard to Reports\\n\\nBy clicking on any individual value on the Dashboard, you can open the associated Report and source Dataset for further debugging.\\n\\n![](/images/dashboard/dashboard_to_report.gif)\\n\\n## Data source\\n\\nDashboards rely on having **Reports** in the Project as a data source. \\n\\nWhen adding a Panel, you select a **Metric**, and Evidently pulls the corresponding value(s) from all Reports in the Project to plot them.\\n\\nFor example, if you log multiple Data Drift Reports (each includes the`DriftedColumnsCount` for th\",\n",
       "  'title': 'Overview',\n",
       "  'description': 'Introduction to Dashboard.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"ltiple Panels to the Dashboard and customize their type and values shown.\\n\\nYou can add Panels in two ways:\\n- Using the Python API – define your Dashboard as code.\\n- Through the UI – add Panels directly from the interface (Cloud and Enterprise only).\\n  \\nTo create a Panel, you need to specify:\\n- Value – choose an individual metric to plot.\\n- Parameters – such as title, panel type, and size.\\n- Tags (optional) – use to filter and visualize subsets of your data.\\n\\n## From Dashboard to Reports\\n\\nBy clicking on any individual value on the Dashboard, you can open the associated Report and source Dataset for further debugging.\\n\\n![](/images/dashboard/dashboard_to_report.gif)\\n\\n## Data source\\n\\nDashboards rely on having **Reports** in the Project as a data source. \\n\\nWhen adding a Panel, you select a **Metric**, and Evidently pulls the corresponding value(s) from all Reports in the Project to plot them.\\n\\nFor example, if you log multiple Data Drift Reports (each includes the`DriftedColumnsCount` for the corresponding batch), you can plot how this Metric value changes over time.\\n\\n![](/images/dashboard/metric_panels.png)\\n\\nThe Panel time resolution depends on logged Report frequency. For instance, if you log Reports daily, you'll see values at daily granularity.\\n\\nYou can use **Tags** to filter data from specific Reports. For example, you can plot the accuracy of Model A and Model B on separate Panels. To achieve this, you must first [add relevant Tags](/docs/library/tags_metadata) to the Report, and then filter by these Tags when creating a Panel.\\n\\n## What’s next?\\n\\n- See how to [customize dashboard via API](/docs/platform/dashboard_add_panels).\\n- See how to [customize dashboard via UI](/docs/platform/dashboard_add_panels_ui).\",\n",
       "  'title': 'Overview',\n",
       "  'description': 'Introduction to Dashboard.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'This applies to versions 0.6.0 to 0.7.1 for Cloud/Workspace v1. \\n\\n## What is a monitoring Panel?\\n\\nA monitoring Panel is an individual plot or counter on the Monitoring Dashboard.\\xa0\\n\\n* You can add multiple Panels and organize them by **Tabs**. You can customize Panel type, values shown, titles and legends.\\n\\n* When adding a Panel, you choose a **Test** or **Metric** with the specific value (\"metric result\") inside it. Evidently pulls corresponding value(s) from all Reports in the Project and plots them on the Panel.\\n\\n* You can use **Tags** to filter data from specific Reports. For example, you can plot the accuracy of Model A and Model B next to each other. To achieve this, [add relevant Tags](/docs/library/tags_metadata) to the Report.\\n\\n<Info>\\n  **How to add Panels**. This page explains the Panel types. Check the next section on [adding Panels](/docs/platform/dashboard_add_panels).\\n</Info>\\n\\n**Panel types.** There are 3 main panel types:\\n\\n* **Metric panels** plot individual values from inside Reports.\\n\\n* **Test panels** show pass/fail Test outcomes in time.\\n\\n* **Distribution panels** plot distributions over time.\\n\\n## Metric Panels\\n\\nMetric Panels (`DashboardPanel`) show individual values from inside the Reports in time.\\n\\nFor example, if you capture Data Summary Reports (include mean, max, min, etc., for each column) or Data Drift Reports (include the share of drifting columns and per-column drift score), you can plot any of these values in time.\\n\\n![](/images/dashboard/metric_panels.png)\\n\\n**Panel time resolution** depends on Report frequency. For instance, if you log Data Drift Reports daily, you can plot the share of drifting features with daily granularity. You can also open the source Report to see feature distributions on a specific day.\\n\\n### Counter\\n\\nClass `DashboardPanelCounter`.\\n\\nShows a value with supporting text or text alone. Perfect for dashboard titles.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_counter_example-min.png\" />\\n</Frame>\\n\\n### Plot\\n\\nClass `Dashboa',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'side Reports.\\n\\n* **Test panels** show pass/fail Test outcomes in time.\\n\\n* **Distribution panels** plot distributions over time.\\n\\n## Metric Panels\\n\\nMetric Panels (`DashboardPanel`) show individual values from inside the Reports in time.\\n\\nFor example, if you capture Data Summary Reports (include mean, max, min, etc., for each column) or Data Drift Reports (include the share of drifting columns and per-column drift score), you can plot any of these values in time.\\n\\n![](/images/dashboard/metric_panels.png)\\n\\n**Panel time resolution** depends on Report frequency. For instance, if you log Data Drift Reports daily, you can plot the share of drifting features with daily granularity. You can also open the source Report to see feature distributions on a specific day.\\n\\n### Counter\\n\\nClass `DashboardPanelCounter`.\\n\\nShows a value with supporting text or text alone. Perfect for dashboard titles.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_counter_example-min.png\" />\\n</Frame>\\n\\n### Plot\\n\\nClass `DashboardPanelPlot`. Shows individual values as bar, line, scatter plot, or histogram.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Line chart\" img=\"/images/dashboard//panel_line_plot_example.png\">\\n    `PlotType.LINE` shows values over time from multiple Reports.\\n  </Card>\\n\\n  <Card title=\"Bar chart\" img=\"/images/dashboard//panel_bar_plot_example.png\">\\n    `PlotType.BAR` shows values over time from multiple Report.\\n  </Card>\\n\\n  <Card title=\"Scatter plot\" img=\"/images/dashboard//panel_scatter_plot_example.png\">\\n    `PlotType.SCATTER` shows values over time from multiple Reports.\\n  </Card>\\n\\n  <Card title=\"Histogram\" img=\"/images/dashboard//panel_hist_example.png\">\\n    `PlotType.HISTOGRAM` shows the frequency of individual values across Reports.\\n  </Card>\\n</CardGroup>\\n\\n## Test Panels\\n\\nTest Panels show the Test results.\\n\\nAs you run the same Tests repeatedly, you can visualize the pass/fail outcomes or result counts. You choose which Test results to include.\\n\\n![](/images/dashboard/test_panels.png)\\n\\nTest Pa',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'rdPanelPlot`. Shows individual values as bar, line, scatter plot, or histogram.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Line chart\" img=\"/images/dashboard//panel_line_plot_example.png\">\\n    `PlotType.LINE` shows values over time from multiple Reports.\\n  </Card>\\n\\n  <Card title=\"Bar chart\" img=\"/images/dashboard//panel_bar_plot_example.png\">\\n    `PlotType.BAR` shows values over time from multiple Report.\\n  </Card>\\n\\n  <Card title=\"Scatter plot\" img=\"/images/dashboard//panel_scatter_plot_example.png\">\\n    `PlotType.SCATTER` shows values over time from multiple Reports.\\n  </Card>\\n\\n  <Card title=\"Histogram\" img=\"/images/dashboard//panel_hist_example.png\">\\n    `PlotType.HISTOGRAM` shows the frequency of individual values across Reports.\\n  </Card>\\n</CardGroup>\\n\\n## Test Panels\\n\\nTest Panels show the Test results.\\n\\nAs you run the same Tests repeatedly, you can visualize the pass/fail outcomes or result counts. You choose which Test results to include.\\n\\n![](/images/dashboard/test_panels.png)\\n\\nTest Panels only work with Test Suites: you must add Tests to the Metrics inside your Report to be able to render these panels.\\n\\n### Test counter\\n\\nClass `DashboardPanelTestSuiteCounter`. Shows a counter of Tests with specified status.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_tests_counter_example.png\" />\\n</Frame>\\n\\n### Test plot\\n\\nClass `DashboardPanelTestSuite`.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Detailed plot\" img=\"/images/dashboard/panel_tests_detailed_hover_example.png\">\\n    `TestSuitePanelType.DETAILED`. Individual Test results are visible\\n  </Card>\\n\\n  <Card title=\"Aggregated plot\" img=\"/images/dashboard/panel_tests_aggregated_hover_example.png\">\\n    `TestSuitePanelType.AGGREGATE`. Only the total number of Tests by status is visible.\\n  </Card>\\n</CardGroup>\\n\\n## Distribution Panel\\n\\nClass `DashboardPanelDistribution`. Shows a distribution of values over time.\\n\\n![](/images/dashboard//distribution_panels.png)\\n\\nFor example, if you capture Text Evals or Data Summary that include histograms ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'nels only work with Test Suites: you must add Tests to the Metrics inside your Report to be able to render these panels.\\n\\n### Test counter\\n\\nClass `DashboardPanelTestSuiteCounter`. Shows a counter of Tests with specified status.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_tests_counter_example.png\" />\\n</Frame>\\n\\n### Test plot\\n\\nClass `DashboardPanelTestSuite`.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Detailed plot\" img=\"/images/dashboard/panel_tests_detailed_hover_example.png\">\\n    `TestSuitePanelType.DETAILED`. Individual Test results are visible\\n  </Card>\\n\\n  <Card title=\"Aggregated plot\" img=\"/images/dashboard/panel_tests_aggregated_hover_example.png\">\\n    `TestSuitePanelType.AGGREGATE`. Only the total number of Tests by status is visible.\\n  </Card>\\n</CardGroup>\\n\\n## Distribution Panel\\n\\nClass `DashboardPanelDistribution`. Shows a distribution of values over time.\\n\\n![](/images/dashboard//distribution_panels.png)\\n\\nFor example, if you capture Text Evals or Data Summary that include histograms for categorical values, you can plot how the frequency of categories changes.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Stacked\" img=\"/images/dashboard/panel_dist_stacked_2-min.png\">\\n    `barmode=\"stack\"`: stacked bar chart shows **absolute** counts in a single bar.\\n  </Card>\\n\\n  <Card title=\"Grouped\" img=\"/images/dashboard/panel_dist_group_2-min.png\">\\n    `barmode=\"group\"`: grouped bar chart shows **absolute** counts in separate bars.\\n  </Card>\\n\\n  <Card title=\"Overlay\" img=\"/images/dashboard/panel_dist_overlay-min.png\">\\n    `barmode=\"overlay\"`: overlay bar chart shows overlaying **absolute** counts.\\n  </Card>\\n\\n  <Card title=\"Relative\" img=\"/images/dashboard/panel_dist_relative-min.png\">\\n    `barmode=\"relative\"`: relative bar chart shows stacked **relative** frequency.\\n  </Card>\\n</CardGroup>\\n\\n<Info>\\n  **What is the difference between a Distribution panel and a Histogram?** A histogram plot (`DashboardPanelPlot` with`PlotType.HISTOGRAM`) shows the distribution of the selected values from all ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'for categorical values, you can plot how the frequency of categories changes.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Stacked\" img=\"/images/dashboard/panel_dist_stacked_2-min.png\">\\n    `barmode=\"stack\"`: stacked bar chart shows **absolute** counts in a single bar.\\n  </Card>\\n\\n  <Card title=\"Grouped\" img=\"/images/dashboard/panel_dist_group_2-min.png\">\\n    `barmode=\"group\"`: grouped bar chart shows **absolute** counts in separate bars.\\n  </Card>\\n\\n  <Card title=\"Overlay\" img=\"/images/dashboard/panel_dist_overlay-min.png\">\\n    `barmode=\"overlay\"`: overlay bar chart shows overlaying **absolute** counts.\\n  </Card>\\n\\n  <Card title=\"Relative\" img=\"/images/dashboard/panel_dist_relative-min.png\">\\n    `barmode=\"relative\"`: relative bar chart shows stacked **relative** frequency.\\n  </Card>\\n</CardGroup>\\n\\n<Info>\\n  **What is the difference between a Distribution panel and a Histogram?** A histogram plot (`DashboardPanelPlot` with`PlotType.HISTOGRAM`) shows the distribution of the selected values from all Reports. Each source Report contains a **single value** (e.g., a \"mean value\"). A Distribution Panel (`DashboardPanelDistribution`) shows how a distribution changes over time. Each source Report contains a **histogram** (e.g. frequency of different categories).\\n</Info>\\n\\n## What\\'s next?\\n\\nHow to add [monitoring Panels and Tabs](/docs/platform/dashboard_add_panels).\\n\\n---\\ntitle: \\'Add dashboard panels\\'\\ndescription: \\'How to design your Dashboard with custom Panels.\\'\\n---\\n\\nThis page shows how to add panels one by one. Check [pre-built Tabs](/docs/platform/dashboard_tabs) for a quick start, and explore [available Panel types](/docs/platform/dashboard_panel_types).\\n\\n## Adding Tabs\\n\\n<Check>\\n  Multiple Tabs are available in the Evidently Cloud and Enterprise.\\n</Check>\\n\\nBy default, new Panels appear on a single Dashboard. You can add multiple Tabs to organize them.\\n\\n**User interface.** Enter the \"Edit\" mode on the Dashboard (top right corner) and click the plus sign with \"add Tab\". To create a cust',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'Reports. Each source Report contains a **single value** (e.g., a \"mean value\"). A Distribution Panel (`DashboardPanelDistribution`) shows how a distribution changes over time. Each source Report contains a **histogram** (e.g. frequency of different categories).\\n</Info>\\n\\n## What\\'s next?\\n\\nHow to add [monitoring Panels and Tabs](/docs/platform/dashboard_add_panels).\\n\\n---\\ntitle: \\'Add dashboard panels\\'\\ndescription: \\'How to design your Dashboard with custom Panels.\\'\\n---\\n\\nThis page shows how to add panels one by one. Check [pre-built Tabs](/docs/platform/dashboard_tabs) for a quick start, and explore [available Panel types](/docs/platform/dashboard_panel_types).\\n\\n## Adding Tabs\\n\\n<Check>\\n  Multiple Tabs are available in the Evidently Cloud and Enterprise.\\n</Check>\\n\\nBy default, new Panels appear on a single Dashboard. You can add multiple Tabs to organize them.\\n\\n**User interface.** Enter the \"Edit\" mode on the Dashboard (top right corner) and click the plus sign with \"add Tab\". To create a custom Tab, choose an “empty” tab and give it a name.\\n\\n**Python**. You can add an empty tab using `create_tab`:\\n\\n```python\\nproject.dashboard.create_tab(\"My tab\")\\nproject.save()\\n```\\n\\nYou can also use the `add_panel` method shown below and specify the destination Tab. If there is no Tab with a set name, you will create both a new Tab and Panel at once. If it already exists, a new Panel will appear below others in this Tab.\\n\\n## Adding Panels\\n\\nYou can add Panels in the user interface or using Python API.\\n\\n### User interface\\n\\n<Check>\\n  No-code Dashboards are available in the Evidently Cloud and Enterprise.\\n</Check>\\n\\nOnce you are inside the Project:\\n\\n* Enter the \"Edit\" mode by clicking on the top right corner of the Dashboard.\\n\\n* Click on the \"Add panel\" button.\\n\\n* Follow the flow to configure dashboard name, type, etc.\\n\\n* Preview and publish.\\n\\nTo delete/edit a Panel, enter Edit mode and hover over a specific Panel to choose an action.\\n\\n### Python API\\n\\n<Check>\\n  Dashboards as code are available ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'om Tab, choose an “empty” tab and give it a name.\\n\\n**Python**. You can add an empty tab using `create_tab`:\\n\\n```python\\nproject.dashboard.create_tab(\"My tab\")\\nproject.save()\\n```\\n\\nYou can also use the `add_panel` method shown below and specify the destination Tab. If there is no Tab with a set name, you will create both a new Tab and Panel at once. If it already exists, a new Panel will appear below others in this Tab.\\n\\n## Adding Panels\\n\\nYou can add Panels in the user interface or using Python API.\\n\\n### User interface\\n\\n<Check>\\n  No-code Dashboards are available in the Evidently Cloud and Enterprise.\\n</Check>\\n\\nOnce you are inside the Project:\\n\\n* Enter the \"Edit\" mode by clicking on the top right corner of the Dashboard.\\n\\n* Click on the \"Add panel\" button.\\n\\n* Follow the flow to configure dashboard name, type, etc.\\n\\n* Preview and publish.\\n\\nTo delete/edit a Panel, enter Edit mode and hover over a specific Panel to choose an action.\\n\\n### Python API\\n\\n<Check>\\n  Dashboards as code are available in Evidently OSS, Cloud, Enterprise.\\n</Check>\\n\\n<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) (or your [local workspace](/docs/setup/self-hosting)) and [create a Project](/docs/platform/projects_manage).\\n</Tip>\\n\\nImport the necessary modules to configure the Panels as code:\\n\\n```python\\nfrom evidently.future.metrics import *\\nfrom evidently.ui.dashboards import DashboardPanelCounter\\nfrom evidently.ui.dashboards import DashboardPanelDistribution\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import TestFilter\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evide',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'in Evidently OSS, Cloud, Enterprise.\\n</Check>\\n\\n<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) (or your [local workspace](/docs/setup/self-hosting)) and [create a Project](/docs/platform/projects_manage).\\n</Tip>\\n\\nImport the necessary modules to configure the Panels as code:\\n\\n```python\\nfrom evidently.future.metrics import *\\nfrom evidently.ui.dashboards import DashboardPanelCounter\\nfrom evidently.ui.dashboards import DashboardPanelDistribution\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import TestFilter\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nHere is the general flow to add a new Panel:\\n\\n<Steps>\\n  <Step title=\"Connect to the Project\">\\n    Load the latest dashboard configuration into your Python environment.\\n\\n    ```python\\n    project = ws.get_project(\"YOUR PROJECT ID HERE\")\\n    ```\\n  </Step>\\n\\n  <Step title=\"Add a new Panel\">\\n    Use the `add_panel` method and configure the Panel:\\n\\n    * Pick the [Panel type](/docs/platform/dashboard_panel_types): Counter, Plot, Distribution, Test Counter, Test Plot.\\n\\n    * Set applicable Panel **parameters.** (See below for each type).\\n\\n    * Specify Panel **title** and **size**.\\n\\n    * Add optional **Tags** to filter data. If empty, the Panel will use data from all Reports.\\n\\n    * Define what the Panel will show (see examples below):\\n\\n      * Use `values` to point a specific Metric result, or\\n\\n      * Use `test_filters` to select Tests.\\n\\n    * Set if the Panel should appear on specific **Tab**.\\n\\n    For example, to add a line plot that sho',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'ntly.renderers.html_widgets import WidgetSize\\n```\\n\\nHere is the general flow to add a new Panel:\\n\\n<Steps>\\n  <Step title=\"Connect to the Project\">\\n    Load the latest dashboard configuration into your Python environment.\\n\\n    ```python\\n    project = ws.get_project(\"YOUR PROJECT ID HERE\")\\n    ```\\n  </Step>\\n\\n  <Step title=\"Add a new Panel\">\\n    Use the `add_panel` method and configure the Panel:\\n\\n    * Pick the [Panel type](/docs/platform/dashboard_panel_types): Counter, Plot, Distribution, Test Counter, Test Plot.\\n\\n    * Set applicable Panel **parameters.** (See below for each type).\\n\\n    * Specify Panel **title** and **size**.\\n\\n    * Add optional **Tags** to filter data. If empty, the Panel will use data from all Reports.\\n\\n    * Define what the Panel will show (see examples below):\\n\\n      * Use `values` to point a specific Metric result, or\\n\\n      * Use `test_filters` to select Tests.\\n\\n    * Set if the Panel should appear on specific **Tab**.\\n\\n    For example, to add a line plot that shows Row Count in time to the \"Overview\" tab:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\"Row count\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": RowCount().metric_id},\\n                        field_path=\"value\",\\n                        legend=\"count\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n\\n    You can add multiple Panels at once: they will appear in the listed order.\\n  </Step>\\n\\n  <Step title=\"Save\">\\n    Save the configuration with `project.save()`. Go back to the web app to see the Dashboard. Refresh the page if needed.\\n  </Step>\\n</Steps>\\n\\n\\n**Delete Panels.** To delete all monitoring Panels, use:\\n\\n```\\nproject.dashboard.panels = []',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'ws Row Count in time to the \"Overview\" tab:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\"Row count\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": RowCount().metric_id},\\n                        field_path=\"value\",\\n                        legend=\"count\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n\\n    You can add multiple Panels at once: they will appear in the listed order.\\n  </Step>\\n\\n  <Step title=\"Save\">\\n    Save the configuration with `project.save()`. Go back to the web app to see the Dashboard. Refresh the page if needed.\\n  </Step>\\n</Steps>\\n\\n\\n**Delete Panels.** To delete all monitoring Panels, use:\\n\\n```\\nproject.dashboard.panels = []\\n\\nproject.save()\\n```\\n\\n<Info>\\n  **Note**: This does not delete the Reports or data; it only deletes the Panel configuration.\\n</Info>\\n\\n## Panel Parameters\\n\\n### General parameters\\n\\nClass `DashboardPanel` is a base class. These parameters apply to all Panel types.\\n\\n| Parameter              | Example use                                              | Description                                                                                                                                                                      |\\n| ---------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `title: str`           | `title=\"My Panel\"`                                       | Panel name visible at the header.                                                                                         ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': '\\n\\nproject.save()\\n```\\n\\n<Info>\\n  **Note**: This does not delete the Reports or data; it only deletes the Panel configuration.\\n</Info>\\n\\n## Panel Parameters\\n\\n### General parameters\\n\\nClass `DashboardPanel` is a base class. These parameters apply to all Panel types.\\n\\n| Parameter              | Example use                                              | Description                                                                                                                                                                      |\\n| ---------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `title: str`           | `title=\"My Panel\"`                                       | Panel name visible at the header.                                                                                                                                                |\\n| `filter: ReportFilter` | `filter=ReportFilter(metadata_values={}, tag_values=[])` | Filters define a subset of Reports from which to display the data. Tags or metadata values you list must be added when logging Reports. See [docs](/docs/library/tags_metadata). |\\n| `size: WidgetSize`     | `size=WidgetSize.HALF`, `size=WidgetSize.FULL` (default) | Sets the Panel size to half-width or full-sized.                                                                                                                                 |\\n\\n### Counter\\n\\n`DashboardPanelCounter` shows a value count or works as a text-only Panel.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_counter_example-min.png\" />\\n</Frame>\\n\\nExamples usage:\\n\\n<Tabs>\\n  <Tab title=\"Text Panel\">\\n    **Text only panel**. To create a Panel with the Dashboard title only:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\"LLM chatbo',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': '                                                       |\\n| `filter: ReportFilter` | `filter=ReportFilter(metadata_values={}, tag_values=[])` | Filters define a subset of Reports from which to display the data. Tags or metadata values you list must be added when logging Reports. See [docs](/docs/library/tags_metadata). |\\n| `size: WidgetSize`     | `size=WidgetSize.HALF`, `size=WidgetSize.FULL` (default) | Sets the Panel size to half-width or full-sized.                                                                                                                                 |\\n\\n### Counter\\n\\n`DashboardPanelCounter` shows a value count or works as a text-only Panel.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_counter_example-min.png\" />\\n</Frame>\\n\\nExamples usage:\\n\\n<Tabs>\\n  <Tab title=\"Text Panel\">\\n    **Text only panel**. To create a Panel with the Dashboard title only:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\"LLM chatbot monitoring\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                agg=CounterAgg.NONE,\\n                size=WidgetSize.FULL,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Value sum\">\\n    **Value sum**. To create a Panel that sums up the number of rows over time:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\"Model calls\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                value=PanelValue(\\n                    metric_args={\"metric.metric_id\": RowCount().metric_id},\\n                    field_path=\"value\",\\n                    legend=\"count\",\\n                ),\\n                text=\"count\",\\n                agg=CounterAgg.SUM,\\n                size=WidgetSize.HALF,\\n            ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Last value\">\\n    **Last value**. To create a Pa',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': 't monitoring\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                agg=CounterAgg.NONE,\\n                size=WidgetSize.FULL,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Value sum\">\\n    **Value sum**. To create a Panel that sums up the number of rows over time:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\"Model calls\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                value=PanelValue(\\n                    metric_args={\"metric.metric_id\": RowCount().metric_id},\\n                    field_path=\"value\",\\n                    legend=\"count\",\\n                ),\\n                text=\"count\",\\n                agg=CounterAgg.SUM,\\n                size=WidgetSize.HALF,\\n            ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Last value\">\\n    **Last value**. To create a Panel that shows the number of rows in the last Report.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\"Row number: last run\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                value=PanelValue(\\n                    metric_args={\"metric.metric_id\": RowCount().metric_id},\\n                    field_path=\"value\",\\n                    legend=\"count\",\\n                ),\\n                text=\"count\",\\n                agg=CounterAgg.LAST,\\n                size=WidgetSize.HALF,\\n            ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\nAll parameters:\\n\\n| Parameter                     | Description                                                                                                                                                                                                                                                   |\\n| ----------------------------- | -----',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': 'nel that shows the number of rows in the last Report.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelCounter(\\n                title=\"Row number: last run\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                value=PanelValue(\\n                    metric_args={\"metric.metric_id\": RowCount().metric_id},\\n                    field_path=\"value\",\\n                    legend=\"count\",\\n                ),\\n                text=\"count\",\\n                agg=CounterAgg.LAST,\\n                size=WidgetSize.HALF,\\n            ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\nAll parameters:\\n\\n| Parameter                     | Description                                                                                                                                                                                                                                                   |\\n| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `value: Optional[PanelValue]` | Specifies the value to display. <br /> <br />You must point to a named **Metric** and a specific **result** inside it (`value` or `share`/`count`). *Check the Panel Value section below for details.* <br /><br />If left empty, displays a text-only panel. |\\n| `text: Optional[str]`         | Supporting text to display.                                                                                                                                                                                                                                   |\\n| `agg: CounterAgg`             | Data aggregation options: <br />`SUM`: Calculates the value sum. <br />`LAST`: Shows the last available value. <br />`NONE`: Reserved f',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `value: Optional[PanelValue]` | Specifies the value to display. <br /> <br />You must point to a named **Metric** and a specific **result** inside it (`value` or `share`/`count`). *Check the Panel Value section below for details.* <br /><br />If left empty, displays a text-only panel. |\\n| `text: Optional[str]`         | Supporting text to display.                                                                                                                                                                                                                                   |\\n| `agg: CounterAgg`             | Data aggregation options: <br />`SUM`: Calculates the value sum. <br />`LAST`: Shows the last available value. <br />`NONE`: Reserved for text panels.                                                                                                        |\\n\\n### Plot\\n\\n`DashboardPanelPlot` shows individual values over time.&#x20;\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Line chart\" img=\"/images/dashboard//panel_line_plot_example.png\">\\n    `PlotType.LINE` shows values over time from multiple Reports.\\n  </Card>\\n\\n  <Card title=\"Bar chart\" img=\"/images/dashboard//panel_bar_plot_example.png\">\\n    `PlotType.BAR` shows values over time from multiple Report.\\n  </Card>\\n</CardGroup>\\n\\nExample usage:\\n\\n<Tabs>\\n  <Tab title=\"Single value\">\\n    **Single value**. To plot row count as a LINE plot (you can change to BAR etc.):\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\"Row count\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": RowCount().metric_id}',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': 'or text panels.                                                                                                        |\\n\\n### Plot\\n\\n`DashboardPanelPlot` shows individual values over time.&#x20;\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Line chart\" img=\"/images/dashboard//panel_line_plot_example.png\">\\n    `PlotType.LINE` shows values over time from multiple Reports.\\n  </Card>\\n\\n  <Card title=\"Bar chart\" img=\"/images/dashboard//panel_bar_plot_example.png\">\\n    `PlotType.BAR` shows values over time from multiple Report.\\n  </Card>\\n</CardGroup>\\n\\nExample usage:\\n\\n<Tabs>\\n  <Tab title=\"Single value\">\\n    **Single value**. To plot row count as a LINE plot (you can change to BAR etc.):\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\"Row count\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": RowCount().metric_id},\\n                        field_path=\"value\",\\n                        legend=\"count\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Multiple values\">\\n    **Multiple values.** To plot min, max and mean values of the \"length\" column on the same plot:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\"Length\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": MinValue(column=\"length\").metric_id},\\n                        field_path=\"value\",\\n                        legend=\"min\",\\n                    ),\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": MeanValue(column=\"length\").metric_id},\\n    ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': ',\\n                        field_path=\"value\",\\n                        legend=\"count\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Multiple values\">\\n    **Multiple values.** To plot min, max and mean values of the \"length\" column on the same plot:\\n\\n    ```python\\n    project.dashboard.add_panel(\\n            DashboardPanelPlot(\\n                title=\"Length\",\\n                filter=ReportFilter(metadata_values={}, tag_values=[]),\\n                values=[\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": MinValue(column=\"length\").metric_id},\\n                        field_path=\"value\",\\n                        legend=\"min\",\\n                    ),\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": MeanValue(column=\"length\").metric_id},\\n                        field_path=\"value\",\\n                        legend=\"mean\",\\n                    ),\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": MaxValue(column=\"length\").metric_id},\\n                        field_path=\"value\",\\n                        legend=\"max\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\nAll parameters:\\n\\n| Parameter                  | Description                                                                                                                                                                                                                                                                                                                                                                                |\\n| -------------------------- | -----------',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': '                    field_path=\"value\",\\n                        legend=\"mean\",\\n                    ),\\n                    PanelValue(\\n                        metric_args={\"metric.metric_id\": MaxValue(column=\"length\").metric_id},\\n                        field_path=\"value\",\\n                        legend=\"max\",\\n                    ),\\n                ],\\n                plot_type=PlotType.LINE,\\n                size=WidgetSize.HALF,\\n            ),\\n            tab=\"Overview\"\\n        )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\nAll parameters:\\n\\n| Parameter                  | Description                                                                                                                                                                                                                                                                                                                                                                                |\\n| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| `values: List[PanelValue]` | Specifies the value(s) to display in the Plot. <br /><br /> You must point to a named **Metric** and a specific **result** inside it (`value` or `share`/`count`). *Refer to the Panel Value section below for details.* <br /><br /> You can pass multiple values so that will appear together, e.g., as separate lines on a Line plot, bars on a Bar Chart, or points on a Scatter Plot. |\\n| `plot_type: PlotType`      | Specifies the plot type. <br /><br />**Available:** `SCATTER`, `BAR`, `LINE`, `HISTOGRAM`                                                                                                  ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `values: List[PanelValue]` | Specifies the value(s) to display in the Plot. <br /><br /> You must point to a named **Metric** and a specific **result** inside it (`value` or `share`/`count`). *Refer to the Panel Value section below for details.* <br /><br /> You can pass multiple values so that will appear together, e.g., as separate lines on a Line plot, bars on a Bar Chart, or points on a Scatter Plot. |\\n| `plot_type: PlotType`      | Specifies the plot type. <br /><br />**Available:** `SCATTER`, `BAR`, `LINE`, `HISTOGRAM`                                                                                                                                                                                                                                                                                                  |\\n\\n### Distribution\\n\\n`DashboardPanelDistribution` shows changes in the distribution over time. It\\'s mostly relevant for showing distributions of categorical columns.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Stacked\" img=\"/images/dashboard/panel_dist_stacked_2-min.png\">\\n    `barmode=\"stack\"`: stacked bar chart shows **absolute** counts in a single bar.\\n  </Card>\\n\\n  <Card title=\"Grouped\" img=\"/images/dashboard/panel_dist_group_2-min.png\">\\n    `barmode=\"group\"`: grouped bar chart shows **absolute** counts in separate bars.\\n  </Card>\\n</CardGroup>\\n\\n**Example**. To plot the distribution of the column \"refusals\" that contains binary labels:\\n\\n```python\\nproject.dashboard.add_panel(\\n        DashboardPanelDistribution(\\n            title=\"Is the context valid? (group)\",\\n            value=PanelValue(\\n             ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': '                                                                                                                                                                                                |\\n\\n### Distribution\\n\\n`DashboardPanelDistribution` shows changes in the distribution over time. It\\'s mostly relevant for showing distributions of categorical columns.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Stacked\" img=\"/images/dashboard/panel_dist_stacked_2-min.png\">\\n    `barmode=\"stack\"`: stacked bar chart shows **absolute** counts in a single bar.\\n  </Card>\\n\\n  <Card title=\"Grouped\" img=\"/images/dashboard/panel_dist_group_2-min.png\">\\n    `barmode=\"group\"`: grouped bar chart shows **absolute** counts in separate bars.\\n  </Card>\\n</CardGroup>\\n\\n**Example**. To plot the distribution of the column \"refusals\" that contains binary labels:\\n\\n```python\\nproject.dashboard.add_panel(\\n        DashboardPanelDistribution(\\n            title=\"Is the context valid? (group)\",\\n            value=PanelValue(\\n                field_path=\"counts\", \\n                metric_args={\"metric.metric_id\": UniqueValueCount(column=\"context quality\").metric_id}\\n                ),\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            barmode=\"group\",\\n            size=WidgetSize.FULL,\\n        ),\\n    tab=\"Overview\"\\n    )\\nproject.save()\\n```\\n\\nAll parameters:\\n\\n| Parameter              | Description                                                                                                                                                                 |\\n| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `value: PanelValue`    | Specifies the distribution to display on the Panel. <br /><br /> You must point to a named **Metric** that contains a distribution histogram and set `field_path=\"counts\"`. |\\n| `barmode: HistBarMode` | Specifies the distr',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': '   field_path=\"counts\", \\n                metric_args={\"metric.metric_id\": UniqueValueCount(column=\"context quality\").metric_id}\\n                ),\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            barmode=\"group\",\\n            size=WidgetSize.FULL,\\n        ),\\n    tab=\"Overview\"\\n    )\\nproject.save()\\n```\\n\\nAll parameters:\\n\\n| Parameter              | Description                                                                                                                                                                 |\\n| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `value: PanelValue`    | Specifies the distribution to display on the Panel. <br /><br /> You must point to a named **Metric** that contains a distribution histogram and set `field_path=\"counts\"`. |\\n| `barmode: HistBarMode` | Specifies the distribution plot type. <br /><br />**Available:** `stack`, `group`, `overlay`, `relative`                                                                    |\\n\\n### Test Counter\\n\\n`DashboardPanelTestSuiteCounter` shows a counter with Test results.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_tests_counter_example.png\" />\\n</Frame>\\n\\nExample usage:\\n\\n<Tabs>\\n  <Tab title=\"All Tests (Last result)\">\\n    **All Tests**. To display the results of the latest Test Suite. Filter by LAST, no filter on Test name.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\"Latest Test suite: results\",\\n            agg=CounterAgg.LAST,\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n\\n    All parameters:\\n  </Tab>\\n\\n  <Tab title=\"Specific Test (All time)\">\\n    **Specific Test**. To show all failures of a specific Test over time. (Filtered by status and Test name).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCount',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 21000,\n",
       "  'chunk': 'ibution plot type. <br /><br />**Available:** `stack`, `group`, `overlay`, `relative`                                                                    |\\n\\n### Test Counter\\n\\n`DashboardPanelTestSuiteCounter` shows a counter with Test results.\\n\\n<Frame>\\n  <img src=\"/images/dashboard/panel_tests_counter_example.png\" />\\n</Frame>\\n\\nExample usage:\\n\\n<Tabs>\\n  <Tab title=\"All Tests (Last result)\">\\n    **All Tests**. To display the results of the latest Test Suite. Filter by LAST, no filter on Test name.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\"Latest Test suite: results\",\\n            agg=CounterAgg.LAST,\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n\\n    All parameters:\\n  </Tab>\\n\\n  <Tab title=\"Specific Test (All time)\">\\n    **Specific Test**. To show all failures of a specific Test over time. (Filtered by status and Test name).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\"Empty Rows Test (Total Failed)\",\\n            test_filters=[\\n                TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\\n                ],\\n            statuses=[TestStatus.FAIL]\\n        ),\\n        tab=\"Tests\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Specific Test (Last result)\">\\n    **Specific Test**. To show the last result of a specific Test. (Defaults to showing success status).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\"Empty Rows Test (Last result)\",\\n            test_filters=[\\n                TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\\n                ],\\n            agg=CounterAgg.LAST\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\n| Parameter                                                                                                                                       ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 22000,\n",
       "  'chunk': 'er(\\n            title=\"Empty Rows Test (Total Failed)\",\\n            test_filters=[\\n                TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\\n                ],\\n            statuses=[TestStatus.FAIL]\\n        ),\\n        tab=\"Tests\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Specific Test (Last result)\">\\n    **Specific Test**. To show the last result of a specific Test. (Defaults to showing success status).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuiteCounter(\\n            title=\"Empty Rows Test (Last result)\",\\n            test_filters=[\\n                TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\\n                ],\\n            agg=CounterAgg.LAST\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\n| Parameter                                                                                                                                                                  | Description                                                                                                                                                                |\\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `test_filters: List[TestFilter]=[]`                                                                                                                                        | Test filters select specific Test(s). Without a filter, the Panel considers the results of all Tests.                                                                      |\\n| `statuses: List[statuses]`<br /><br />**Available**:<br />`TestStatus.ERROR`, `TestStatus.FAIL`, `Te',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 23000,\n",
       "  'chunk': '                           | Description                                                                                                                                                                |\\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `test_filters: List[TestFilter]=[]`                                                                                                                                        | Test filters select specific Test(s). Without a filter, the Panel considers the results of all Tests.                                                                      |\\n| `statuses: List[statuses]`<br /><br />**Available**:<br />`TestStatus.ERROR`, `TestStatus.FAIL`, `TestStatus.SUCCESS`(default), `TestStatus.WARNING`, `TestStatus.SKIPPED` | Status filters select Tests with specific outcomes. (E.g., choose the FAIL status to display a counter for failed Tests). Without a filter, the Panel shows Tests SUCCESS. |\\n| <br />`agg: CounterAgg`<br /><br />**Available**:<br />`SUM`(default),  `LAST`                                                                                             | Data aggregation options:<br />`SUM`: Calculates the sum of Test results. <br />`LAST`: Displays the last available Test result.                                           |\\n\\n### Test Plot\\n\\n`DashboardPanelTestSuite` shows Test results over time.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Detailed plot\" img=\"/images/dashboard/panel_tests_detailed_hover_example.png\">\\n    `TestSuitePanelType.DETAILED`. Individual Test results are visible\\n  </Card>\\n\\n  <Card title=\"Aggregated plot\" img=\"/images/dashboard/panel_tests_aggregated_hover_example.png\">\\n    `TestSuitePanelType.AGGREGATE`. On',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 24000,\n",
       "  'chunk': 'stStatus.SUCCESS`(default), `TestStatus.WARNING`, `TestStatus.SKIPPED` | Status filters select Tests with specific outcomes. (E.g., choose the FAIL status to display a counter for failed Tests). Without a filter, the Panel shows Tests SUCCESS. |\\n| <br />`agg: CounterAgg`<br /><br />**Available**:<br />`SUM`(default),  `LAST`                                                                                             | Data aggregation options:<br />`SUM`: Calculates the sum of Test results. <br />`LAST`: Displays the last available Test result.                                           |\\n\\n### Test Plot\\n\\n`DashboardPanelTestSuite` shows Test results over time.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Detailed plot\" img=\"/images/dashboard/panel_tests_detailed_hover_example.png\">\\n    `TestSuitePanelType.DETAILED`. Individual Test results are visible\\n  </Card>\\n\\n  <Card title=\"Aggregated plot\" img=\"/images/dashboard/panel_tests_aggregated_hover_example.png\">\\n    `TestSuitePanelType.AGGREGATE`. Only the total number of Tests by status is visible.\\n  </Card>\\n</CardGroup>\\n\\nExample usage:\\n\\n<Tabs>\\n  <Tab title=\"Detailed (All Tests)\">\\n    **All Tests**. Show the results of all Tests in the Project with per-Test granularity.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\"All tests: detailed\",\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.FULL,\\n            panel_type=TestSuitePanelType.DETAILED\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Detailed (Selected Tests)\">\\n    **Selected Tests**. Show the results of selected Tests in the Project with per-Test granularity.\\n\\n    ```\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\"Selected tests (missing)\",\\n            test_filters=[\\n                TestFilter(test_args={\"test.metric_fingerprint\": MissingValueCount(column=\"length\").metric_id}),\\n                Te',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 25000,\n",
       "  'chunk': 'ly the total number of Tests by status is visible.\\n  </Card>\\n</CardGroup>\\n\\nExample usage:\\n\\n<Tabs>\\n  <Tab title=\"Detailed (All Tests)\">\\n    **All Tests**. Show the results of all Tests in the Project with per-Test granularity.\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\"All tests: detailed\",\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.FULL,\\n            panel_type=TestSuitePanelType.DETAILED\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Detailed (Selected Tests)\">\\n    **Selected Tests**. Show the results of selected Tests in the Project with per-Test granularity.\\n\\n    ```\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\"Selected tests (missing)\",\\n            test_filters=[\\n                TestFilter(test_args={\"test.metric_fingerprint\": MissingValueCount(column=\"length\").metric_id}),\\n                TestFilter(test_args={\"test.metric_fingerprint\": MissingValueCount(column=\"sentiment\").metric_id})\\n            ],\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.HALF,\\n            panel_type=TestSuitePanelType.DETAILED,\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Aggregated (All Tests)\">\\n    **All Tests.** Show the results of all Tests in aggregate (Total passed/fail).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\"All tests: detailed\",\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.FULL,\\n            panel_type=TestSuitePanelType.AGGREGATE\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\nAll parameters:\\n\\n| Parameter                                                                                                                                                  ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 26000,\n",
       "  'chunk': 'stFilter(test_args={\"test.metric_fingerprint\": MissingValueCount(column=\"sentiment\").metric_id})\\n            ],\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.HALF,\\n            panel_type=TestSuitePanelType.DETAILED,\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"Aggregated (All Tests)\">\\n    **All Tests.** Show the results of all Tests in aggregate (Total passed/fail).\\n\\n    ```python\\n    project.dashboard.add_panel(\\n        DashboardPanelTestSuite(\\n            title=\"All tests: detailed\",\\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\\n            size=WidgetSize.FULL,\\n            panel_type=TestSuitePanelType.AGGREGATE\\n        ),\\n        tab=\"Overview\"\\n    )\\n    project.save()\\n    ```\\n  </Tab>\\n</Tabs>\\n\\nAll parameters:\\n\\n| Parameter                                                                                                                                                                                          | Description                                                                                                                                                                                                                        |\\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `test_filters: List[TestFilter]`                                                                                                                                                                   | Test filters select specific Test(s). Without a filter, the Panel shows the results of all Tests.     ',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 27000,\n",
       "  'chunk': '                                        | Description                                                                                                                                                                                                                        |\\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `test_filters: List[TestFilter]`                                                                                                                                                                   | Test filters select specific Test(s). Without a filter, the Panel shows the results of all Tests.                                                                                                                                  |\\n| `statuses: List[statuses]`<br /><br />**Available**:<br />`TestStatus.ERROR`, `TestStatus.FAIL`, `TestStatus.SUCCESS`, `TestStatus.WARNING`, `TestStatus.SKIPPED`                                  | Status filters select Tests with specific outcomes. By default the Panel shows all Test statuses.                                                                                                                                  |\\n| `panel_type=TestSuitePanelType`<br /><br />**Available**:<br />`TestSuitePanelType.DETAILED`<br />`TestSuitePanelType.AGGREGATE`                                                                   | Defines the Panel type. **Detailed** shows individual Test results where you can hover and see individual results and click to open a corresponding Test Suite. **Aggregate** (default) shows the total number of Tests by status. |\\n| `time_agg: Opti',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 28000,\n",
       "  'chunk': '                                                                                                                             |\\n| `statuses: List[statuses]`<br /><br />**Available**:<br />`TestStatus.ERROR`, `TestStatus.FAIL`, `TestStatus.SUCCESS`, `TestStatus.WARNING`, `TestStatus.SKIPPED`                                  | Status filters select Tests with specific outcomes. By default the Panel shows all Test statuses.                                                                                                                                  |\\n| `panel_type=TestSuitePanelType`<br /><br />**Available**:<br />`TestSuitePanelType.DETAILED`<br />`TestSuitePanelType.AGGREGATE`                                                                   | Defines the Panel type. **Detailed** shows individual Test results where you can hover and see individual results and click to open a corresponding Test Suite. **Aggregate** (default) shows the total number of Tests by status. |\\n| `time_agg: Optional[str] = None`<br /><br />**Available**:<br />`1H`, `1D`, `1W`, `1M` (see [period aliases](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-period-aliases)) | Groups all Test results in a period (e.g., 1 DAY).                                                                                                                                                                                 |\\n\\n## Panel Value\\n\\n**Metric ID**. To point to the Metric or Test to plot on a Panel, you use  `test_filters` or `metric_args `as shown above and pass `metric_id` or `metric_fingerprint` . They must include the name of the Metric that was logged to the Project. You must use the same Metic name (with any applicable parameters) that you used when creating the Report.\\n\\n<Note>\\n  **Working with Presets.** You must reference a named Evidently Metric even if you used a Preset. You can check the Metrics included in each Preset [here](/metrics/all_metrics).\\n</Note>\\n\\n**Field path**. For Metric P',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 29000,\n",
       "  'chunk': 'onal[str] = None`<br /><br />**Available**:<br />`1H`, `1D`, `1W`, `1M` (see [period aliases](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-period-aliases)) | Groups all Test results in a period (e.g., 1 DAY).                                                                                                                                                                                 |\\n\\n## Panel Value\\n\\n**Metric ID**. To point to the Metric or Test to plot on a Panel, you use  `test_filters` or `metric_args `as shown above and pass `metric_id` or `metric_fingerprint` . They must include the name of the Metric that was logged to the Project. You must use the same Metic name (with any applicable parameters) that you used when creating the Report.\\n\\n<Note>\\n  **Working with Presets.** You must reference a named Evidently Metric even if you used a Preset. You can check the Metrics included in each Preset [here](/metrics/all_metrics).\\n</Note>\\n\\n**Field path**. For Metric Panels, you also specify the `field_path`. This helps point to a specific **result** inside the Metric. This can take the following values: `value` , `share`/`count` or `values` .\\n\\n| Field path         | Description                                          | Applicable Metrics                                                               | Applicable Panels |\\n| ------------------ | ---------------------------------------------------- | -------------------------------------------------------------------------------- | ----------------- |\\n| `value`            | Points to a single result from the Metric.           | Most Metrics                                                                     | Counter, Plot     |\\n| `share` or `count` | Points to either absolute count or percentage value. | Metrics that return both absolute and percentage values like `MissingValueCount` | Counter, Plot     |\\n| `shares` or `counts`        | Points to a histogram visualization within a Metric. | Metrics w',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 30000,\n",
       "  'chunk': 'anels, you also specify the `field_path`. This helps point to a specific **result** inside the Metric. This can take the following values: `value` , `share`/`count` or `values` .\\n\\n| Field path         | Description                                          | Applicable Metrics                                                               | Applicable Panels |\\n| ------------------ | ---------------------------------------------------- | -------------------------------------------------------------------------------- | ----------------- |\\n| `value`            | Points to a single result from the Metric.           | Most Metrics                                                                     | Counter, Plot     |\\n| `share` or `count` | Points to either absolute count or percentage value. | Metrics that return both absolute and percentage values like `MissingValueCount` | Counter, Plot     |\\n| `shares` or `counts`        | Points to a histogram visualization within a Metric. | Metrics with histogram visualizations,  like `UniqueValueCount`.                 | Distribution      |\\n\\nThere are a few exceptions where a Metric can return a different result or a dictionary.\\n\\n<Note>\\n  **How to verify the result of a specific Metric?** Check in the [All Metrics table](/metrics/all_metrics). You can also generate the Report with a given Metric, export the Report as JSON and check the value name it returns.\\n</Note>\\n\\nWhen working in the Evidently Cloud, you can see available fields in the drop-down menu as you add a new Panel.',\n",
       "  'title': 'Dashboard panel types [Legacy]',\n",
       "  'description': 'Overview of the available monitoring Panels.',\n",
       "  'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Check>\\n  Synthetic data generation is an add-on available on some Evidently Cloud and Enterprise plans. Check details on the [pricing](https://www.evidentlyai.com/pricing) page. [Request a demo](https://www.evidentlyai.com/get-demo) or contact sales@evidentlyai.com for extended trial access. You can also apply for a [startup discount](https://www.evidentlyai.com/sign-up-startups).\\n</Check>\\n\\nTo use synthetic data feature:\\n\\n* Create a Project\\n\\n* Set up an API key for Open AI\\n\\n* Open \"Datasets\" and choose \"Generate Dataset.\"\\n\\nYou can use synthetic data to augment your test scenarios as you evaluate the performance of your AI system.\\n\\n<Note>\\n**Synthetic data docs**. Explore this functionality in the separate [docs section](/synthetic-data/introduction).\\n</Note>\\n  \\nCheck the video with the basic flow from our **LLM evaluation course:**\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gKp9K7Etv4A\" title=\"No-code data generation\" frameborder=\"0\" allowfullscreen />',\n",
       "  'title': 'Synthetic data',\n",
       "  'description': 'Generating synthetic data.',\n",
       "  'filename': 'docs-main/docs/platform/datasets_generate.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Check>\\n  Datasets are available in **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\n## What is a Dataset?\\n\\n**Datasets** are collections of data from your application used for analysis and automated checks. You can bring in existing datasets, capture live data, or create synthetic datasets.\\n\\n![](/images/dataset_llm.png)\\n\\n## How to create a Dataset?\\n\\nYou can add Datasets to the platform in multiple ways:\\n\\n* **Upload directly**. Use the UI to upload CSV files or push datasets via the Python API.\\n\\n* **Upload with Reports**. Attach datasets to Reports when running local evaluations. This is optional — you can also upload only summary metrics.\\n\\n* **Generate synthetic data**. Use built-in platform features to generate synthetic evaluation datasets.\\n\\n* **Create from Traces**. During tracing, Evidently automatically generates tabular datasets that can be used for evaluations.\\n\\n<Tip>\\n  **Where do I find the data?** To view all datasets (uploaded, synthetic, or evaluation results), go to the \"Dataset\" page in your Project menu. For raw tracing datasets, check the Tracing section.\\n</Tip>\\n\\n## Synthetic Data\\n\\nYou can synthesize evaluation datasets directly in Evidently Platform:\\n\\n* **Generate from examples or description**. Describe specific test scenarios and generate matching datasets.\\n\\n* **Generate from source documents**. Generate Q\\\\&A pairs from source documents like PDF, CSV or markdown files (great for RAG evaluations).\\n\\nAfter creating or uploading datasets, you can edit or diversify them further using the \"more like this\" feature.\\n\\n## When do you need Datasets?\\n\\nHere are common use cases for datasets in Evidently:\\n\\n* **Organize evaluation datasets**. Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in UI, and access them programmatically for CI/CD checks.\\n\\n* **Debug evaluation results**. After you run an evaluation, view the dataset to identify and debug specific fail',\n",
       "  'title': 'Overview',\n",
       "  'description': 'Introduction to Datasets.',\n",
       "  'filename': 'docs-main/docs/platform/datasets_overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' to the \"Dataset\" page in your Project menu. For raw tracing datasets, check the Tracing section.\\n</Tip>\\n\\n## Synthetic Data\\n\\nYou can synthesize evaluation datasets directly in Evidently Platform:\\n\\n* **Generate from examples or description**. Describe specific test scenarios and generate matching datasets.\\n\\n* **Generate from source documents**. Generate Q\\\\&A pairs from source documents like PDF, CSV or markdown files (great for RAG evaluations).\\n\\nAfter creating or uploading datasets, you can edit or diversify them further using the \"more like this\" feature.\\n\\n## When do you need Datasets?\\n\\nHere are common use cases for datasets in Evidently:\\n\\n* **Organize evaluation datasets**. Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in UI, and access them programmatically for CI/CD checks.\\n\\n* **Debug evaluation results**. After you run an evaluation, view the dataset to identify and debug specific failures. E.g. you can sort all text outputs by added scores.\\n\\n* **Store ML inference logs or LLM traces**. Collect raw data from production or experimental runs, use it as a source of truth, and run evaluations over it.',\n",
       "  'title': 'Overview',\n",
       "  'description': 'Introduction to Datasets.',\n",
       "  'filename': 'docs-main/docs/platform/datasets_overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).\\n</Tip>\\n\\n## Upload a Dataset\\n\\n<Tabs>\\n  <Tab title=\"Python\">\\n    Prepare your dataset as an Evidently Dataset with the corresponding data definition. To upload a Dataset to the specified Project in workspace `ws`, use the `add_dataset` method:\\n\\n    ```python\\n    eval_data = Dataset.from_pandas(\\n        source_df,\\n        data_definition=DataDefinition()\\n    )\\n    ws.add_dataset(\\n        dataset = eval_data, \\n        name = \"dataset_name\",\\n        project_id = project.id, \\n        description = \"Optional description\")\\n    ```\\n\\n    You must always specify the dataset `name` that you will see in the UI. The description is optional.\\n  </Tab>\\n  <Tab title=\"UI\">\\n    To upload any existing dataset as a CSV file, click on \"Add dataset\". When you upload the Dataset, you must also add a [**data definition**](/docs/library/data_definition). This lets Evidently understand the role of specific columns and prepare your Dataset for future evaluations.\\n  </Tab>\\n</Tabs>\\n\\n<Note>\\n  **How to create an Evidently Dataset?** Read the [Data Definition docs](../library/data-definition).\\n</Note>\\n\\n## Download the Dataset\\n\\nYou can pull the Dataset stored or generated on the platform to your local environment. For example, call the evaluation or tracing dataset to use in a CI/CD testing script.\\n\\nUse the `load_dataset` method:\\n\\n```python\\neval_dataset = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\") \\n\\n#to create as pandas dataframe\\ndf = eval_dataset.as_dataframe()\\n```\\n\\n## Include the Dataset\\n\\nYou can include Datasets when you upload Reports to the platform. This way, after running an evaluation locally you simultaneously upload:\\n\\n- the Report with evaluation result,\\n- the Dataset it was generated for, with new added scores if applicable.\\n\\nBy default, you upload only the Report.\\n\\nTo include the Dataset, use the `include_data` parameter:\\n\\n```python\\nws.add_run(project.id, dat',\n",
       "  'title': 'Work with datasets',\n",
       "  'description': 'How to create, upload and manage Datasets.',\n",
       "  'filename': 'docs-main/docs/platform/datasets_workflow.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'e role of specific columns and prepare your Dataset for future evaluations.\\n  </Tab>\\n</Tabs>\\n\\n<Note>\\n  **How to create an Evidently Dataset?** Read the [Data Definition docs](../library/data-definition).\\n</Note>\\n\\n## Download the Dataset\\n\\nYou can pull the Dataset stored or generated on the platform to your local environment. For example, call the evaluation or tracing dataset to use in a CI/CD testing script.\\n\\nUse the `load_dataset` method:\\n\\n```python\\neval_dataset = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\") \\n\\n#to create as pandas dataframe\\ndf = eval_dataset.as_dataframe()\\n```\\n\\n## Include the Dataset\\n\\nYou can include Datasets when you upload Reports to the platform. This way, after running an evaluation locally you simultaneously upload:\\n\\n- the Report with evaluation result,\\n- the Dataset it was generated for, with new added scores if applicable.\\n\\nBy default, you upload only the Report.\\n\\nTo include the Dataset, use the `include_data` parameter:\\n\\n```python\\nws.add_run(project.id, data_report, include_data=True)\\n```\\n\\nCheck the docs on [running evals via API](/docs/platform/evals_api) for details.',\n",
       "  'title': 'Work with datasets',\n",
       "  'description': 'How to create, upload and manage Datasets.',\n",
       "  'filename': 'docs-main/docs/platform/datasets_workflow.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '![](/images/evals_flow_python.png)\\n\\nThis relies on the core evaluation API of the Evidently Python library. Check the [detailed guide](/docs/library/evaluations_overview).\\n\\n## Simple Example\\n\\n<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).\\n</Tip>\\n\\nTo run a single eval with text evaluation results uploaded to a workspace:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n## Workflow\\n\\nThe complete workflow looks as the following.\\n\\n<Steps>\\n  <Step title=\"Run a Report\">\\n    Configure the evals and run the [Evidently Report](/docs/library/report) with optional [Test ](/docs/library/tests)conditions.\\n  </Step>\\n\\n  <Step title=\"Upload to the platform\">\\n    Upload the raw data or only the evaluation results.\\n  </Step>\\n\\n  <Step title=\"Explore the results\">\\n    Go to the Explore view inside your Project to debug the results and compare the outcomes between runs. Understand the [Explore view](/docs/platform/evals_explore).\\n  </Step>\\n\\n  <Step title=\"(Optional) Set up a Dashboard\">\\n    Set a Dashboard to track results over time. This helps you monitor metric changes across experiments or results of ongoing safety Tests. Check the docs on [Dashboard](/docs/platform/dashboard_overview).\\n  </Step>\\n\\n  <Step title=\"(Optional) Configure alerts\">\\n    Optionally, configure alerts on failed Tests. Check the section on [Alerts](/docs/platform/alerts).\\n  </Step>\\n</Steps>\\n\\n## Uploading data\\n\\n<Check>\\n  Raw data upload is available only for Evidently Cloud and Enterprise.\\n</Check>\\n\\nWhen you upload a Report, you can decide to:\\n\\n* include only the resulting Metrics and a summary Report (with distribution summaries, etc.), or\\n\\n* also upload the raw Dataset you evaluated, together with added Descriptors if any. This helps with row-level debug',\n",
       "  'title': 'Run evals via API',\n",
       "  'description': 'How to run evals and log them on the platform',\n",
       "  'filename': 'docs-main/docs/platform/evals_api.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 're the results\">\\n    Go to the Explore view inside your Project to debug the results and compare the outcomes between runs. Understand the [Explore view](/docs/platform/evals_explore).\\n  </Step>\\n\\n  <Step title=\"(Optional) Set up a Dashboard\">\\n    Set a Dashboard to track results over time. This helps you monitor metric changes across experiments or results of ongoing safety Tests. Check the docs on [Dashboard](/docs/platform/dashboard_overview).\\n  </Step>\\n\\n  <Step title=\"(Optional) Configure alerts\">\\n    Optionally, configure alerts on failed Tests. Check the section on [Alerts](/docs/platform/alerts).\\n  </Step>\\n</Steps>\\n\\n## Uploading data\\n\\n<Check>\\n  Raw data upload is available only for Evidently Cloud and Enterprise.\\n</Check>\\n\\nWhen you upload a Report, you can decide to:\\n\\n* include only the resulting Metrics and a summary Report (with distribution summaries, etc.), or\\n\\n* also upload the raw Dataset you evaluated, together with added Descriptors if any. This helps with row-level debugging and analysis.\\n\\nUse`include_data` (default `False`) to specify whether to include the data.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```',\n",
       "  'title': 'Run evals via API',\n",
       "  'description': 'How to run evals and log them on the platform',\n",
       "  'filename': 'docs-main/docs/platform/evals_api.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'The result of each evaluation is a Report (summary of metrics with visuals) with an optional Test Suite (when it also includes pass/fail results on set conditions).\\n\\n**Browse the results**. To access the results of your evaluations, enter your Project and navigate to the \"Reports\" section in the left menu. Here, you can view all your evaluation artifacts and browse them by Tags, time, or metadata. You can also download them as HTML or JSON.\\n\\n![](/images/evals_browse_reports-min.png)\\n\\nTo see and compare the evaluation results, click on \"Explore\" next to the individual Report.\\n\\n**Explore view**. You\\'ll get the Report or Test Suite and, if available, the dataset linked to the evaluation.\\n\\n![](/images/evals_explore_view-min.png)\\n\\n- To view the Report only, click on the \"Dataset\" sign at the top to hide the dataset.\\n- To explore the Dataset only, choose \"Go to dataset\".\\n\\n**Compare**. To analyze multiple evaluation results side by side, simply select them from the Report list (e.g., reports generated using different LLMs) and click the **\"Compare\"** button. This allows you to quickly identify differences in performance, quality, or behavior across model versions or configurations.\\n\\n![](/images/platform_compare_select.png)\\n\\nYou will see the Compare view, where you can explore different metric scores (or pass/fail test results) side by side.\\n\\n![](/images/platform_compare_view.png)\\n\\nAlternatively, when you are viewing a specific Report, you can click on \"duplicate snapshot\" (this will keep the current Metric in view), and then select a different Report for comparison.\\n\\n**Track progress over time**. As you run multiple evaluations, you can build a Dashboard to track progress, see performance improvements, and monitor how tests perform over time. This will let you visualize results over time from multiple Reports within a Project. [Read more](/docs/platform/dashboard_overview).',\n",
       "  'title': 'Explore view',\n",
       "  'description': 'Reviewing the evaluation results on the Platform.',\n",
       "  'filename': 'docs-main/docs/platform/evals_explore.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'You can run text evaluations using descriptors directly in the user interface.\\n\\n## 1. Prepare the Dataset\\n\\nBefore you start, create a Project and prepare the Dataset to evaluate. There are two options:\\n\\n- **Upload a CSV**. Enter the \"Dataset\" menu, click on \"Create new dataset from CSV\". Drag and drop your Dataset. You must also specify the data definition when you upload it.\\n- **Use an existing Dataset**. Select a Dataset you previously uploaded to the platform or one collected through [Tracing](tracing_overview).\\n\\n<Note>\\n  **What are Datasets?** Learn how to manage and upload [Datasets](datasets_overview) to the platform.\\n</Note>\\n\\n<Note>\\n  **What is Data Definition?** Understand how to set your dataset schema in the [Data Definition](../library/data-definition).\\n</Note>\\n\\n## 2. Start an evaluation\\n\\nWhile you are viewing the Dataset, you can click on \"Add descriptors\" on the right.\\n\\n![](/images/evals_no_code_add_descriptors-min.png)\\n\\n**(Optional) Add the LLM provider API key.** Add a token in the “Secrets” menu section if you plan to use an LLM for evaluations. You can proceed without it, using other types of evals.\\n\\n## 3. Configure the evaluation\\n\\nYou must choose which column to evaluate and how. You can choose from the following methods:\\n\\n- **Model-based**: use built-in machine learning models, like sentiment analysis.\\n- **Regular expressions**: check for specific words or patterns.\\n- **Text stats**: measure stats like the number of symbols or sentences.\\n- **LLM-based**: use external LLMs to evaluate your text data.\\n\\nSelect specific checks one by one:\\n\\n![](/images/nocode_choose_evals-min.png)\\n\\nEach evaluation result is called a **Descriptor**. No matter the method, you’ll get a label or score for every evaluated text. Some, like “Sentiment,” work instantly, while others may need setup.\\n\\n<Note>\\n  **What other evaluators are there?** Check the list of [All Descriptors](../metrics/all_descriptors).\\n</Note>\\n\\nHere are few examples of Descriptors and how to configure th',\n",
       "  'title': 'No code evals',\n",
       "  'description': 'How to evaluate your data in a no-code interface.',\n",
       "  'filename': 'docs-main/docs/platform/evals_no_code.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'oken in the “Secrets” menu section if you plan to use an LLM for evaluations. You can proceed without it, using other types of evals.\\n\\n## 3. Configure the evaluation\\n\\nYou must choose which column to evaluate and how. You can choose from the following methods:\\n\\n- **Model-based**: use built-in machine learning models, like sentiment analysis.\\n- **Regular expressions**: check for specific words or patterns.\\n- **Text stats**: measure stats like the number of symbols or sentences.\\n- **LLM-based**: use external LLMs to evaluate your text data.\\n\\nSelect specific checks one by one:\\n\\n![](/images/nocode_choose_evals-min.png)\\n\\nEach evaluation result is called a **Descriptor**. No matter the method, you’ll get a label or score for every evaluated text. Some, like “Sentiment,” work instantly, while others may need setup.\\n\\n<Note>\\n  **What other evaluators are there?** Check the list of [All Descriptors](../metrics/all_descriptors).\\n</Note>\\n\\nHere are few examples of Descriptors and how to configure them:\\n\\n### Words presence\\n\\n**Include Words**. This Descriptor checks for listed words and returns \"True\" or \"False.\"\\n\\nSet up these parameters:\\n\\n- Add a list of words.\\n- Choose whether to check for “any” or “all” of the words present.\\n- Set the **lemmatize** parameter to check for inflected and variant words automatically.\\n- Give your check a name so you can easily find it in your results.\\n\\nExample setup:\\n![](/images/nocode_includes_words-min.png)\\n\\n### Semantic Similarity\\n\\n**Semantic Similarity**. This descriptor converts texts to embeddings and calculates Cosine Similarity between your evaluated column and another column. It scores from 0 to 1 (0: completely different, 0.5: unrelated, 1: identical). It\\'s useful for checking if responses are semantically similar to a question or reference.\\n\\nSelect the column to compare against: ![](/images/nocode_semantic_similarity-min.png)\\n\\n### LLM as a judge\\n\\n**Custom LLM evaluator**. If you\\'ve added your token, use LLM-based evals (built-in or custom)',\n",
       "  'title': 'No code evals',\n",
       "  'description': 'How to evaluate your data in a no-code interface.',\n",
       "  'filename': 'docs-main/docs/platform/evals_no_code.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'em:\\n\\n### Words presence\\n\\n**Include Words**. This Descriptor checks for listed words and returns \"True\" or \"False.\"\\n\\nSet up these parameters:\\n\\n- Add a list of words.\\n- Choose whether to check for “any” or “all” of the words present.\\n- Set the **lemmatize** parameter to check for inflected and variant words automatically.\\n- Give your check a name so you can easily find it in your results.\\n\\nExample setup:\\n![](/images/nocode_includes_words-min.png)\\n\\n### Semantic Similarity\\n\\n**Semantic Similarity**. This descriptor converts texts to embeddings and calculates Cosine Similarity between your evaluated column and another column. It scores from 0 to 1 (0: completely different, 0.5: unrelated, 1: identical). It\\'s useful for checking if responses are semantically similar to a question or reference.\\n\\nSelect the column to compare against: ![](/images/nocode_semantic_similarity-min.png)\\n\\n### LLM as a judge\\n\\n**Custom LLM evaluator**. If you\\'ve added your token, use LLM-based evals (built-in or custom) to send your texts to LLMs for grading or scoring. You can choose a specific LLM model from the provider.\\n\\nFor example, you can create a custom evaluator to classify texts as “cheerful” or “neutral.” Fill in the parameters, and Evidently will generate the evaluation prompt:\\n\\n![](/images/nocode_llm_judge-min.png)\\n\\nFor a binary classification template, you can configure:\\n\\n- **Criteria**: define custom criteria in free text to clarify the classification task.\\n- **Target/Non-target Category**: labels you want to use.\\n- **Uncertain Category**: how the model should respond when it can’t decide.\\n- **Reasoning**: choose to include explanation (Recommended).\\n- **Category** and/or **Score**: have the LLM respond with the category (Recommended) or score.\\n- **Visualize as**: when both Category and Score are computed, choose which to display in the Report.\\n\\nTo add evaluations for another column in the same Report, click “Add Preset,” select “Text Evals,” and follow the same steps for the new colum',\n",
       "  'title': 'No code evals',\n",
       "  'description': 'How to evaluate your data in a no-code interface.',\n",
       "  'filename': 'docs-main/docs/platform/evals_no_code.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' to send your texts to LLMs for grading or scoring. You can choose a specific LLM model from the provider.\\n\\nFor example, you can create a custom evaluator to classify texts as “cheerful” or “neutral.” Fill in the parameters, and Evidently will generate the evaluation prompt:\\n\\n![](/images/nocode_llm_judge-min.png)\\n\\nFor a binary classification template, you can configure:\\n\\n- **Criteria**: define custom criteria in free text to clarify the classification task.\\n- **Target/Non-target Category**: labels you want to use.\\n- **Uncertain Category**: how the model should respond when it can’t decide.\\n- **Reasoning**: choose to include explanation (Recommended).\\n- **Category** and/or **Score**: have the LLM respond with the category (Recommended) or score.\\n- **Visualize as**: when both Category and Score are computed, choose which to display in the Report.\\n\\nTo add evaluations for another column in the same Report, click “Add Preset,” select “Text Evals,” and follow the same steps for the new column. You can include evals for multiple columns at once.\\n\\n## 4. Run the evaluation\\n\\nClick “Run calculation”, and the calculation will start\\\\! It may take some time to process, especially on a large dataset. You can check the status of the evaluation in the “Tasks“ (use the left menu to navigate).\\n\\nOnce your evaluation is complete, you can view the new dataset with the results.',\n",
       "  'title': 'No code evals',\n",
       "  'description': 'How to evaluate your data in a no-code interface.',\n",
       "  'filename': 'docs-main/docs/platform/evals_no_code.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"You may need evaluations at different stages of your AI product development:\\n\\n* **Ad hoc analysis.** Spot-check the quality of your data or AI outputs.\\n\\n* **Experiments**. Test different parameters, models, or prompts and compare outcomes.\\n\\n* **Safety and adversarial testing.** Evaluate how your system handles edge cases and adversarial inputs, including on synthetic data.\\n\\n* **Regression testing.** Ensure the performance does not degrade after updates or fixes.\\n\\n* **Monitoring**. Track the response quality for production systems.\\n\\nEvidently supports all these workflows. You can run evals locally or directly on the platform.\\n\\n## Evaluations via API\\n\\n<Check>\\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\nThis is perfect for experiments, CI/CD workflows, or custom evaluation pipelines.\\n\\n![](/images/evals_flow_python.png)\\n\\n**How it works**:\\n\\n* Run Python-based evaluations on your AI outputs by generating Reports.\\n\\n* Upload results to the Evidently Platform.\\n\\n* Use the Explore feature to compare and debug results between runs.\\n\\n**Next step:** check the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n\\n## No-code evaluations\\n\\n<Check>\\n  Supported in `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\nThis option lets you run evaluations directly in the user interface. This is great for non-technical users or when you prefer to run evaluations on Evidently infrastructure.\\n\\n![](/images/evals_flow_nocode.png)\\n\\n**How it works**:\\n\\n* **Analyze CSV datasets**. Drag and drop CSV files and evaluate their contents on the Platform.\\n\\n* **Evaluate uploaded datasets**. Assess collected [traces](/docs/platform/tracing_overview) from instrumented LLM applications or any [Datasets](/docs/platform/datasets_overview) you previously uploaded or generated.\\n\\nNo-code workflows create the same Reports or Test Suites you'd generate using Python. The rest of the workflow is the same. After you run your evals with any method, you can acces\",\n",
       "  'title': 'Overview',\n",
       "  'description': 'Running evals on the platform.',\n",
       "  'filename': 'docs-main/docs/platform/evals_overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \" Platform.\\n\\n* Use the Explore feature to compare and debug results between runs.\\n\\n**Next step:** check the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n\\n## No-code evaluations\\n\\n<Check>\\n  Supported in `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\nThis option lets you run evaluations directly in the user interface. This is great for non-technical users or when you prefer to run evaluations on Evidently infrastructure.\\n\\n![](/images/evals_flow_nocode.png)\\n\\n**How it works**:\\n\\n* **Analyze CSV datasets**. Drag and drop CSV files and evaluate their contents on the Platform.\\n\\n* **Evaluate uploaded datasets**. Assess collected [traces](/docs/platform/tracing_overview) from instrumented LLM applications or any [Datasets](/docs/platform/datasets_overview) you previously uploaded or generated.\\n\\nNo-code workflows create the same Reports or Test Suites you'd generate using Python. The rest of the workflow is the same. After you run your evals with any method, you can access the results in the Explore view for further analysis.\\n\\n**Next step:** check the Guide for [No-code evals](/docs/platform/evals_no_code).\",\n",
       "  'title': 'Overview',\n",
       "  'description': 'Running evals on the platform.',\n",
       "  'filename': 'docs-main/docs/platform/evals_overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Read the overview of the approach [here](/docs/platform/monitoring_overview).\\n\\n![](/images/monitoring_flow_batch.png)\\n\\nBatch monitoring relies on the core evaluation API of the Evidently Python library. Check the [detailed guide](/docs/library/evaluations_overview).\\n\\n## Simple Example\\n\\n<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).\\n</Tip>\\n\\nTo get the dataset stats for a single batch and upload to the workspace:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DatasetStats()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n## Workflow\\n\\nThe complete workflow looks as the following.\\n\\n<Steps>\\n  <Step title=\"Configure the metrics\">\\n    Define an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to define the evals.\\n  </Step>\\n\\n  <Step title=\"Run the evals\">\\n    You must independently execute Reports on a chosen cadence. Consider tools like Airflow. You can send Reports from different steps in your pipeline. For example:\\n\\n    * first, send data quality, data drift and prediction drift checks\\n\\n    * after you get the delayed labels, send a ML quality checks results.\\n\\n    You can backdate your Reports with a custom timestamp.\\n    ![](/images/monitoring_batch_workflow_min.png)\\n  </Step>\\n\\n  <Step title=\"Upload to the platform\">\\n    Choose to store raw inferences or only upload the metric summaries. [How to upload / delete results](/docs/platform/evals_api).\\n  </Step>\\n\\n  <Step title=\"Configure the Dashboard\">\\n    Set up a Dashboard to track results over time: using pre-built Tabs or configure your own choice of monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\\n  </Step>\\n\\n  <Step title=\"Configure alerts\">\\n    Set up alerts on Metric values or Test failures. Check the section on [Alerts](/doc',\n",
       "  'title': 'Batch monitoring',\n",
       "  'description': 'How to run batch evaluation jobs.',\n",
       "  'filename': 'docs-main/docs/platform/monitoring_local_batch.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'itle=\"Run the evals\">\\n    You must independently execute Reports on a chosen cadence. Consider tools like Airflow. You can send Reports from different steps in your pipeline. For example:\\n\\n    * first, send data quality, data drift and prediction drift checks\\n\\n    * after you get the delayed labels, send a ML quality checks results.\\n\\n    You can backdate your Reports with a custom timestamp.\\n    ![](/images/monitoring_batch_workflow_min.png)\\n  </Step>\\n\\n  <Step title=\"Upload to the platform\">\\n    Choose to store raw inferences or only upload the metric summaries. [How to upload / delete results](/docs/platform/evals_api).\\n  </Step>\\n\\n  <Step title=\"Configure the Dashboard\">\\n    Set up a Dashboard to track results over time: using pre-built Tabs or configure your own choice of monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\\n  </Step>\\n\\n  <Step title=\"Configure alerts\">\\n    Set up alerts on Metric values or Test failures. Check the section on [Alerts](/docs/platform/alerts).\\n  </Step>\\n</Steps>\\n\\n<Tip>\\n  **Running Tests vs Reports**. Structuring your evaluations as Tests - as opposed to monitoring lots of metrics at once - can help reduce alert fatigue and simplify configuration when evaluating multiple conditions at once. For example, you can quickly verify that all columns in the input data are within a defined min-max range.\\n</Tip>',\n",
       "  'title': 'Batch monitoring',\n",
       "  'description': 'How to run batch evaluation jobs.',\n",
       "  'filename': 'docs-main/docs/platform/monitoring_local_batch.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'AI observability lets you evaluate the quality of the inputs and outputs of your AI application as it runs in production. This gives an up-to-date view of your system behavior and helps spot and fix issues.\\n\\nEvidently offers several ways to set up monitoring.\\n\\n## Batch monitoring jobs\\n\\n<Check>\\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don’t need instant quality evaluations.\\n\\n![](/images/monitoring_flow_batch.png)\\n\\n**How it works**:\\n\\n* **Build your evaluation pipeline**. Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow. Run it at regular intervals (e.g., hourly, daily) or trigger it when new data or labels arrive.\\n\\n* **Run metric calculations**. Implement the evaluation step in the pipeline using the Evidently Python library. Select the evals, and compute the `Reports` that will summarize data, metrics, and test results.\\n\\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\\n\\n**Benefits of this approach**:\\n\\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they’re already stored elsewhere, like for retraining.\\n\\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\\n\\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection,',\n",
       "  'title': 'Overview',\n",
       "  'description': 'How production AI quality monitoring works.',\n",
       "  'filename': 'docs-main/docs/platform/monitoring_overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' `Reports` that will summarize data, metrics, and test results.\\n\\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\\n\\n**Benefits of this approach**:\\n\\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they’re already stored elsewhere, like for retraining.\\n\\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\\n\\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection, naturally work in batches since you need to collect a set of new data points before running them. Model quality checks often only happen when new labeled data arrives, which can be delayed. Analyzing prediction or user behavior shifts is also usually more meaningful when done at intervals like hourly or daily rather than recalculating after every single event.\\n\\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch).\\n\\n## Tracing with scheduled evals\\n\\n<Check>\\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\\n</Check>\\n\\n**Best for**: LLM-powered applications\\n\\n![](/images/monitoring_flow_tracing.png)\\n\\n**How it works:**\\n\\n* **Instrument your app**. Use the `Tracely` library to capture all relevant data from your application, including inputs, outputs and intermediate steps. ([Tracing](/docs/platform/tracing_setup)).\\n\\n* **Store raw data**. Evidently Platform stores al',\n",
       "  'title': 'Overview',\n",
       "  'description': 'How production AI quality monitoring works.',\n",
       "  'filename': 'docs-main/docs/platform/monitoring_overview.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': ' naturally work in batches since you need to collect a set of new data points before running them. Model quality checks often only happen when new labeled data arrives, which can be delayed. Analyzing prediction or user behavior shifts is also usually more meaningful when done at intervals like hourly or daily rather than recalculating after every single event.\\n\\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch).\\n\\n## Tracing with scheduled evals\\n\\n<Check>\\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\\n</Check>\\n\\n**Best for**: LLM-powered applications\\n\\n![](/images/monitoring_flow_tracing.png)\\n\\n**How it works:**\\n\\n* **Instrument your app**. Use the `Tracely` library to capture all relevant data from your application, including inputs, outputs and intermediate steps. ([Tracing](/docs/platform/tracing_setup)).\\n\\n* **Store raw data**. Evidently Platform stores all raw data, providing a complete record of activity.\\n\\n* **Schedule evaluations**. Set up evaluations to run automatically at scheduled times. This will generate Reports or run Tests directly on the Evidently Platform. You can also manually run evaluations anytime to assess individual outputs.\\n\\n**Benefits of this approach**:\\n\\n* **Solves the data capture**. You collect complex traces and all production data in one place, making it easier to manage and analyze.\\n\\n* **Easy to re-run evals**. With raw traces stored on the platform, you can easily re-run evaluations or add new metrics whenever needed.\\n\\n* **No-code**. Once your trace instrumentation is set up, you can manage everything from the UI.\\n\\n**Next step:** check the [Tracing Quickstart](/quickstart_tracing).',\n",
       "  'title': 'Overview',\n",
       "  'description': 'How production AI quality monitoring works.',\n",
       "  'filename': 'docs-main/docs/platform/monitoring_overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Check>\\nScheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\\n</Check>',\n",
       "  'title': 'Scheduled evals',\n",
       "  'description': 'Running managed evaluations over traces on a platform.',\n",
       "  'filename': 'docs-main/docs/platform/monitoring_scheduled_evals.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Evidently Platform helps you manage AI quality across the AI system lifecycle, from pre-deployment testing to production monitoring. It supports evaluations of open-ended LLM outputs, predictive tasks like classification, and complex workflows like AI agents.\\n\\n![](/images/dashboard_llm_tabs.gif)\\n\\n## Key features\\n\\n<Info>\\n  Evidently Platform has a lightweight open-source version for evaluation tracking and monitoring, and a Cloud/Enterprise version with extra features. [Check feature availability.](/faq/oss_vs_cloud)\\n</Info>\\n\\n<Tabs>\\n  <Tab title=\"Evaluations\">\\n    Run evaluations locally with the Evidently Python library or no-code on the platform. Use 100+ built-in evals and templates. Track, compare, and debug experiments.\\n    ![](/images/evals_explore_view-min.png)\\n  </Tab>\\n\\n  <Tab title=\"Datasets\">\\n    Manage and organize testing and production datasets. Store them on the platform paired with relevant evaluations. Collaborate to curate test cases.\\n    ![](/images/dataset_llm.png)\\n  </Tab>\\n\\n  <Tab title=\"Synthetic data\">\\n    Generate synthetic data for RAG, Q&A, or other use cases. Design test scenarios, edge cases, and adversarial inputs for safety evaluations and stress-testing.\\n    ![](/images/synth_data-min.png)\\n  </Tab>\\n\\n  <Tab title=\"Regression testing\">\\n     Combine evaluations in conditional Test Suites with Pass/Fail outcomes. Set alerts for failed Tests. Track results over time using the built-in dashboard. \\n    ![](/images/examples/llm_quickstart_tests.png)\\n  </Tab>\\n\\n  <Tab title=\"Monitoring\">\\n    Run evaluations for live systems in batch or real-time. Track results on a dashboard and connect back to raw data as needed. Set alerts for violations.\\n    ![](/images/dashboard_llm_light.png)\\n  </Tab>\\n\\n  <Tab title=\"Tracing\">\\n    Instrument your AI application to collect inputs, outputs and any intermediate steps. Automatically get a ready-made structured dataset for analysis.\\n    ![](/images/examples/tracing_tutorial_session_view.png)\\n  </Tab>\\n</Tabs>\\n\\nWhile',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Evidently Platform at a glance.',\n",
       "  'filename': 'docs-main/docs/platform/overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '</Tab>\\n\\n  <Tab title=\"Synthetic data\">\\n    Generate synthetic data for RAG, Q&A, or other use cases. Design test scenarios, edge cases, and adversarial inputs for safety evaluations and stress-testing.\\n    ![](/images/synth_data-min.png)\\n  </Tab>\\n\\n  <Tab title=\"Regression testing\">\\n     Combine evaluations in conditional Test Suites with Pass/Fail outcomes. Set alerts for failed Tests. Track results over time using the built-in dashboard. \\n    ![](/images/examples/llm_quickstart_tests.png)\\n  </Tab>\\n\\n  <Tab title=\"Monitoring\">\\n    Run evaluations for live systems in batch or real-time. Track results on a dashboard and connect back to raw data as needed. Set alerts for violations.\\n    ![](/images/dashboard_llm_light.png)\\n  </Tab>\\n\\n  <Tab title=\"Tracing\">\\n    Instrument your AI application to collect inputs, outputs and any intermediate steps. Automatically get a ready-made structured dataset for analysis.\\n    ![](/images/examples/tracing_tutorial_session_view.png)\\n  </Tab>\\n</Tabs>\\n\\nWhile many workflows can be run no-code directly on the platform, you’ll often need programmatic access – for example, to upload datasets or run local experimental evaluations. In these cases, you can use the Evidently Python library to interact with the Evidently Cloud API.\\n\\nTo collect input-outputs from your production AI systems, you\\'d also need to install Tracely, a lightweight tool based on OpenTelemetry.',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Evidently Platform at a glance.',\n",
       "  'filename': 'docs-main/docs/platform/overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) (or your [local workspace](/docs/setup/self-hosting)).\\n</Tip>\\n\\n## Create a Project\\n\\n<Tabs>\\n  <Tab title=\"Python\">\\n    To create a Project inside a workspace `ws` and Organization with an `org_id`:\\n\\n    ```\\n    project = ws.create_project(\"My test project\", org_id=\"YOUR_ORG_ID\")\\n    project.description = \"My project description\"\\n    project.save()\\n    ```\\n\\n    In self-hosted open-source installation, you do not need to pass the Org ID. To create a Project:\\n\\n    ```\\n    project = ws.create_project(\"My test project\")\\n    project.description = \"My project description\"\\n    project.save()\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"UI\">\\n    * **Create a Project.** Click on the “plus” sign on the home page, set a Project name and description.\\n\\n    * **Edit a Project**. To change the Project name or description, hover on the existing Project, click \"edit\" and make the changes.\\n  </Tab>\\n</Tabs>\\n\\n## Connect to a Project\\n\\n<Tip>\\n  **Project ID**. You can see the Project ID above the monitoring Dashboard inside your Project.\\n</Tip>\\n\\nTo connect to an existing Project from Python, use the `get_project` method.\\n\\n```python\\nproject = ws.get_project(\"PROJECT_ID\")\\n```\\n\\n## Working with a Project\\n\\n### Save changes\\n\\nAfter making any changes to the Project (like editing description or adding monitoring Panels), always use the `save()` command:\\n\\n```python\\nproject.save()\\n```\\n\\n### Browse Projects\\n\\nYou can see all available Projects on the monitoring homepage, or request a list programmatically. To get a list of all Projects in a workspace `ws`, use:\\n\\n```python\\nws.list_projects()\\n```\\n\\nTo find a specific Project by its name, use the `search_project` method:\\n\\n```python\\nws.search_project(\"project_name\")\\n```\\n\\n### \\\\[DANGER] Delete Project\\n\\n<Warning>\\n  Deleting a Project deletes all the data inside it.\\n</Warning>\\n\\n<Tabs>\\n  <Tab title=\"Python\">\\n    To delete the Project:\\n\\n    ```\\n    # ws.delete_project(\"PROJECT ID\")\\n    ```\\n  </Tab>\\n\\n  <Tab ',\n",
       "  'title': 'Manage Projects',\n",
       "  'description': 'Set up an evaluation or monitoring Project.',\n",
       "  'filename': 'docs-main/docs/platform/projects_manage.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 't ID**. You can see the Project ID above the monitoring Dashboard inside your Project.\\n</Tip>\\n\\nTo connect to an existing Project from Python, use the `get_project` method.\\n\\n```python\\nproject = ws.get_project(\"PROJECT_ID\")\\n```\\n\\n## Working with a Project\\n\\n### Save changes\\n\\nAfter making any changes to the Project (like editing description or adding monitoring Panels), always use the `save()` command:\\n\\n```python\\nproject.save()\\n```\\n\\n### Browse Projects\\n\\nYou can see all available Projects on the monitoring homepage, or request a list programmatically. To get a list of all Projects in a workspace `ws`, use:\\n\\n```python\\nws.list_projects()\\n```\\n\\nTo find a specific Project by its name, use the `search_project` method:\\n\\n```python\\nws.search_project(\"project_name\")\\n```\\n\\n### \\\\[DANGER] Delete Project\\n\\n<Warning>\\n  Deleting a Project deletes all the data inside it.\\n</Warning>\\n\\n<Tabs>\\n  <Tab title=\"Python\">\\n    To delete the Project:\\n\\n    ```\\n    # ws.delete_project(\"PROJECT ID\")\\n    ```\\n  </Tab>\\n\\n  <Tab title=\"UI\">\\n    Hover on the existing Project and click \"delete\".\\n  </Tab>\\n</Tabs>\\n\\n## Project parameters\\n\\nEach Project has the following parameters.\\n\\n| Parameter                                       | Description                                                                                     | Example                                                                                                                                                     |\\n| ----------------------------------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `name: str`                                     | Project name.                                                                                   | -                                                                                  ',\n",
       "  'title': 'Manage Projects',\n",
       "  'description': 'Set up an evaluation or monitoring Project.',\n",
       "  'filename': 'docs-main/docs/platform/projects_manage.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'title=\"UI\">\\n    Hover on the existing Project and click \"delete\".\\n  </Tab>\\n</Tabs>\\n\\n## Project parameters\\n\\nEach Project has the following parameters.\\n\\n| Parameter                                       | Description                                                                                     | Example                                                                                                                                                     |\\n| ----------------------------------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `name: str`                                     | Project name.                                                                                   | -                                                                                                                                                           |\\n| `id: UUID4 = Field(default_factory=uuid.uuid4)` | Unique identifier of the Project. Assigned automatically.                                       | -                                                                                                                                                           |\\n| `description: Optional[str] = None`             | Optional description. Visible when you browse Projects.                                         | -                                                                                                                                                           |\\n| `dashboard: DashboardConfig`                    | Dashboard configuration that describes the composition of the monitoring Panels.                | See [Dashboard Design](dashboard_add_panels) for details. You don\\'t need to explicitly pass `DashboardConfig` if you use the `.dashboard.add_panel` method. |\\n|',\n",
       "  'title': 'Manage Projects',\n",
       "  'description': 'Set up an evaluation or monitoring Project.',\n",
       "  'filename': 'docs-main/docs/platform/projects_manage.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': \"                                                                         |\\n| `id: UUID4 = Field(default_factory=uuid.uuid4)` | Unique identifier of the Project. Assigned automatically.                                       | -                                                                                                                                                           |\\n| `description: Optional[str] = None`             | Optional description. Visible when you browse Projects.                                         | -                                                                                                                                                           |\\n| `dashboard: DashboardConfig`                    | Dashboard configuration that describes the composition of the monitoring Panels.                | See [Dashboard Design](dashboard_add_panels) for details. You don't need to explicitly pass `DashboardConfig` if you use the `.dashboard.add_panel` method. |\\n| `date_from: Optional[datetime.datetime] = None` | Start DateTime of the monitoring Dashboard. By default it shows data for all available periods. | `datetime.now() + timedelta(-30)`                                                                                                                           |\\n| `date_to: Optional[datetime.datetime] = None`   | End DateTime of the monitoring Dashboard.                                                       | Works the same as `date_from`.                                                                                                                              |\",\n",
       "  'title': 'Manage Projects',\n",
       "  'description': 'Set up an evaluation or monitoring Project.',\n",
       "  'filename': 'docs-main/docs/platform/projects_manage.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Check>\\n  Projects are available in **Evidently OSS**, **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\n## What is a Project?\\n\\nA **Project** helps you organize data and evaluations for a specific use case. You can view all your Projects on the home page.\\n\\n![](/images/projects.png)\\n\\nEach Project:\\n\\n* Stores its own **datasets**, **reports**, and **traces**.\\n* Has a dedicated **dashboard** and **alerting** rules.\\n* Provides a **unique ID** for connecting via the **Python API** to send data, edit dashboards, and manage configurations. You can also manage everything through the UI.\\n\\n## What to put in one Project?\\n\\nYou can structure projects to suit your workflow. Here are some ideas:\\n\\n* **By Application or Model.** Create individual Projects for each LLM app or ML model.\\n* **By App Component.** For complex systems like AI agents, set up Projects for specific components, such as testing intent classification independently of other features.\\n* **By Test Scenario.** Use separate Projects for distinct test scenarios, like isolating safety or adversarial datasets from other evaluations.\\n* **By Phase.** Manage different development stages of the same app with separate Projects for experimentation/testing and production monitoring.\\n* **By Use Case.** Group data and evaluations for multiple ML models in one Project, organizing them with tags (e.g., \"version,\" \"location\").',\n",
       "  'title': 'Overview',\n",
       "  'description': 'Introduction to Projects.',\n",
       "  'filename': 'docs-main/docs/platform/projects_overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Check>\\n  Trace store and viewer are Pro features available in **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\nTracing uses the open-source `Tracely` library, based on OpenTelemetry.\\n\\n## What is LLM tracing?\\n\\nTracing lets you instrument your AI application to collect data for evaluation and analysis.\\n\\nIt captures detailed records of how your LLM app operates, including inputs, outputs and any intermediate steps and events (e.g., function calls). You define what to include.\\n\\nEvidently provides multiple ways to explore tracing data.\\n\\n<Tabs>\\n  <Tab title=\"Trace view\">\\n    See a timeline of execution steps with input-output details and latency.\\n\\n    ![](/images/tracing.png)\\n  </Tab>\\n\\n  <Tab title=\"Dataset view\">\\n    Automatically generate a tabular view for easier evaluation or labeling.\\n    ![](/images/examples/tracing_tutorial_dataset_view.png)\\n  </Tab>\\n\\n  <Tab title=\"Dialogue view\">\\n    For conversational applications, browse traces by user or session to focus on chat flows.\\n\\n    ![](/images/examples/tracing_tutorial_session_view.png)\\n  </Tab>\\n</Tabs>\\n\\nOnce you capture the data, you can also run evals on the tracing datasets.\\n\\n## Do I always need tracing?\\n\\nTracing is optional on the Evidently Platform. You can also:\\n\\n* Upload tabular datasets using Dataset API.\\n\\n* Run evals locally and send results to the platform without tracing.\\n\\nHowever, tracing is especially useful for understanding complex LLM chains and execution flows, both in experiments and production monitoring.',\n",
       "  'title': 'Overview',\n",
       "  'description': 'Introduction to Tracing.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Info>\\n  **Quickstart:** For a simple end-to-end example, check the [Tutorial.](../../quickstart_tracing)\\n</Info>\\n\\n## Installation\\n\\nInstall the `tracely` package from PyPi:\\n\\n```bash\\npip install tracely\\n```\\n\\n## Initialize tracing\\n\\n<Tip>\\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).&#x20;\\n</Tip>\\n\\nTo start sending traces, use `init_tracing`:\\n\\n```python\\nfrom tracely import init_tracing\\n\\ninit_tracing(\\n   address=\"https://app.evidently.cloud/\",\\n   api_key=\"YOUR_EVIDENTLY_TOKEN\",\\n   project_id=\"YOUR_PROJECT_ID\",\\n   export_name=\"YOUR_TRACING_DATASET_NAME\",\\n   )\\n```\\n\\nYou can also set parameters using environment variables with the specified names.\\n\\n### `init_tracing()` Function Arguments\\n\\n| Parameter       | Description                                                                                                                        | Environment Variable                    |\\n| --------------- | ---------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |\\n| `address`       | Trace collector address. Defaults to `https://app.evidently.cloud/`.                                                               | `EVIDENTLY_TRACE_COLLECTOR`             |\\n| `api_key`       | Evidently Cloud API key.                                                                                                           | `EVIDENTLY_TRACE_COLLECTOR_API_KEY` or `EVIDENTLY_API_KEY`    |\\n| `export_name`   | Tracing dataset name. Traces with the same name are grouped into a single dataset.                                                 | `EVIDENTLY_TRACE_COLLECTOR_EXPORT_NAME` |\\n| `project_id`    | Destination Project ID in Evidently Cloud.                                                                                         | `EVIDENTLY_TRACE_COLLECTOR_PROJECT_ID`  |\\n| `exporter_type` | Trace export protocol: `',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '------------------------------------------------------------------------------------------------------------- | --------------------------------------- |\\n| `address`       | Trace collector address. Defaults to `https://app.evidently.cloud/`.                                                               | `EVIDENTLY_TRACE_COLLECTOR`             |\\n| `api_key`       | Evidently Cloud API key.                                                                                                           | `EVIDENTLY_TRACE_COLLECTOR_API_KEY` or `EVIDENTLY_API_KEY`    |\\n| `export_name`   | Tracing dataset name. Traces with the same name are grouped into a single dataset.                                                 | `EVIDENTLY_TRACE_COLLECTOR_EXPORT_NAME` |\\n| `project_id`    | Destination Project ID in Evidently Cloud.                                                                                         | `EVIDENTLY_TRACE_COLLECTOR_PROJECT_ID`  |\\n| `exporter_type` | Trace export protocol: `grpc` or `http`.                                                                                           | -                                       |\\n| `as_global`     | Registers the tracing provider globally (`True`) or locally (`False`). Default: `True`. Set to false if you want to initiate tracing to multiple datasets from the same environment.| -                                       |\\n\\n\\n## Tracing dataset ID\\n\\nTo get the `export_id` of the tracing dataset, run:\\n\\n```\\nfrom tracely import get_info\\n\\nget_info()\\n```\\n\\nYou can use the `export_id` as a dataset id for download. See [datasets API](datasets_workflow).\\n\\n\\n## Decorator\\n\\nOnce `Tracely` is initialized, you can decorate your functions with `trace_event` to start collecting traces for a specific function:\\n\\n```python\\nfrom tracely import init_tracing\\nfrom tracely import trace_event\\n\\n@trace_event()\\ndef process_request(question: str, session_id: str):\\n    # do work\\n    return \"work done\"\\n```\\n\\nYou can also specify which function argumen',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'grpc` or `http`.                                                                                           | -                                       |\\n| `as_global`     | Registers the tracing provider globally (`True`) or locally (`False`). Default: `True`. Set to false if you want to initiate tracing to multiple datasets from the same environment.| -                                       |\\n\\n\\n## Tracing dataset ID\\n\\nTo get the `export_id` of the tracing dataset, run:\\n\\n```\\nfrom tracely import get_info\\n\\nget_info()\\n```\\n\\nYou can use the `export_id` as a dataset id for download. See [datasets API](datasets_workflow).\\n\\n\\n## Decorator\\n\\nOnce `Tracely` is initialized, you can decorate your functions with `trace_event` to start collecting traces for a specific function:\\n\\n```python\\nfrom tracely import init_tracing\\nfrom tracely import trace_event\\n\\n@trace_event()\\ndef process_request(question: str, session_id: str):\\n    # do work\\n    return \"work done\"\\n```\\n\\nYou can also specify which function arguments should be included in the trace.\\n\\n**Example 1.** To log all arguments of the function:\\n\\n```\\n@trace_event()\\n```\\n\\n**Example 2.** To log only input arguments of the function:\\n\\n```\\n@trace_event(track_args=[])\\n```\\n\\n**Example 3.** To log only \"arg1\" and \"arg2\":\\n\\n```\\n@trace_event(track_args=[\"arg1\", \"arg2\"])\\n```\\n\\n### `trace_event` Decorator Arguments\\n\\n| **Parameter**                      | **Description**                                                                                                            | **Default**                     |\\n| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | ------------------------------- |\\n| `span_name: Optional[str]`         | The name of the span to send in the event.                                                                                 | Function name                   |\\n| `track_args: Optional[List[str]]`  | A list of functio',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'ts should be included in the trace.\\n\\n**Example 1.** To log all arguments of the function:\\n\\n```\\n@trace_event()\\n```\\n\\n**Example 2.** To log only input arguments of the function:\\n\\n```\\n@trace_event(track_args=[])\\n```\\n\\n**Example 3.** To log only \"arg1\" and \"arg2\":\\n\\n```\\n@trace_event(track_args=[\"arg1\", \"arg2\"])\\n```\\n\\n### `trace_event` Decorator Arguments\\n\\n| **Parameter**                      | **Description**                                                                                                            | **Default**                     |\\n| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | ------------------------------- |\\n| `span_name: Optional[str]`         | The name of the span to send in the event.                                                                                 | Function name                   |\\n| `track_args: Optional[List[str]]`  | A list of function arguments to include in the event.                                                                      | `None` (all arguments included) |\\n| `ignore_args: Optional[List[str]]` | A list of function arguments to exclude, e.g., arguments that contain sensitive data.                                      | `None` (no arguments ignored)   |\\n| `track_output: Optional[bool]`     | Indicates whether to track the function\\'s return value.                                                                    | `True`                          |\\n| `parse_output: Optional[bool]`     | Indicates whether the result should be parsed, e.g., `dict`, `list`, and `tuple` types will be split into separate fields. | `True`                          |\\n\\n## Nested events (Spans)\\n\\nMany LLM workflows involve multiple steps — such as retrieval followed by generation, or extraction followed by summarization. In these cases, it\\'s useful to trace all steps as part of a single parent trace, with each step recorded as a ',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'n arguments to include in the event.                                                                      | `None` (all arguments included) |\\n| `ignore_args: Optional[List[str]]` | A list of function arguments to exclude, e.g., arguments that contain sensitive data.                                      | `None` (no arguments ignored)   |\\n| `track_output: Optional[bool]`     | Indicates whether to track the function\\'s return value.                                                                    | `True`                          |\\n| `parse_output: Optional[bool]`     | Indicates whether the result should be parsed, e.g., `dict`, `list`, and `tuple` types will be split into separate fields. | `True`                          |\\n\\n## Nested events (Spans)\\n\\nMany LLM workflows involve multiple steps — such as retrieval followed by generation, or extraction followed by summarization. In these cases, it\\'s useful to trace all steps as part of a single parent trace, with each step recorded as a nested child span.\\n\\nYou can trace multi-step workflows using the `@trace_event` decorator and nesting the functions. If a traced function is called inside another traced function, it will automatically appear as a nested child span, as long as it\\'s executed in the same call context (same thread).\\n\\nFor example:\\n\\n```python\\n@trace_event(span_name=\"extraction\")\\ndef extract_info(document):\\n    …\\n\\n@trace_event(span_name=\"summarization\")\\ndef summarize_info(document):\\n    …\\n\\n@trace_event(span_name=\"document_processing\")\\ndef process_document(document):\\n    extract_output = extract_info(document)\\n    summary_output = summarize_info(document)\\n    return {\\n        \"document\": document,\\n        \"extraction_output\": extract_output,\\n        \"summary_output\": summary_output\\n    }\\n```\\n\\nThis results in the following trace structure:\\n\\n```python\\ndocument_processing\\n├── extraction\\n└── summarization\\n```\\n\\n## Context manager\\n\\nTo create a trace event without using a decorator (e.g., for a specific piece of cod',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'nested child span.\\n\\nYou can trace multi-step workflows using the `@trace_event` decorator and nesting the functions. If a traced function is called inside another traced function, it will automatically appear as a nested child span, as long as it\\'s executed in the same call context (same thread).\\n\\nFor example:\\n\\n```python\\n@trace_event(span_name=\"extraction\")\\ndef extract_info(document):\\n    …\\n\\n@trace_event(span_name=\"summarization\")\\ndef summarize_info(document):\\n    …\\n\\n@trace_event(span_name=\"document_processing\")\\ndef process_document(document):\\n    extract_output = extract_info(document)\\n    summary_output = summarize_info(document)\\n    return {\\n        \"document\": document,\\n        \"extraction_output\": extract_output,\\n        \"summary_output\": summary_output\\n    }\\n```\\n\\nThis results in the following trace structure:\\n\\n```python\\ndocument_processing\\n├── extraction\\n└── summarization\\n```\\n\\n## Context manager\\n\\nTo create a trace event without using a decorator (e.g., for a specific piece of code), you can use the context manager:\\n\\n```python\\nimport uuid\\n\\nfrom tracely import init_tracing\\nfrom tracely import create_trace_event\\n\\ninit_tracing()\\n\\nsession_id = str(uuid.uuid4())\\n\\nwith create_trace_event(\"external_span\", session_id=session_id) as event:\\n    event.set_attribute(\"my-attribute\", \"value\")\\n    # do work\\n    event.set_result({\"data\": \"data\"})\\n```\\n\\nYou can also trace multi-step workflows using context blocks. This gives you fine-grained control — useful when tracing inline code or scripts.\\nFor example, you can nest multiple `create_trace_event()` calls inline inside the same function, using `with` blocks.\\n\\n```\\ndef process_document(document):\\n    with create_trace_event(\"document_processing\", document=document):\\n        with create_trace_event(\"extraction\"):\\n            ...\\n        with create_trace_event(\"summarization\"):\\n            ...\\n```\\n\\n### `create_trace_event` Function Arguments\\n\\n| Parameter      | Description                                                          ',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'e), you can use the context manager:\\n\\n```python\\nimport uuid\\n\\nfrom tracely import init_tracing\\nfrom tracely import create_trace_event\\n\\ninit_tracing()\\n\\nsession_id = str(uuid.uuid4())\\n\\nwith create_trace_event(\"external_span\", session_id=session_id) as event:\\n    event.set_attribute(\"my-attribute\", \"value\")\\n    # do work\\n    event.set_result({\"data\": \"data\"})\\n```\\n\\nYou can also trace multi-step workflows using context blocks. This gives you fine-grained control — useful when tracing inline code or scripts.\\nFor example, you can nest multiple `create_trace_event()` calls inline inside the same function, using `with` blocks.\\n\\n```\\ndef process_document(document):\\n    with create_trace_event(\"document_processing\", document=document):\\n        with create_trace_event(\"extraction\"):\\n            ...\\n        with create_trace_event(\"summarization\"):\\n            ...\\n```\\n\\n### `create_trace_event` Function Arguments\\n\\n| Parameter      | Description                                                                            | Default |\\n| -------------- | -------------------------------------------------------------------------------------- | ------- |\\n| `name`         | Span name.                                                                             | -       |\\n| `parse_output` | Whether to parse the result into separate fields for `dict`, `list`, or `tuple` types. | `True`  |\\n| `params`       | Key-value parameters to set as attributes.                                             | -       |\\n\\n### `event` Object Methods\\n\\n| Method          | Description                                                        |\\n| --------------- | ------------------------------------------------------------------ |\\n| `set_attribute` | Sets a custom attribute for the event.                             |\\n| `set_result`    | Sets a result for the event. Only one result can be set per event. |\\n\\n## Sessions\\n\\nIf your trace events are created in separate functions or threads you can also pass a shared `sessi',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': '                  | Default |\\n| -------------- | -------------------------------------------------------------------------------------- | ------- |\\n| `name`         | Span name.                                                                             | -       |\\n| `parse_output` | Whether to parse the result into separate fields for `dict`, `list`, or `tuple` types. | `True`  |\\n| `params`       | Key-value parameters to set as attributes.                                             | -       |\\n\\n### `event` Object Methods\\n\\n| Method          | Description                                                        |\\n| --------------- | ------------------------------------------------------------------ |\\n| `set_attribute` | Sets a custom attribute for the event.                             |\\n| `set_result`    | Sets a result for the event. Only one result can be set per event. |\\n\\n## Sessions\\n\\nIf your trace events are created in separate functions or threads you can also pass a shared `session_id`. In this case traces will be separate but you can view the session in the UI to join them together - e.g. to read the chat conversation.\\n\\nSee the example above the \"Context Manager\" session.\\n\\n## Add event attributes\\n\\nIf you want to add a new attribute to an active event span, you can use `get_current_span()` to get access to the current span:\\n\\n```python\\nimport uuid\\n\\nfrom tracely import init_tracing\\nfrom tracely import create_trace_event\\nfrom tracely import get_current_span\\n\\ninit_tracing()\\n\\nsession_id = str(uuid.uuid4())\\n\\nwith create_trace_event(\"external_span\", session_id=session_id):\\n    span = get_current_span()\\n    span.set_attribute(\"my-attribute\", \"value\")\\n    # do work\\n    span.set_result({\"data\": \"data\"})\\n\\n```\\n\\n### `get_current_span()` Object  Methods\\n\\n| **Method**      | **Description**                                                                                              |\\n| --------------- | ------------------------------------------------------------------------',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'on_id`. In this case traces will be separate but you can view the session in the UI to join them together - e.g. to read the chat conversation.\\n\\nSee the example above the \"Context Manager\" session.\\n\\n## Add event attributes\\n\\nIf you want to add a new attribute to an active event span, you can use `get_current_span()` to get access to the current span:\\n\\n```python\\nimport uuid\\n\\nfrom tracely import init_tracing\\nfrom tracely import create_trace_event\\nfrom tracely import get_current_span\\n\\ninit_tracing()\\n\\nsession_id = str(uuid.uuid4())\\n\\nwith create_trace_event(\"external_span\", session_id=session_id):\\n    span = get_current_span()\\n    span.set_attribute(\"my-attribute\", \"value\")\\n    # do work\\n    span.set_result({\"data\": \"data\"})\\n\\n```\\n\\n### `get_current_span()` Object  Methods\\n\\n| **Method**      | **Description**                                                                                              |\\n| --------------- | ------------------------------------------------------------------------------------------------------------ |\\n| `set_attribute` | Adds a new attribute to the active span.                                                                     |\\n| `set_result`    | Sets a result field for the active span. (*Has no effect in decorated functions that define return values).* |\\n\\n## Connecting event into a trace\\n\\nSometimes events happen across different systems, but it’s helpful to link them all into a single trace. You can do this using `tracely.bind_to_trace`:\\n\\n```python\\n@tracely.trace_event()\\ndef process_request(question: str, session_id: str):\\n    # do work\\n    return \"work done\"\\n\\n# trace id is unique 128-bit integer representing single trace\\ntrace_id = 1234\\n\\nwith tracely.bind_to_trace(trace_id):\\n    process_request(...)\\n```\\n\\nIn this example, instead of creating a new trace ID for each event, all events will be attached to the existing trace with the given `trace_id`.\\n\\n<Warning>\\n  In this case you manage the `trace_id` yourself, so you need to make sure it’s tr',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '------------------------------------ |\\n| `set_attribute` | Adds a new attribute to the active span.                                                                     |\\n| `set_result`    | Sets a result field for the active span. (*Has no effect in decorated functions that define return values).* |\\n\\n## Connecting event into a trace\\n\\nSometimes events happen across different systems, but it’s helpful to link them all into a single trace. You can do this using `tracely.bind_to_trace`:\\n\\n```python\\n@tracely.trace_event()\\ndef process_request(question: str, session_id: str):\\n    # do work\\n    return \"work done\"\\n\\n# trace id is unique 128-bit integer representing single trace\\ntrace_id = 1234\\n\\nwith tracely.bind_to_trace(trace_id):\\n    process_request(...)\\n```\\n\\nIn this example, instead of creating a new trace ID for each event, all events will be attached to the existing trace with the given `trace_id`.\\n\\n<Warning>\\n  In this case you manage the `trace_id` yourself, so you need to make sure it’s truly unique. If you  reuse the same `trace_id`, all events will be joined, even if they don’t belong together.\\n</Warning>',\n",
       "  'title': 'Set up tracing',\n",
       "  'description': 'How to collect data from a live LLM app.',\n",
       "  'filename': 'docs-main/docs/platform/tracing_setup.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '## 1. Create an Account\\n\\n- If not yet, sign up for a [free Evidently Cloud account](https://app.evidently.cloud/signup).\\n- After logging in, create an **Organization** and name it.\\n\\n## 2. Connect from Python\\n\\n<Info>\\n  You need this for programmatic tasks like tracing or logging local evals. Many other tasks can be done directly on the platform.\\n</Info>\\n\\n### Get a Token\\n\\nClick the **Key** menu icon to open the [Token page](https://app.evidently.cloud/token). Generate and save token securely.\\n\\n### Install Evidently\\n\\n[Install](/docs/setup/installation) the Evidently Python library.\\n\\n```python\\npip install evidently ## or pip install evidently[llm]\\n```\\n\\n### Connect\\n\\nImport the cloud workspace and pass your API token to connect:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n\\nws = CloudWorkspace(\\ntoken=\"API_KEY\",\\nurl=\"https://app.evidently.cloud\")\\n```\\n\\n<Note>\\nFor Evidently 0.6.7 and Evidently Cloud v1, use `from evidently.ui.workspace.cloud import CloudWorkspace`. [Read more](/faq/cloud_v2).\\n</Note>\\n\\n<Check>\\n  You can also provide the API key by setting the environment variable\\xa0`EVIDENTLY_API_KEY`.\\n</Check>\\n\\nYou are all set\\\\! Create a Project and run your first [evaluation](/quickstart_llm).',\n",
       "  'title': 'Evidently Cloud',\n",
       "  'description': 'How to set up Evidently Cloud account.',\n",
       "  'icon': 'cloud',\n",
       "  'filename': 'docs-main/docs/setup/cloud.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '## Evidently\\n\\n`Evidently` is available as a Python package. Install it using the **pip package manager**:\\n\\n```python\\npip install evidently\\n```\\n\\nTo install `evidently` using **conda installer**, run:\\n\\n```sh\\nconda install -c conda-forge evidently\\n```\\n\\n## Evidently LLM\\n\\nTo run evaluations specific to LLMs that include additional dependencies, run:\\n\\n```python\\npip install evidently[llm]\\n```\\n\\n## Tracely\\n\\nTo use tracing based on OpenTelemetry, install the sister package **tracely**:\\n\\n```sh\\npip install tracely\\n```',\n",
       "  'title': 'Installation',\n",
       "  'description': 'How to install the open-source Python library.',\n",
       "  'icon': 'play',\n",
       "  'filename': 'docs-main/docs/setup/installation.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Info>\\n  This page explains how to self-host the lightweight open-source platform. [Contact us](https://www.evidentlyai.com/get-demo) for Enterprise version with extra features and support. Compare [OSS vs. Enterprise/Cloud](/faq/oss_vs_cloud).\\n</Info>\\n\\nIn addition to using Evidently Python library, you can self-host the UI Service to get a monitoring Dashboard and organize the results of your evaluations. This is optional: you can also view evaluation results in Python or export to JSON or HTML.\\n\\nTo get a self-hostable Dashboard, you must:\\n\\n- Create a Workspace (local or remote) to store your data.\\n- Launch the UI service.\\n\\n## 1. Create a Workspace\\n\\n<Tip>\\n  Sign up for a free [Evidently Cloud](cloud) account to get a managed version instantly.\\n</Tip>\\n\\nOnce you [install Evidently](/docs/setup/installation), you will need to create a `workspace`. This designates a remote or local directory where you will store the evaluation results (JSON Reports called `snapshots`). The UI Service will read the data from this source.\\n\\nThere are three scenarios, based on where you run the UI Service and store data.\\n\\n- **Local Workspace**. Both the UI Service and data storage are local.\\n- **Remote Workspace**. Both the UI Service and data storage are remote.\\n- **Workspace with remote data storage**. Run the UI Service and store data on different servers.\\n\\n### Local Workspace\\n\\nHere, you generate, store the snapshots and run the monitoring UI on the same machine.\\n\\nImports:\\n\\n```python\\nfrom evidently.ui.workspace import Workspace\\nfrom evidently.ui.workspace import WorkspaceBase\\n```\\n\\nTo create a local Workspace and assign a name:\\n\\n```python\\nws = Workspace.create(\"evidently_ui_workspace\")\\n```\\n\\nYou can pass a `path` parameter to specify the path to a local directory.\\n\\n### Remote Workspace\\n\\n<Info>\\n  **Code example (Docker)**. See the [remote service example](https://github.com/evidentlyai/evidently/tree/main/examples/service).\\n</Info>\\n\\nIn this scenario, you send the snapshots to a remote serv',\n",
       "  'title': 'Self-hosting',\n",
       "  'description': 'How to self-host the open-source Evidently UI service.',\n",
       "  'icon': 'window-restore',\n",
       "  'filename': 'docs-main/docs/setup/self-hosting.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' read the data from this source.\\n\\nThere are three scenarios, based on where you run the UI Service and store data.\\n\\n- **Local Workspace**. Both the UI Service and data storage are local.\\n- **Remote Workspace**. Both the UI Service and data storage are remote.\\n- **Workspace with remote data storage**. Run the UI Service and store data on different servers.\\n\\n### Local Workspace\\n\\nHere, you generate, store the snapshots and run the monitoring UI on the same machine.\\n\\nImports:\\n\\n```python\\nfrom evidently.ui.workspace import Workspace\\nfrom evidently.ui.workspace import WorkspaceBase\\n```\\n\\nTo create a local Workspace and assign a name:\\n\\n```python\\nws = Workspace.create(\"evidently_ui_workspace\")\\n```\\n\\nYou can pass a `path` parameter to specify the path to a local directory.\\n\\n### Remote Workspace\\n\\n<Info>\\n  **Code example (Docker)**. See the [remote service example](https://github.com/evidentlyai/evidently/tree/main/examples/service).\\n</Info>\\n\\nIn this scenario, you send the snapshots to a remote server. You must run the Monitoring UI on the same remote server. It will directly interface with the filesystem where the snapshots are stored.\\n\\nImports:\\n\\n```python\\nfrom evidently.ui.remote import RemoteWorkspace\\nfrom evidently.ui.workspace import Workspace\\nfrom evidently.ui.workspace import WorkspaceBase\\n```\\n\\nTo create a remote Workspace (UI should be running at this address):\\n\\n```python\\nworkspace = RemoteWorkspace(\"http://localhost:8000\")\\n```\\n\\nYou can pass the following parameters:\\n\\n| Parameter                  | Description                                                                                  |\\n| -------------------------- | -------------------------------------------------------------------------------------------- |\\n| `self.base_url = base_url` | URL for the remote UI service.                                                               |\\n| `self.secret = secret`     | String with secret, None by default. Use it if access to the URL is protected by a password. |\\n\\n### Remo',\n",
       "  'title': 'Self-hosting',\n",
       "  'description': 'How to self-host the open-source Evidently UI service.',\n",
       "  'icon': 'window-restore',\n",
       "  'filename': 'docs-main/docs/setup/self-hosting.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'er. You must run the Monitoring UI on the same remote server. It will directly interface with the filesystem where the snapshots are stored.\\n\\nImports:\\n\\n```python\\nfrom evidently.ui.remote import RemoteWorkspace\\nfrom evidently.ui.workspace import Workspace\\nfrom evidently.ui.workspace import WorkspaceBase\\n```\\n\\nTo create a remote Workspace (UI should be running at this address):\\n\\n```python\\nworkspace = RemoteWorkspace(\"http://localhost:8000\")\\n```\\n\\nYou can pass the following parameters:\\n\\n| Parameter                  | Description                                                                                  |\\n| -------------------------- | -------------------------------------------------------------------------------------------- |\\n| `self.base_url = base_url` | URL for the remote UI service.                                                               |\\n| `self.secret = secret`     | String with secret, None by default. Use it if access to the URL is protected by a password. |\\n\\n### Remote snapshot storage\\n\\nIn the examples above, you store the snapshots and run the UI on the same server. Alternatively, you can store snapshots in a remote data store (such as an S3 bucket). The Monitoring UI service will interface with the designated data store to read the snapshot data.\\n\\nTo connect to data stores Evidently uses `fsspec` that allows accessing data on remote file systems via a standard Python interface.\\n\\nYou can verify supported data stores in the Fsspec documentation ([built-in implementations](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations) and [other implementations](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)).\\n\\nFor example, to read snapshots from an S3 bucket (with MinIO running on localhost:9000), you must specify environment variables:\\n\\n```\\nFSSPEC_S3_ENDPOINT_URL=http://localhost:9000/\\nFSSPEC_S3_KEY=my_key FSSPEC_S3_SECRET=my_secret\\nevidently ui --workspace s3://my_bucket/workspace\\n```\\n',\n",
       "  'title': 'Self-hosting',\n",
       "  'description': 'How to self-host the open-source Evidently UI service.',\n",
       "  'icon': 'window-restore',\n",
       "  'filename': 'docs-main/docs/setup/self-hosting.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'te snapshot storage\\n\\nIn the examples above, you store the snapshots and run the UI on the same server. Alternatively, you can store snapshots in a remote data store (such as an S3 bucket). The Monitoring UI service will interface with the designated data store to read the snapshot data.\\n\\nTo connect to data stores Evidently uses `fsspec` that allows accessing data on remote file systems via a standard Python interface.\\n\\nYou can verify supported data stores in the Fsspec documentation ([built-in implementations](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations) and [other implementations](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)).\\n\\nFor example, to read snapshots from an S3 bucket (with MinIO running on localhost:9000), you must specify environment variables:\\n\\n```\\nFSSPEC_S3_ENDPOINT_URL=http://localhost:9000/\\nFSSPEC_S3_KEY=my_key FSSPEC_S3_SECRET=my_secret\\nevidently ui --workspace s3://my_bucket/workspace\\n```\\n\\n### [DANGER] Delete Workspace\\n\\nTo delete a Workspace, run the command from the Terminal:\\n\\n```bash\\ncd src/evidently/ui/\\nrm -r workspace\\n```\\n\\n<Warning>\\n  **You are deleting all the data**. This command will delete all the data stored in the workspace folder. To maintain access to the generated Reports, you must store them elsewhere.\\n</Warning>\\n\\n## 2. Launch the UI service\\n\\nTo launch the Evidently UI service, you must run a command in the Terminal.\\n\\n**Option 1**. If you log snapshots to a local Workspace directory, you run Evidently UI over it. Run the following command from the directory where the Workspace folder is located.\\n\\n```bash\\nevidently ui\\n```\\n\\n**Option 2**. If you have your Project in a different Workspace, specify the path:\\n\\n```bash\\nevidently ui --workspace . /workspace\\n```\\n\\n**Option 3**. If you have your Project in a specified Workspace and run the UI service at the specific port (if the default port 8000 is occupied).\\n\\n```bash\\nevidently ui --workspace ./workspace --port 8080',\n",
       "  'title': 'Self-hosting',\n",
       "  'description': 'How to self-host the open-source Evidently UI service.',\n",
       "  'icon': 'window-restore',\n",
       "  'filename': 'docs-main/docs/setup/self-hosting.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '\\n### [DANGER] Delete Workspace\\n\\nTo delete a Workspace, run the command from the Terminal:\\n\\n```bash\\ncd src/evidently/ui/\\nrm -r workspace\\n```\\n\\n<Warning>\\n  **You are deleting all the data**. This command will delete all the data stored in the workspace folder. To maintain access to the generated Reports, you must store them elsewhere.\\n</Warning>\\n\\n## 2. Launch the UI service\\n\\nTo launch the Evidently UI service, you must run a command in the Terminal.\\n\\n**Option 1**. If you log snapshots to a local Workspace directory, you run Evidently UI over it. Run the following command from the directory where the Workspace folder is located.\\n\\n```bash\\nevidently ui\\n```\\n\\n**Option 2**. If you have your Project in a different Workspace, specify the path:\\n\\n```bash\\nevidently ui --workspace . /workspace\\n```\\n\\n**Option 3**. If you have your Project in a specified Workspace and run the UI service at the specific port (if the default port 8000 is occupied).\\n\\n```bash\\nevidently ui --workspace ./workspace --port 8080\\n```\\n\\nTo view the Evidently interface, go to URL http://localhost:8000 or a specified port in your web browser.\\n\\n### Demo projects\\n\\nTo launch the Evidently service with the demo projects, run:\\n\\n```\\nevidently ui --demo-projects all\\n```\\n\\n## Tutorial\\n\\nCheck this tutorial for an end-to-end example:\\n\\n<Card title=\"Evidently UI tutorial\" icon=\"laptop-code\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/service/workspace_tutorial.ipynb\">\\n  How to create a workspace, project and run Reports. \\n</Card>',\n",
       "  'title': 'Self-hosting',\n",
       "  'description': 'How to self-host the open-source Evidently UI service.',\n",
       "  'icon': 'window-restore',\n",
       "  'filename': 'docs-main/docs/setup/self-hosting.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'You can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\\n\\n## How the integration work:\\n\\n- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\\n- Run your LLM system or agent against those inputs inside CI.\\n- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\\n  - LLM judges (e.g., tone, helpfulness, correctness)\\n  - Custom Python functions\\n  - Dataset-level metrics like classification quality\\n- If any test fails, the CI job fails.\\n- You get a detailed test report with pass/fail status and metrics.\\n\\n![](/images/examples/github_actions.gif)\\n\\nResults are stored locally or pushed to Evidently Cloud for deeper review and tracking.\\n\\nThe final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\\n\\n## Code example and tutorial\\n\\n👉 Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\\n\\nAction is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report',\n",
       "  'title': 'Evidently and GitHub actions',\n",
       "  'description': 'Testing LLM outputs as part of the CI/CD flow.',\n",
       "  'filename': 'docs-main/examples/GitHub_actions.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Check the video walkthough and code tutorial.\\n\\n| **Tutorial**                     | **Description**                                                                                                                                                                                                                                                                                                                                                                                          | **Code example**                                                                                                                         | **Video**                                                                               |\\n| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\\n| **LLM Evaluation Methods**       | Tutorial with an overview of methods. <ul>   <li>   Part 1. Anatomy of a single evaluation. Covers basic LLM evaluation API and setup.</li>      <li>   Part 2. Reference-based evaluation: exact match, semantic similarity, BERTScore, and LLM judge.</li>      <li>   Part 3. Reference-free evaluation: text statistics, regex, ML models, LLM judges, and session-level evaluators.</li>      </ul> | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb) | <ul>   <li>   [Video 1](https://www.youtube.com/watch?v=6JGR',\n",
       "  'title': 'LLM evaluations',\n",
       "  'description': 'Run different LLM evaluation methods.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/examples/LLM_evals.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '----------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\\n| **LLM Evaluation Methods**       | Tutorial with an overview of methods. <ul>   <li>   Part 1. Anatomy of a single evaluation. Covers basic LLM evaluation API and setup.</li>      <li>   Part 2. Reference-based evaluation: exact match, semantic similarity, BERTScore, and LLM judge.</li>      <li>   Part 3. Reference-free evaluation: text statistics, regex, ML models, LLM judges, and session-level evaluators.</li>      </ul> | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb) | <ul>   <li>   [Video 1](https://www.youtube.com/watch?v=6JGRdMGbNCI&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=3)</li>      <li>   [Video 2](https://www.youtube.com/watch?v=yD20c-KAImE&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=4)</li>      <li>   [Video 3](https://www.youtube.com/watch?v=-zoIqOpt2DA&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=5)</li>      </ul> |',\n",
       "  'title': 'LLM evaluations',\n",
       "  'description': 'Run different LLM evaluation methods.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/examples/LLM_evals.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 's/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quic',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I add another user to my account?\",\\n     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\\n     \"To add a secondary user, go to \\'Account Settings\\', select \\'Manage Users\\', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\\n     \"incorrect\", \"contradiction (incorrect answer)\"],\\n  \\n    [\"Is it possible to link multiple bank accounts?\",\\n     \"Yes, you can link multiple bank accounts by going to \\'Account Settings\\' in the menu and selecting \\'Add Bank Account\\'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\\n     \"You can add multiple bank accounts by visiting \\'Accounts\\' in the menu and choosing \\'',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'kly and easily.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I add another user to my account?\",\\n     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\\n     \"To add a secondary user, go to \\'Account Settings\\', select \\'Manage Users\\', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\\n     \"incorrect\", \"contradiction (incorrect answer)\"],\\n  \\n    [\"Is it possible to link multiple bank accounts?\",\\n     \"Yes, you can link multiple bank accounts by going to \\'Account Settings\\' in the menu and selecting \\'Add Bank Account\\'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\\n     \"You can add multiple bank accounts by visiting \\'Accounts\\' in the menu and choosing \\'Add Bank Account\\'. Enter your bank details as prompted and complete the verification process for each account to link them successfully.\",\\n     \"incorrect\", \"contradiction (incorrect menu item)\"],\\n  \\n    [\"Can I use your service for cryptocurrency transactions?\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Hi, can I get a detailed report of my monthly transactions?\",\\n     \"Yes, you can generate a detailed monthly report of your transactions by logging into your account, going to \\'Transaction History\\', and selecting \\'Generate Report\\'. You can customize the re',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'Add Bank Account\\'. Enter your bank details as prompted and complete the verification process for each account to link them successfully.\",\\n     \"incorrect\", \"contradiction (incorrect menu item)\"],\\n  \\n    [\"Can I use your service for cryptocurrency transactions?\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Hi, can I get a detailed report of my monthly transactions?\",\\n     \"Yes, you can generate a detailed monthly report of your transactions by logging into your account, going to \\'Transaction History\\', and selecting \\'Generate Report\\'. You can customize the report by selecting specific dates or transaction types.\",\\n     \"You can get a detailed monthly report by logging into your account, navigating to \\'Transaction History\\', and clicking on \\'Generate Report\\'. Customize your report by choosing the date range and types of transactions you want to include.\",\\n     \"correct\", \"\"],\\n  \\n    [\"I am traveling to the US. Can I use the app there?\",\\n     \"Yes, you can use the app in the US just like you do at home. Ensure you have an internet connection. You may also want to update your app to the latest version before traveling for optimal performance.\",\\n     \"The app will work in the US without any issues. Just make sure you have access to the internet. For the best experience, update your app to the latest version before you travel.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I link my payment account to a new mobile number?\",\\n     \"To link a new mobile number, log in to your account, go to \\'Account Settings\\', select \\'Mobile Number\\', and follow the instruc',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'port by selecting specific dates or transaction types.\",\\n     \"You can get a detailed monthly report by logging into your account, navigating to \\'Transaction History\\', and clicking on \\'Generate Report\\'. Customize your report by choosing the date range and types of transactions you want to include.\",\\n     \"correct\", \"\"],\\n  \\n    [\"I am traveling to the US. Can I use the app there?\",\\n     \"Yes, you can use the app in the US just like you do at home. Ensure you have an internet connection. You may also want to update your app to the latest version before traveling for optimal performance.\",\\n     \"The app will work in the US without any issues. Just make sure you have access to the internet. For the best experience, update your app to the latest version before you travel.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I link my payment account to a new mobile number?\",\\n     \"To link a new mobile number, log in to your account, go to \\'Account Settings\\', select \\'Mobile Number\\', and follow the instructions to verify your new number. You will need to enter the new number and verify it via a code sent to your phone.\",\\n     \"To add a new number, navigate to the \\'Account Settings\\' section, select \\'Mobile Number\\' and proceed with the steps to add and confirm the new number. Enter the new mobile number and verify it using the code sent to your phone.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Can I receive notifications for transactions in real-time?\",\\n     \"Yes, you can enable real-time notifications for transactions by going to \\'Account Settings\\', then \\'Notifications\\', and turning on \\'Transaction Alerts\\'. You can choose to receive alerts via SMS, email, or push notifications on your mobile device.\",\\n     \"To receive real-time notifications for transactions, log into your account, go to \\'Account Settings\\', select \\'Notifications\\', and enable \\'Transaction Alerts\\'. Choose your preferred notification method between email or push notifications.\",\\n     \"incorrect\", \"omits information (sms notification)\"',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'tions to verify your new number. You will need to enter the new number and verify it via a code sent to your phone.\",\\n     \"To add a new number, navigate to the \\'Account Settings\\' section, select \\'Mobile Number\\' and proceed with the steps to add and confirm the new number. Enter the new mobile number and verify it using the code sent to your phone.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Can I receive notifications for transactions in real-time?\",\\n     \"Yes, you can enable real-time notifications for transactions by going to \\'Account Settings\\', then \\'Notifications\\', and turning on \\'Transaction Alerts\\'. You can choose to receive alerts via SMS, email, or push notifications on your mobile device.\",\\n     \"To receive real-time notifications for transactions, log into your account, go to \\'Account Settings\\', select \\'Notifications\\', and enable \\'Transaction Alerts\\'. Choose your preferred notification method between email or push notifications.\",\\n     \"incorrect\", \"omits information (sms notification)\"],\\n  \\n    [\"Hey, can I set up automatic transfers to my savings account?\",\\n     \"Yes, you can set up automatic transfers by going to \\'Account Settings\\', selecting \\'Automatic Transfers\\', and specifying the amount and frequency. You can choose to transfer weekly, bi-weekly, or monthly. Make sure to save the settings to activate the transfers.\",\\n     \"You can arrange automatic transfers by going to \\'Account Settings\\', choosing \\'Automatic Transfers\\', and setting the desired amount and frequency. Don\\'t forget to save the changes to enable the automatic transfers.\",\\n     \"incorrect\", \"omits information (limited frequency of transfers available)\"],\\n  \\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the emai',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '],\\n  \\n    [\"Hey, can I set up automatic transfers to my savings account?\",\\n     \"Yes, you can set up automatic transfers by going to \\'Account Settings\\', selecting \\'Automatic Transfers\\', and specifying the amount and frequency. You can choose to transfer weekly, bi-weekly, or monthly. Make sure to save the settings to activate the transfers.\",\\n     \"You can arrange automatic transfers by going to \\'Account Settings\\', choosing \\'Automatic Transfers\\', and setting the desired amount and frequency. Don\\'t forget to save the changes to enable the automatic transfers.\",\\n     \"incorrect\", \"omits information (limited frequency of transfers available)\"],\\n  \\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How can I update my billing address?\",\\n     \"To update your billing address, log into your account, go to \\'Account Settings\\', select \\'Billing Information\\', and enter your new address. Make sure to save the changes once you are done.\",\\n     \"To update your billing address, log into your account, navigate to \\'Account Settings\\', and select \\'Billing Information\\'. Enter your new address and ensure all fields are filled out correctly. Save the changes, and you will receive a confirmation email with the updated address details.\",\\n     \"incorrect\", \"adds new information (confirmation email)\"],\\n  \\n    [\"How do I contact customer support?\",\\n     \"You can contact customer support by logging into your account, going to the \\'Help\\' section, and selecting \\'Contact Us\\'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\\n     \"To contact customer support, log into your account and go to the \\'Help\\' section. Sel',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'l, check your spam folder.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How can I update my billing address?\",\\n     \"To update your billing address, log into your account, go to \\'Account Settings\\', select \\'Billing Information\\', and enter your new address. Make sure to save the changes once you are done.\",\\n     \"To update your billing address, log into your account, navigate to \\'Account Settings\\', and select \\'Billing Information\\'. Enter your new address and ensure all fields are filled out correctly. Save the changes, and you will receive a confirmation email with the updated address details.\",\\n     \"incorrect\", \"adds new information (confirmation email)\"],\\n  \\n    [\"How do I contact customer support?\",\\n     \"You can contact customer support by logging into your account, going to the \\'Help\\' section, and selecting \\'Contact Us\\'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\\n     \"To contact customer support, log into your account and go to the \\'Help\\' section. Select \\'Contact Us\\' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\\n     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\\n  \\n    [\"What should I do if my card is lost or stolen?\",\\n     \"If your card is lost or stolen, immediately log into your account, go to \\'Card Management\\', and select \\'Report Lost/Stolen\\'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\\n     \"If your card is lost or stolen, navigate to \\'Card Management\\' in your account, and select \\'Report Lost/Stolen\\'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I enable two-factor authentication (2FA)?\",\\n     \"To enable two-factor auth',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'ect \\'Contact Us\\' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\\n     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\\n  \\n    [\"What should I do if my card is lost or stolen?\",\\n     \"If your card is lost or stolen, immediately log into your account, go to \\'Card Management\\', and select \\'Report Lost/Stolen\\'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\\n     \"If your card is lost or stolen, navigate to \\'Card Management\\' in your account, and select \\'Report Lost/Stolen\\'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I enable two-factor authentication (2FA)?\",\\n     \"To enable two-factor authentication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Cr',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': 'entication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nN',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': 'eate an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': 'ow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can ',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': ' REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': 'view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': '\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s ho',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': 'me(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': 'w to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically tr',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'docs-main/examples/LLM_judge.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'This evaluation approach uses multiple LLMs to evaluate the same output. You can do this to obtain an aggregate evaluation result — e.g., consider an output a \"pass\" only if all or the majority of LLMs approve — or to explicitly surface disagreements.\\n\\nBlog explaining the concept of LLM jury: https://www.evidentlyai.com/blog/llm-judges-jury .\\n\\nCode example as a Jupyter notebook: https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_jury_Example.ipynb\\n\\n## Preparation\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently litellm \\n```\\n\\n(Or install `evidently[llm]`.)\\n\\nImport the components you\\'ll use:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import eq, is_in, not_in\\nfrom evidently.descriptors import LLMEval, TestSummary, ColumnTest\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\nfrom evidently.core.datasets import DatasetColumn\\nfrom evidently.descriptors import CustomColumnDescriptor\\n\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n## Step 1: Set up evaluator LLMs\\n\\nPass the API keys for the LLMs you\\'ll use as judges.\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"GEMINI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  You can use any other LLMs, including self-hosted ones. Check the [docs on LLM judges](/metrics/customize_llm_judge).\\n</Info>\\n\\n**Optional**. Set up Evidently Cloud workspace:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a project to store your evaluation results:\\n\\n```python\\nproject = ws.create_project(\"LLM jury\", org_id=\"YOUR ORG ID\")\\nproject.description = \"Evals with different LLMs\"\\n\\n# or project = ws.get_project(\"PROJECT_ID\")\\n```\\n\\n## Step 1: Toy Data\\n\\nLet\\'s define a small dataset of user intents and generated emails.\\n\\nThis dataset simulates ',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'datasets import DatasetColumn\\nfrom evidently.descriptors import CustomColumnDescriptor\\n\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n## Step 1: Set up evaluator LLMs\\n\\nPass the API keys for the LLMs you\\'ll use as judges.\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"GEMINI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  You can use any other LLMs, including self-hosted ones. Check the [docs on LLM judges](/metrics/customize_llm_judge).\\n</Info>\\n\\n**Optional**. Set up Evidently Cloud workspace:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a project to store your evaluation results:\\n\\n```python\\nproject = ws.create_project(\"LLM jury\", org_id=\"YOUR ORG ID\")\\nproject.description = \"Evals with different LLMs\"\\n\\n# or project = ws.get_project(\"PROJECT_ID\")\\n```\\n\\n## Step 1: Toy Data\\n\\nLet\\'s define a small dataset of user intents and generated emails.\\n\\nThis dataset simulates user instructions for an email generation tool and the corresponding model outputs. We’ll evaluate whether the tone of the generated emails is appropriate using a panel of LLM judges.\\n\\n```python\\ndata = [\\n    [\"don’t want to attend, say no\", \"Hey,\\\\nGonna skip the meeting tomorrow — nothing new for me there. Loop me in if anything big happens.\\\\n— A\"],\\n    [\"write a fluffy sales email that\\'ll convert well\", \"Hi Rachel,\\\\nI just have to say — your company is revolutionizing the industry, and it’s genuinely inspiring. We\\'d love to be part of your visionary journey. Let’s hop on a call!\"],\\n    [\"make it so they feel like they HAVE to reply\", \"Hi David,\\\\nI\\'ve reached out a couple of times and still haven\\'t heard back — I assume something must be wrong. If I don\\'t hear from you this week, I’ll have to assume this isn\\'t a priority.\"],\\n    [\"we have an outage idk when we resolve it\", \"Well, the server decided to die again. Congrats to anyone who bet on a Wednesday failure. ETA for resurrection: T',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'user instructions for an email generation tool and the corresponding model outputs. We’ll evaluate whether the tone of the generated emails is appropriate using a panel of LLM judges.\\n\\n```python\\ndata = [\\n    [\"don’t want to attend, say no\", \"Hey,\\\\nGonna skip the meeting tomorrow — nothing new for me there. Loop me in if anything big happens.\\\\n— A\"],\\n    [\"write a fluffy sales email that\\'ll convert well\", \"Hi Rachel,\\\\nI just have to say — your company is revolutionizing the industry, and it’s genuinely inspiring. We\\'d love to be part of your visionary journey. Let’s hop on a call!\"],\\n    [\"make it so they feel like they HAVE to reply\", \"Hi David,\\\\nI\\'ve reached out a couple of times and still haven\\'t heard back — I assume something must be wrong. If I don\\'t hear from you this week, I’ll have to assume this isn\\'t a priority.\"],\\n    [\"we have an outage idk when we resolve it\", \"Well, the server decided to die again. Congrats to anyone who bet on a Wednesday failure. ETA for resurrection: TBD\"],\\n    [\"send a price email that makes them feel it’s stupid not to buy\", \"See the pricing attached. Only a fool would pass this up — we’re slashing prices for 24 hours only. You won’t see this again, so don’t wait. Most of your competitors already upgraded to Pro\"],\\n    [\"say the bug’s not on our side but politely\", \"After checking the logs, we couldn’t find any issues on our end. You might want to double-check your setup — maybe something’s off in your environment?\"],\\n    [\"send a friendly meeting reminder\", \"Hi all! Just a reminder that we’re meeting tomorrow at 3PM! Looking forward to it!\"],\\n    [\"set up a meeting but make it chill\", \"Hey, just wondering if you might be free sometime next week to chat? Totally cool if not.\"],\\n    [\"write a status update, casual\", \"Hi all! Working on the UI bug 🐞 — should have it fixed by EOD 🙏\"],\\n    [\"update we ship today\", \"All good on our side — we’re shipping v2 today. Cheers!\"],\\n    [\"thanks for demo say it\\'s awesome for a vp\", \"Hey! Really',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'BD\"],\\n    [\"send a price email that makes them feel it’s stupid not to buy\", \"See the pricing attached. Only a fool would pass this up — we’re slashing prices for 24 hours only. You won’t see this again, so don’t wait. Most of your competitors already upgraded to Pro\"],\\n    [\"say the bug’s not on our side but politely\", \"After checking the logs, we couldn’t find any issues on our end. You might want to double-check your setup — maybe something’s off in your environment?\"],\\n    [\"send a friendly meeting reminder\", \"Hi all! Just a reminder that we’re meeting tomorrow at 3PM! Looking forward to it!\"],\\n    [\"set up a meeting but make it chill\", \"Hey, just wondering if you might be free sometime next week to chat? Totally cool if not.\"],\\n    [\"write a status update, casual\", \"Hi all! Working on the UI bug 🐞 — should have it fixed by EOD 🙏\"],\\n    [\"update we ship today\", \"All good on our side — we’re shipping v2 today. Cheers!\"],\\n    [\"thanks for demo say it\\'s awesome for a vp\", \"Hey! Really appreciated the walkthrough. Cool to see a VP getting into the weeds like that\"],\\n    [\"sending a rough draft\", \"Here’s a rough first draft — not sure it’s any good but hopefully it’s a start.\"],\\n    [\"don’t want to attend, say no\", \"Hi Sam,\\\\nThanks for the invite. I won’t be able to join the meeting tomorrow, but I’ll review the notes afterward and follow up with any questions.\"],\\n    [\"ask if the want to see the new platform demo\", \"Hi Rachel,\\\\nI’m reaching out to introduce our latest platform update — designed to streamline onboarding and improve conversion by up to 25%.\\\\nI’d love to show you a quick demo if you\\'re interested. Let me know what works for your schedule.\\\\nBest regards,\"],\\n    [\"follow up politely second time\", \"Hi David,\\\\nJust checking in on the proposal I sent last week — let me know if you had a chance to review, or if any questions came up. Happy to help clarify.\\\\nWarm regards,\"]\\n]\\n\\ncolumns = [\"user input\", \"generated email\"]\\n\\neval_df = pd.DataFrame(data, columns=c',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': ' appreciated the walkthrough. Cool to see a VP getting into the weeds like that\"],\\n    [\"sending a rough draft\", \"Here’s a rough first draft — not sure it’s any good but hopefully it’s a start.\"],\\n    [\"don’t want to attend, say no\", \"Hi Sam,\\\\nThanks for the invite. I won’t be able to join the meeting tomorrow, but I’ll review the notes afterward and follow up with any questions.\"],\\n    [\"ask if the want to see the new platform demo\", \"Hi Rachel,\\\\nI’m reaching out to introduce our latest platform update — designed to streamline onboarding and improve conversion by up to 25%.\\\\nI’d love to show you a quick demo if you\\'re interested. Let me know what works for your schedule.\\\\nBest regards,\"],\\n    [\"follow up politely second time\", \"Hi David,\\\\nJust checking in on the proposal I sent last week — let me know if you had a chance to review, or if any questions came up. Happy to help clarify.\\\\nWarm regards,\"]\\n]\\n\\ncolumns = [\"user input\", \"generated email\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n```\\n\\n## Step 2: Define the Evaluation Prompt\\n\\nUse `BinaryClassificationPromptTemplate` to define what the LLM is judging.\\n\\n```python\\nus_corp_email_appropriateness = BinaryClassificationPromptTemplate(\\n    pre_messages=[\\n        (\"system\", \"\"\"You are an expert in U.S. corporate and workplace communication in tech companies.\\n        You will be shown a snippet of an email generated by the assistant.\\n        Your task is to judge whether the text would be considered *appropriate* for email communication.\\n        \"\"\")\\n    ],\\n    criteria=\"\"\"An APPROPRIATE email text is one that would be acceptable in real-world professional email communication.\\n    An INAPPROPRIATE email text includes tone, language, or content that would be questionable or unacceptable.\\n\\n    Focus only on whether the tone, style, and content are suitable. Do not penalize the text for being incomplete — it may be a snippet or excerpt.\\n    \"\"\",\\n    target_category=\"APPROPRIATE\",\\n    non_target_category=\"INAPPROPRIAT',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'olumns)\\n```\\n\\n## Step 2: Define the Evaluation Prompt\\n\\nUse `BinaryClassificationPromptTemplate` to define what the LLM is judging.\\n\\n```python\\nus_corp_email_appropriateness = BinaryClassificationPromptTemplate(\\n    pre_messages=[\\n        (\"system\", \"\"\"You are an expert in U.S. corporate and workplace communication in tech companies.\\n        You will be shown a snippet of an email generated by the assistant.\\n        Your task is to judge whether the text would be considered *appropriate* for email communication.\\n        \"\"\")\\n    ],\\n    criteria=\"\"\"An APPROPRIATE email text is one that would be acceptable in real-world professional email communication.\\n    An INAPPROPRIATE email text includes tone, language, or content that would be questionable or unacceptable.\\n\\n    Focus only on whether the tone, style, and content are suitable. Do not penalize the text for being incomplete — it may be a snippet or excerpt.\\n    \"\"\",\\n    target_category=\"APPROPRIATE\",\\n    non_target_category=\"INAPPROPRIATE\",\\n    include_reasoning=True,\\n)\\n```\\n\\n## Step 3: Create a panel of LLM judges\\n\\nWe\\'ll create evaluators from multiple LLM providers using the same evaluation prompt. The code below scores the \"generated email\" column using three different judges.\\n\\nEach judge includes a Pass condition that returns `True` if the email tone is considered \"appropriate\" by this judge.\\n\\nWe also add a `TestSummary` for each row to compute:\\n\\n- A final success check (`true` if all three models approve),\\n- A total count / share of approvals by judges.\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"openai\", model=\"gpt-4o-mini\",\\n                alias=\"OpenAI_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"OpenAI_judge_US\", alias=\"GPT approves\")]),\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                p',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'E\",\\n    include_reasoning=True,\\n)\\n```\\n\\n## Step 3: Create a panel of LLM judges\\n\\nWe\\'ll create evaluators from multiple LLM providers using the same evaluation prompt. The code below scores the \"generated email\" column using three different judges.\\n\\nEach judge includes a Pass condition that returns `True` if the email tone is considered \"appropriate\" by this judge.\\n\\nWe also add a `TestSummary` for each row to compute:\\n\\n- A final success check (`true` if all three models approve),\\n- A total count / share of approvals by judges.\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"openai\", model=\"gpt-4o-mini\",\\n                alias=\"OpenAI_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"OpenAI_judge_US\", alias=\"GPT approves\")]),\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"anthropic\", model=\"claude-3-5-haiku-20241022\",\\n                alias=\"Anthropic_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"Anthropic_judge_US\", alias=\"Claude approves\")]),\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"gemini\", model=\"gemini/gemini-2.0-flash-lite\",\\n                alias=\"Gemini_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"Gemini_judge_US\", alias=\"Gemini approves\")]),\\n        TestSummary(success_all=True, success_count=True, success_rate=True, alias=\"Approve\"),\\n])\\n```\\n\\n<Info>\\n  Need help with understanding the API?\\n\\n  - Check the docs on [LLM judges](/metrics/customize_llm_judge).\\n  - Check the docs on [descriptor tests](/docs/library/descriptors#adding-descriptor-tests).\\n</Info>\\n\\nTo explicitly flag disagreements among LLMs, let’s add a custom descriptor. It will return \"DISAGREE\" if the success rate is not 0 or 1 (i.e., not unanimously rejected or approved).\\n\\n```python\\n# Defi',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'rovider=\"anthropic\", model=\"claude-3-5-haiku-20241022\",\\n                alias=\"Anthropic_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"Anthropic_judge_US\", alias=\"Claude approves\")]),\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"gemini\", model=\"gemini/gemini-2.0-flash-lite\",\\n                alias=\"Gemini_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"Gemini_judge_US\", alias=\"Gemini approves\")]),\\n        TestSummary(success_all=True, success_count=True, success_rate=True, alias=\"Approve\"),\\n])\\n```\\n\\n<Info>\\n  Need help with understanding the API?\\n\\n  - Check the docs on [LLM judges](/metrics/customize_llm_judge).\\n  - Check the docs on [descriptor tests](/docs/library/descriptors#adding-descriptor-tests).\\n</Info>\\n\\nTo explicitly flag disagreements among LLMs, let’s add a custom descriptor. It will return \"DISAGREE\" if the success rate is not 0 or 1 (i.e., not unanimously rejected or approved).\\n\\n```python\\n# Define the descriptor\\ndef judges_disagree(data: DatasetColumn) -> DatasetColumn:\\n    return DatasetColumn(\\n        type=\"cat\",\\n        data=pd.Series([\\n            \"DISAGREE\" if val not in [0.0, 1.0] else \"AGREE\"\\n            for val in data.data]))\\n\\n# Add it to the dataset\\nllm_evals.add_descriptors(descriptors=[\\n    CustomColumnDescriptor(\"Approve_success_rate\", judges_disagree, alias=\"Do LLMs disagree?\"),\\n])            \\n```\\n\\n## Step 4. Run and view the report\\n\\nTo explore results locally, export them to a DataFrame:\\n\\n```python\\nllm_evals.as_dataframe()\\n```\\n\\nTo get a summary report with overall metrics (such as the share of approved emails and disagreements), run:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(llm_evals, None)\\n```\\n\\nTo upload results to Evidently Cloud for ease of exploration:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nOr to view locally:\\n\\n```python\\nmy_eval\\n# my_eval.json()\\n# my_eval.dict()\\n# my_eval.save_html(\"report.html\")\\n```\\n',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'ne the descriptor\\ndef judges_disagree(data: DatasetColumn) -> DatasetColumn:\\n    return DatasetColumn(\\n        type=\"cat\",\\n        data=pd.Series([\\n            \"DISAGREE\" if val not in [0.0, 1.0] else \"AGREE\"\\n            for val in data.data]))\\n\\n# Add it to the dataset\\nllm_evals.add_descriptors(descriptors=[\\n    CustomColumnDescriptor(\"Approve_success_rate\", judges_disagree, alias=\"Do LLMs disagree?\"),\\n])            \\n```\\n\\n## Step 4. Run and view the report\\n\\nTo explore results locally, export them to a DataFrame:\\n\\n```python\\nllm_evals.as_dataframe()\\n```\\n\\nTo get a summary report with overall metrics (such as the share of approved emails and disagreements), run:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(llm_evals, None)\\n```\\n\\nTo upload results to Evidently Cloud for ease of exploration:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nOr to view locally:\\n\\n```python\\nmy_eval\\n# my_eval.json()\\n# my_eval.dict()\\n# my_eval.save_html(\"report.html\")\\n```\\n\\nHere’s a preview of the results. 5 emails received mixed judgments from the LLMs:\\n\\n![](/images/examples/llm_jury_overview.png)\\n\\nYou can filter and inspect individual examples with selectors:\\n\\n![](/images/examples/llm_jury_example.png)',\n",
       "  'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'filename': 'docs-main/examples/LLM_jury.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"import CloudSignup from '/snippets/cloud_signup.mdx';\\nimport CreateProject from '/snippets/create_project.mdx';\\n\\nIn this tutorial, we'll demonstrate how to evaluate different aspects of Retrieval-Augmented Generation (RAG) using Evidently.\\n\\n<Info>\\nWe’ll demonstrate a **local open-source workflow**, viewing results as a pandas dataframe and a visual report — ideal for Jupyter or Colab. At the end, we also show how to upload results to the Evidently Platform. If you are in a non-interactive Python environment, choose this option.\\n</Info>\\n\\nWe will evaluate both retrieval and generation quality:\\n\\n* **Retrieval.** Assessing the quality of retrieved contexts, including per-chunk relevance.\\n\\n* **Generation.** Evaluating the quality of the final response, both with and without ground truth.\\n\\nBy the end of this tutorial, you'll know how to evaluate different aspects of a RAG system, and generate structured reports to track RAG performance.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/rag_metrics.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/rag_metrics.ipynb).\\n</Info>\\n\\n<Note>\\n  To simplify things, we won't create an actual RAG app, but will simulate getting scored outputs. If you want to see an example where we also create a RAG system, check this [video tutorial](https://www.youtube.com/watch?v=jckp5R09Afg&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=10).\\n</Note>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\n!pip install evidently[llm]\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently.descriptors import *\\n\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\nPass your OpenAI key \",\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'ps://github.com/evidentlyai/community-examples/blob/main/tutorials/rag_metrics.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/rag_metrics.ipynb).\\n</Info>\\n\\n<Note>\\n  To simplify things, we won\\'t create an actual RAG app, but will simulate getting scored outputs. If you want to see an example where we also create a RAG system, check this [video tutorial](https://www.youtube.com/watch?v=jckp5R09Afg&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=10).\\n</Note>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\n!pip install evidently[llm]\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently.descriptors import *\\n\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n## 2. Evaluating Retrieval\\n\\n### Single Context\\n\\nFirst, let\\'s test retrieval quality when a single context is retrieved for each query.\\n\\n**Generate a synthetic dataset**. We create a simple dataset with questions, retrieved contexts, and generated responses.\\n\\n```python\\nsynthetic_data = [\\n\\n    [\"Why do flowers bloom in spring?\",\\n     \"Plants require extra care during cold months. You should keep them indoors.\",\\n     \"because of the rising temperatures\"],\\n\\n    [\"Why do we yawn when we see someone else yawn?\",\\n     \"Yawning is contagious due to social bonding and mirror neurons in our brains that trigger the response when we see others yawn.\",\\n     \"because it\\'s a glitch in the matrix\"],\\n\\n    [\"How far is Saturn from Earth?\",\\n     \"The distance between Earth and Saturn varies, but on average, Saturn is about 1.4 billion kilometers (886 million miles) away from Earth.\",\\n     \"about 1.4 billion ki',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n## 2. Evaluating Retrieval\\n\\n### Single Context\\n\\nFirst, let\\'s test retrieval quality when a single context is retrieved for each query.\\n\\n**Generate a synthetic dataset**. We create a simple dataset with questions, retrieved contexts, and generated responses.\\n\\n```python\\nsynthetic_data = [\\n\\n    [\"Why do flowers bloom in spring?\",\\n     \"Plants require extra care during cold months. You should keep them indoors.\",\\n     \"because of the rising temperatures\"],\\n\\n    [\"Why do we yawn when we see someone else yawn?\",\\n     \"Yawning is contagious due to social bonding and mirror neurons in our brains that trigger the response when we see others yawn.\",\\n     \"because it\\'s a glitch in the matrix\"],\\n\\n    [\"How far is Saturn from Earth?\",\\n     \"The distance between Earth and Saturn varies, but on average, Saturn is about 1.4 billion kilometers (886 million miles) away from Earth.\",\\n     \"about 1.4 billion kilometers\"],\\n\\n    [\"Where do penguins live?\",\\n     \"Penguins primarily live in the Southern Hemisphere, with most species found in Antarctica, as well as on islands and coastlines of South America, Africa, Australia, and New Zealand.\",\\n     \"mostly in Antarctica and southern regions\"],\\n]\\n\\ncolumns = [\"Question\", \"Context\", \"Response\"]\\nsynthetic_df = pd.DataFrame(synthetic_data, columns=columns)\\n```\\n\\nTo be able to preview a full-with pandas dataset.\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\n```\\n\\n**Evaluate overall context quality.** We first assess whether the retrieved context provides sufficient information to answer the question and view results as a pandas dataframe.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextQualityLLMEval(\"Context\", question=\"Question\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nWhat happened in this code:\\n\\n* We crea',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'lometers\"],\\n\\n    [\"Where do penguins live?\",\\n     \"Penguins primarily live in the Southern Hemisphere, with most species found in Antarctica, as well as on islands and coastlines of South America, Africa, Australia, and New Zealand.\",\\n     \"mostly in Antarctica and southern regions\"],\\n]\\n\\ncolumns = [\"Question\", \"Context\", \"Response\"]\\nsynthetic_df = pd.DataFrame(synthetic_data, columns=columns)\\n```\\n\\nTo be able to preview a full-with pandas dataset.\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\n```\\n\\n**Evaluate overall context quality.** We first assess whether the retrieved context provides sufficient information to answer the question and view results as a pandas dataframe.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextQualityLLMEval(\"Context\", question=\"Question\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nWhat happened in this code:\\n\\n* We create an [Evidently dataset object](/docs/library/data_definition).\\n\\n* Simultaneously, we add [descriptors](/docs/library/descriptors): evaluators that score each row.\\n\\n* We use a built-in LLM judge metric `ContextQualityLLMEval`.\\n\\n<Info>\\n  You can also choose a different evaluator LLM or modify the prompt. See [LLM judge parameters](/metrics/customize_llm_judge).\\n</Info>\\n\\nHere is what you get:\\n\\n![](/images/examples/rag_single_context_valid-min.png)\\n\\n**Evaluate chunk relevance**. You can also score the relevance of the chunk using a different `ContextRelevance` metric.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextRelevance(\"Question\", \"Context\", \\n                                  output_scores=True, \\n                                  aggregation_method=\"hit\", \\n                                  method=\"llm\", \\n                                  alias=\"Hit\")',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'te an [Evidently dataset object](/docs/library/data_definition).\\n\\n* Simultaneously, we add [descriptors](/docs/library/descriptors): evaluators that score each row.\\n\\n* We use a built-in LLM judge metric `ContextQualityLLMEval`.\\n\\n<Info>\\n  You can also choose a different evaluator LLM or modify the prompt. See [LLM judge parameters](/metrics/customize_llm_judge).\\n</Info>\\n\\nHere is what you get:\\n\\n![](/images/examples/rag_single_context_valid-min.png)\\n\\n**Evaluate chunk relevance**. You can also score the relevance of the chunk using a different `ContextRelevance` metric.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextRelevance(\"Question\", \"Context\", \\n                                  output_scores=True, \\n                                  aggregation_method=\"hit\", \\n                                  method=\"llm\", \\n                                  alias=\"Hit\")]\\n    )\\ncontext_based_evals.as_dataframe()\\n```\\n\\nIn this case you will get a binary \"Hit\" on whether the context is relevant or not.\\n\\n![](/images/examples/rag_single_context_hit-min.png)\\n\\nIt\\'s more useful for multiple context, though.\\n\\n### Multiple Contexts\\n\\nRAG systems often retrieve multiple chunks. In this case, we can assess the relevance of each individual chunk first.\\n\\nLet\\'s generate a toy dataset. Pass all contexts as a list.\\n\\n```python\\nsynthetic_data = [\\n    [\"Why are bananas healthy?\", [\"Bananas are rich in potassium.\", \"Bananas provide quick energy.\", \"Are bananas actually a vegetable?\"], \"because they are rich in nutrients\"],\\n    [\"How do you cook potatoes?\", [\"Potatoes are easy to grow.\", \"The best way to cook potatoes is to eat them raw.\", \"Can potatoes be cooked in space?\"], \"boil, bake, or fry them\"]\\n]\\ncolumns = [\"Question\", \"Context\", \"Response\"]\\nsynthetic_df_2 = pd.DataFrame(synthetic_data, columns=columns)\\n```\\n\\n**Hit Rate**. To aggregate the results per query, we can a',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': ']\\n    )\\ncontext_based_evals.as_dataframe()\\n```\\n\\nIn this case you will get a binary \"Hit\" on whether the context is relevant or not.\\n\\n![](/images/examples/rag_single_context_hit-min.png)\\n\\nIt\\'s more useful for multiple context, though.\\n\\n### Multiple Contexts\\n\\nRAG systems often retrieve multiple chunks. In this case, we can assess the relevance of each individual chunk first.\\n\\nLet\\'s generate a toy dataset. Pass all contexts as a list.\\n\\n```python\\nsynthetic_data = [\\n    [\"Why are bananas healthy?\", [\"Bananas are rich in potassium.\", \"Bananas provide quick energy.\", \"Are bananas actually a vegetable?\"], \"because they are rich in nutrients\"],\\n    [\"How do you cook potatoes?\", [\"Potatoes are easy to grow.\", \"The best way to cook potatoes is to eat them raw.\", \"Can potatoes be cooked in space?\"], \"boil, bake, or fry them\"]\\n]\\ncolumns = [\"Question\", \"Context\", \"Response\"]\\nsynthetic_df_2 = pd.DataFrame(synthetic_data, columns=columns)\\n```\\n\\n**Hit Rate**. To aggregate the results per query, we can assess if at least one retrieved chunk contains relevant information (Hit).\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df_2,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextRelevance(\"Question\", \"Context\", \\n                                  output_scores=True, \\n                                  aggregation_method=\"hit\", \\n                                  method=\"llm\", \\n                                  alias=\"Hit\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nYou can see the list of individual relevance scores that appear in the same order as your chunks.\\n\\n![](/images/examples/rag_multi_context_hit-min.png)\\n\\n**Mean Relevance.** Alternatively, you can compute an average relevance score.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df_2,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextRelevance(\"Question\", \"Context\", \\n   ',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'ssess if at least one retrieved chunk contains relevant information (Hit).\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df_2,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextRelevance(\"Question\", \"Context\", \\n                                  output_scores=True, \\n                                  aggregation_method=\"hit\", \\n                                  method=\"llm\", \\n                                  alias=\"Hit\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nYou can see the list of individual relevance scores that appear in the same order as your chunks.\\n\\n![](/images/examples/rag_multi_context_hit-min.png)\\n\\n**Mean Relevance.** Alternatively, you can compute an average relevance score.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df_2,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[ContextRelevance(\"Question\", \"Context\", \\n                                  output_scores=True, \\n                                  aggregation_method=\"mean\", \\n                                  method=\"llm\", \\n                                  alias=\"Relevance\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nHere is an example result:\\n\\n![](/images/examples/rag_multi_context_mean-min.png)\\n\\n## 3. Evaluating Generation\\n\\n### With Ground Truth\\n\\nIf you a have ground truth dataset for RAG, you can compare the generated responses against known correct answers.\\n\\n<Note>\\n  **Synthetic data**. You can generate a ground truth dataset for your RAG using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\nLet\\'s generate a new toy example with \"target\" column:\\n\\n```python\\nsynthetic_data = [\\n    [\"Why do we yawn?\", \"because it\\'s a glitch in the matrix\", \"Due to mirror neurons.\"],\\n    [\"Why do flowers bloom?\", \"Because of rising temperatures\", \"Because it gets warmer.\"]\\n]\\ncolumns = [\"Question\", \"Response\", \"Target\"]\\nsynthetic_df_3 = pd.DataF',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': '                               output_scores=True, \\n                                  aggregation_method=\"mean\", \\n                                  method=\"llm\", \\n                                  alias=\"Relevance\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nHere is an example result:\\n\\n![](/images/examples/rag_multi_context_mean-min.png)\\n\\n## 3. Evaluating Generation\\n\\n### With Ground Truth\\n\\nIf you a have ground truth dataset for RAG, you can compare the generated responses against known correct answers.\\n\\n<Note>\\n  **Synthetic data**. You can generate a ground truth dataset for your RAG using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\nLet\\'s generate a new toy example with \"target\" column:\\n\\n```python\\nsynthetic_data = [\\n    [\"Why do we yawn?\", \"because it\\'s a glitch in the matrix\", \"Due to mirror neurons.\"],\\n    [\"Why do flowers bloom?\", \"Because of rising temperatures\", \"Because it gets warmer.\"]\\n]\\ncolumns = [\"Question\", \"Response\", \"Target\"]\\nsynthetic_df_3 = pd.DataFrame(synthetic_data, columns=columns)\\n```\\n\\nThere are multiple ways to run this comparison, including LLM-based matching (`CorrectnessLLMEval`) and non-LLM methods like Semantic similarity and BERTScore. Let\\'s run all three at once, but we\\'d recommend choosing the one:\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df_3,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Response\", \"Target\"]),\\n    descriptors=[\\n        CorrectnessLLMEval(\"Response\", target_output=\"Target\"),\\n        BERTScore(columns=[\"Response\", \"Target\"], alias=\"BERTScore\"),\\n        SemanticSimilarity(columns=[\"Response\", \"Target\"], alias=\"Semantic Similarity\")\\n    ]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nHere is what you get:\\n\\n![](/images/examples/rag_correctness-min.png)\\n\\n<Info>\\n  **Editing the LLM prompt**. You can tweak the definition of correctness to your own liking. Here is an example tutorial on how we tune [a correctness descriptor prompt](/examples/LLM_judge).\\n</Info>\\n\\n### W',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'rame(synthetic_data, columns=columns)\\n```\\n\\nThere are multiple ways to run this comparison, including LLM-based matching (`CorrectnessLLMEval`) and non-LLM methods like Semantic similarity and BERTScore. Let\\'s run all three at once, but we\\'d recommend choosing the one:\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df_3,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Response\", \"Target\"]),\\n    descriptors=[\\n        CorrectnessLLMEval(\"Response\", target_output=\"Target\"),\\n        BERTScore(columns=[\"Response\", \"Target\"], alias=\"BERTScore\"),\\n        SemanticSimilarity(columns=[\"Response\", \"Target\"], alias=\"Semantic Similarity\")\\n    ]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nHere is what you get:\\n\\n![](/images/examples/rag_correctness-min.png)\\n\\n<Info>\\n  **Editing the LLM prompt**. You can tweak the definition of correctness to your own liking. Here is an example tutorial on how we tune [a correctness descriptor prompt](/examples/LLM_judge).\\n</Info>\\n\\n### Without Ground Truth\\n\\nIf you don\\'t have reference answers, you can use reference-free LLM judges to assess response quality. For example, here is you how can run evaluation for `Faithfulness` to detect if the response is contradictory or unfaithful to the context:\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[FaithfulnessLLMEval(\"Response\", context=\"Context\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nHere is an example result:\\n\\n![](/images/examples/rag_faithfulness-min.png)\\n\\nYou can add other useful checks over your final response like:\\n\\n* Length constraints: are responses within expected limits?\\n\\n* Refusal rate: monitoring how often the system declines questions.\\n\\n* String matching: checking for required wording (e.g., disclaimers).\\n\\n* Response tone: ensuring responses match the intended style.\\n\\n<Note>\\n  **Available evaluators**. Check a full [list of availa',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'ithout Ground Truth\\n\\nIf you don\\'t have reference answers, you can use reference-free LLM judges to assess response quality. For example, here is you how can run evaluation for `Faithfulness` to detect if the response is contradictory or unfaithful to the context:\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(text_columns=[\"Question\", \"Context\", \"Response\"]),\\n    descriptors=[FaithfulnessLLMEval(\"Response\", context=\"Context\")]\\n)\\ncontext_based_evals.as_dataframe()\\n```\\n\\nHere is an example result:\\n\\n![](/images/examples/rag_faithfulness-min.png)\\n\\nYou can add other useful checks over your final response like:\\n\\n* Length constraints: are responses within expected limits?\\n\\n* Refusal rate: monitoring how often the system declines questions.\\n\\n* String matching: checking for required wording (e.g., disclaimers).\\n\\n* Response tone: ensuring responses match the intended style.\\n\\n<Note>\\n  **Available evaluators**. Check a full [list of available descriptors](/metrics/all_descriptors).\\n</Note>\\n\\n## 4. Get Reports\\n\\nOnce you have defined what you are evaluating, you can group all your evals in a **Report** to summarize the results across multiple tested inputs.\\n\\nLet\\'s put it all together.\\n\\n**Score data**. Once you have a pandas dataframe `synthetic_df`, you create an Evidently dataset object and choose the selected descriptors by simply listing them.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(\\n        text_columns=[\"Question\", \"Context\", \"Response\"],\\n    ),\\n    descriptors=[\\n        FaithfulnessLLMEval(\"Response\", context=\"Context\"),\\n        ContextQualityLLMEval(\"Context\", question=\"Question\"),\\n    ]\\n)\\n# context_based_evals.as_dataframe()\\n```\\n\\n**Get a Report**. Instead of rendering the results as a dataframe, you create a [Report](/docs/library/report).\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(context_based_evals, None)\\nmy_eval\\n```\\n\\nT',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'ble descriptors](/metrics/all_descriptors).\\n</Note>\\n\\n## 4. Get Reports\\n\\nOnce you have defined what you are evaluating, you can group all your evals in a **Report** to summarize the results across multiple tested inputs.\\n\\nLet\\'s put it all together.\\n\\n**Score data**. Once you have a pandas dataframe `synthetic_df`, you create an Evidently dataset object and choose the selected descriptors by simply listing them.\\n\\n```python\\ncontext_based_evals = Dataset.from_pandas(\\n    synthetic_df,\\n    data_definition=DataDefinition(\\n        text_columns=[\"Question\", \"Context\", \"Response\"],\\n    ),\\n    descriptors=[\\n        FaithfulnessLLMEval(\"Response\", context=\"Context\"),\\n        ContextQualityLLMEval(\"Context\", question=\"Question\"),\\n    ]\\n)\\n# context_based_evals.as_dataframe()\\n```\\n\\n**Get a Report**. Instead of rendering the results as a dataframe, you create a [Report](/docs/library/report).\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(context_based_evals, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/rag_reports-min.png)\\n\\nThis lets you see a well-rounded evaluation. In this toy example, we can see that the system generally retrieves the right data well but struggles with generation. The next step could be improving your prompt to ensure responses stay true to context.\\n\\n**Add test conditions**. You can also set up explicit pass/fail tests based on expected score distributions using the [Tests](/docs/library/tests). These are conditional expectations you add to metrics.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    CategoryCount(column=\"Faithfulness\", category=\"UNFAITHFUL\", tests=[eq(0)]),\\n    CategoryCount(column=\"ContextQuality\", category=\"INVALID\", tests=[eq(0)])\\n])\\n\\nmy_eval = report.run(context_based_evals, None)\\nmy_eval\\n```\\n\\nIn this case, we expect all retrieved contexts to be valid and a',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'his will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/rag_reports-min.png)\\n\\nThis lets you see a well-rounded evaluation. In this toy example, we can see that the system generally retrieves the right data well but struggles with generation. The next step could be improving your prompt to ensure responses stay true to context.\\n\\n**Add test conditions**. You can also set up explicit pass/fail tests based on expected score distributions using the [Tests](/docs/library/tests). These are conditional expectations you add to metrics.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    CategoryCount(column=\"Faithfulness\", category=\"UNFAITHFUL\", tests=[eq(0)]),\\n    CategoryCount(column=\"ContextQuality\", category=\"INVALID\", tests=[eq(0)])\\n])\\n\\nmy_eval = report.run(context_based_evals, None)\\nmy_eval\\n```\\n\\nIn this case, we expect all retrieved contexts to be valid and all responses to be faithful, so our tests fail. You can adjust these conditions — for example, allowing a certain percentage of responses to fail.\\n\\n![](/images/examples/rag_tests-min.png)\\n\\n## 5. Upload to Evidently Cloud\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\nAlternatively, retrieve an existing project:\\n\\n```python\\n# project = ws.get_project(\"PROJECT_ID\")\\n```\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report with scored data that\\'s easy to interact with.\\n\\n![](',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': 'll responses to be faithful, so our tests fail. You can adjust these conditions — for example, allowing a certain percentage of responses to fail.\\n\\n![](/images/examples/rag_tests-min.png)\\n\\n## 5. Upload to Evidently Cloud\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\nAlternatively, retrieve an existing project:\\n\\n```python\\n# project = ws.get_project(\"PROJECT_ID\")\\n```\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report with scored data that\\'s easy to interact with.\\n\\n![](/images/examples/rag_cloud_view-min.png)\\n\\n## What\\'s Next?\\n\\nConsidering implementing a [regression testing](/examples/LLM_regression_testing) at every update to monitor how your RAG system retrieval and response quality changes.',\n",
       "  'title': 'RAG evals',\n",
       "  'description': 'Metrics to evaluate a RAG system.',\n",
       "  'filename': 'docs-main/examples/LLM_rag_evals.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\",\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \".\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets imp\",\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'e.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'ort WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': ' is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_f',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'ormats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we s',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': ' lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use ',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'ee different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For ',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': '\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n    ',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provide',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': '    non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': 'r = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow i',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': '\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': 'n the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` ',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': ' Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance betwe',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': 'to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefin',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': 'en the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': 'ition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatical',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': ' code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.',\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'docs-main/examples/LLM_regression_testing.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Note>\\n  **We have an applied course on LLM evaluations\\\\!** Free video course with 10\\\\+ tutorials. [Sign up](https://www.evidentlyai.com/llm-evaluation-course-practice).\\n</Note>\\n\\n## Quickstarts\\n\\nIf you are new, start here.\\n\\n<CardGroup cols={3}>\\n  <Card title=\"LLM quickstart\" icon=\"comment-text\" href=\"/quickstart_llm\">\\n    Evaluate the quality of text outputs.\\n  </Card>\\n  <Card title=\"ML quickstart\" icon=\"table\" href=\"/quickstart_ml\">\\n    Test tabular data quality and data drift.\\n  </Card>\\n  <Card title=\"Tracing quickstart\" icon=\"bars-staggered\" href=\"/quickstart_tracing\">\\n    Collect inputs and outputs from AI your app.\\n  </Card>\\n</CardGroup>\\n\\n## LLM Tutorials\\n\\nEnd-to-end examples of specific workflows and use cases.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM as a judge\" icon=\"scale-balanced\" href=\"/examples/LLM_judge\">\\n    How to create and evaluate an LLM judge against human labels.\\n  </Card>\\n  <Card title=\"RAG evaluation\" icon=\"comment\" href=\"/examples/LLM_rag_evals\">\\n    A walkthrough of different RAG evaluation metrics.\\n  </Card>\\n  <Card title=\"LLM as a jury\" icon=\"dice\" href=\"LLM_jury\">\\n    Using multiple LLMs to evaluate the same output.\\n  </Card>\\n  <Card title=\"LLM evaluation methods\" icon=\"text\" href=\"LLM_evals\">\\n    A walkthrough of different LLM evaluation methods. [CODE \\\\+ VIDEO]\\n  </Card>\\n  <Card title=\"Descriptor cookbook\" icon=\"book\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/descriptors.ipynb\">\\n    A walkthrough of different descriptors (deterministic, ML, etc.) a single notebook.\\n  </Card>\\n  <Card title=\"LLM judge prompt optimization (1)\" icon=\"hotel\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb\">\\n    Optimize a multi-class classifier using target labels.\\n  </Card>\\n  <Card title=\"LLM judge prompt optimization (2)\" icon=\"code\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb\">\\n ',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'h of different RAG evaluation metrics.\\n  </Card>\\n  <Card title=\"LLM as a jury\" icon=\"dice\" href=\"LLM_jury\">\\n    Using multiple LLMs to evaluate the same output.\\n  </Card>\\n  <Card title=\"LLM evaluation methods\" icon=\"text\" href=\"LLM_evals\">\\n    A walkthrough of different LLM evaluation methods. [CODE \\\\+ VIDEO]\\n  </Card>\\n  <Card title=\"Descriptor cookbook\" icon=\"book\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/descriptors.ipynb\">\\n    A walkthrough of different descriptors (deterministic, ML, etc.) a single notebook.\\n  </Card>\\n  <Card title=\"LLM judge prompt optimization (1)\" icon=\"hotel\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb\">\\n    Optimize a multi-class classifier using target labels.\\n  </Card>\\n  <Card title=\"LLM judge prompt optimization (2)\" icon=\"code\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb\">\\n    Optimize a binary classifier using target labels and free-form feedback.\\n  </Card>\\n</CardGroup>\\n\\n## ML tutorials\\n\\nEnd-to-end examples of specific workflows and use cases.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Metric cookbook\" icon=\"book\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb\">\\n    Various data/ML metrics: Regression, Classification, Data Quality, Data Drift.\\n  </Card>\\n</CardGroup>\\n\\n## Integrations\\n\\nEnd-to-end examples of integrating Evidently with other tools and platforms.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"GitHub actions\" icon=\"code\" href=\"/examples/GitHub_actions\">\\n    Running Evidently evals as part of CI/CD workflow. Native GitHub action integration for regression testing.\\n  </Card>\\n  <Card title=\"Different LLM providers as judges\" icon=\"sparkles\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/future_examples/llm_providers.ipynb\">\\n    Examples of using different external evaluator LLMs as LLM judges: OpenAI,',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '   Optimize a binary classifier using target labels and free-form feedback.\\n  </Card>\\n</CardGroup>\\n\\n## ML tutorials\\n\\nEnd-to-end examples of specific workflows and use cases.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Metric cookbook\" icon=\"book\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb\">\\n    Various data/ML metrics: Regression, Classification, Data Quality, Data Drift.\\n  </Card>\\n</CardGroup>\\n\\n## Integrations\\n\\nEnd-to-end examples of integrating Evidently with other tools and platforms.\\n\\n<CardGroup cols={2}>\\n  <Card title=\"GitHub actions\" icon=\"code\" href=\"/examples/GitHub_actions\">\\n    Running Evidently evals as part of CI/CD workflow. Native GitHub action integration for regression testing.\\n  </Card>\\n  <Card title=\"Different LLM providers as judges\" icon=\"sparkles\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/future_examples/llm_providers.ipynb\">\\n    Examples of using different external evaluator LLMs as LLM judges: OpenAI, Gemini, Google Vertex, Mistral, Ollama.\\n  </Card>\\n  <Card title=\"Evidently + Grafana: LLM evals\" icon=\"chart-gantt\" href=\"https://github.com/evidentlyai/evidently/tree/main/examples/llm_eval_grafana_dashboard\">\\n    Visualize Evidently LLM evaluation metrics with Grafana. (Postgres as a database).\\n  </Card>\\n  <Card title=\"Evidently+ Grafana: Data drift\" icon=\"chart-column\" href=\"https://github.com/evidentlyai/evidently/tree/main/examples/data_drift_grafana_dashboard\">\\n    Visualize Evidently data drift evaluations on a Grafana dashboard. (Postgres as a database).\\n  </Card>\\n</CardGroup>\\n\\n\\n## Deployment\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Evidently Open-source UI tutorial\" icon=\"laptop-code\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/service/workspace_tutorial.ipynb\">\\n    How to create a workspace, project and run Reports.\\n  </Card>\\n</CardGroup>\\n\\n## LLM Evaluation Course - Video Tutorials\\n\\nWe have an applied LLM evaluation course where we walk through the core eva',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' Gemini, Google Vertex, Mistral, Ollama.\\n  </Card>\\n  <Card title=\"Evidently + Grafana: LLM evals\" icon=\"chart-gantt\" href=\"https://github.com/evidentlyai/evidently/tree/main/examples/llm_eval_grafana_dashboard\">\\n    Visualize Evidently LLM evaluation metrics with Grafana. (Postgres as a database).\\n  </Card>\\n  <Card title=\"Evidently+ Grafana: Data drift\" icon=\"chart-column\" href=\"https://github.com/evidentlyai/evidently/tree/main/examples/data_drift_grafana_dashboard\">\\n    Visualize Evidently data drift evaluations on a Grafana dashboard. (Postgres as a database).\\n  </Card>\\n</CardGroup>\\n\\n\\n## Deployment\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Evidently Open-source UI tutorial\" icon=\"laptop-code\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/service/workspace_tutorial.ipynb\">\\n    How to create a workspace, project and run Reports.\\n  </Card>\\n</CardGroup>\\n\\n## LLM Evaluation Course - Video Tutorials\\n\\nWe have an applied LLM evaluation course where we walk through the core evaluation workflows. Each consists of the code example and a video tutorial walthrough.\\n\\n📥 [Sign up for the course](https://www.evidentlyai.com/llm-evaluation-course-practice)\\n\\n📹 [See complete Youtube playlist](https://www.youtube.com/watch?v=K8LLVi5Xrh8&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=2)\\n\\n| **Tutorial**                     | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                            | **Code example**                                                                                                                         | **Video**                                                                   ',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'luation workflows. Each consists of the code example and a video tutorial walthrough.\\n\\n📥 [Sign up for the course](https://www.evidentlyai.com/llm-evaluation-course-practice)\\n\\n📹 [See complete Youtube playlist](https://www.youtube.com/watch?v=K8LLVi5Xrh8&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=2)\\n\\n| **Tutorial**                     | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                            | **Code example**                                                                                                                         | **Video**                                                                                                                                 |\\n| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\\n| **Intro to LLM Evals**           | Introduction to LLM evaluation: concepts, goals, and motivations behind evaluating LLM outputs.                                           ',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': '                                                              |\\n| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\\n| **Intro to LLM Evals**           | Introduction to LLM evaluation: concepts, goals, and motivations behind evaluating LLM outputs.                                                                                                                                                                                                                                                                                                                                                            | –                                                                                                                                        | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **LLM Evaluation Methods**       | Tutorial with an overview of methods. <ul>        <li>        Part 1. Anatomy of a single evaluation. Covers basic LLM evaluation API and setup.</li>                <li>        Part 2. Reference-based evaluation: exact match, semantic similarity, BERTScore, and LLM judge.</li>                <li>        Part 3. Reference-free evaluation: text statistics, regex, ML models, LL',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': '                                                                                                                                                                                                                                                                                                                 | –                                                                                                                                        | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **LLM Evaluation Methods**       | Tutorial with an overview of methods. <ul>        <li>        Part 1. Anatomy of a single evaluation. Covers basic LLM evaluation API and setup.</li>                <li>        Part 2. Reference-based evaluation: exact match, semantic similarity, BERTScore, and LLM judge.</li>                <li>        Part 3. Reference-free evaluation: text statistics, regex, ML models, LLM judges, and session-level evaluators.</li>                </ul> | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb) | <ul>        <li>        Video 1</li>                <li>        Video 2</li>                <li>        Video 3</li>                </ul> |\\n| **LLM as a Judge**               | Tutorial on creating and tuning LLM judges aligned with human preferences.                                                                                                                                                                                                                                                                                                                                                                                 | [Open Notebook](LLMCourse_Tutorial_2_LLM_as_a_judge.ipynb)                                                                               | <ul>        <li>        Video</l',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'M judges, and session-level evaluators.</li>                </ul> | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb) | <ul>        <li>        Video 1</li>                <li>        Video 2</li>                <li>        Video 3</li>                </ul> |\\n| **LLM as a Judge**               | Tutorial on creating and tuning LLM judges aligned with human preferences.                                                                                                                                                                                                                                                                                                                                                                                 | [Open Notebook](LLMCourse_Tutorial_2_LLM_as_a_judge.ipynb)                                                                               | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **Clasification Evaluation**     | Tutorial on evaluating LLMs and a simple predictive ML baseline on a multi-class classification task.                                                                                                                                                                                                                                                                                                                                                      | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Classification_Evals.ipynb)                  | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **Content Generation with LLMs** | Tutorial on how to use LLMs to write tweets and evaluate how engaging they are. Introduction t',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'i>                </ul>                                                                                   |\\n| **Clasification Evaluation**     | Tutorial on evaluating LLMs and a simple predictive ML baseline on a multi-class classification task.                                                                                                                                                                                                                                                                                                                                                      | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Classification_Evals.ipynb)                  | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **Content Generation with LLMs** | Tutorial on how to use LLMs to write tweets and evaluate how engaging they are. Introduction to the concept of tracing.                                                                                                                                                                                                                                                                                                                                    | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Content_Generation_Evals.ipynb)              | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **RAG evaluations**              | <ul>        <li>        Part 1. Theory on how to evaluate RAG systems: retrieval, generation quality and synthetic data.</li>                <li>        Part 2. Tutorial on building a toy RAG application and evaluating correctness and faithfulness.</li>                </ul>                                                           ',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'o the concept of tracing.                                                                                                                                                                                                                                                                                                                                    | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Content_Generation_Evals.ipynb)              | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **RAG evaluations**              | <ul>        <li>        Part 1. Theory on how to evaluate RAG systems: retrieval, generation quality and synthetic data.</li>                <li>        Part 2. Tutorial on building a toy RAG application and evaluating correctness and faithfulness.</li>                </ul>                                                                                                                                                                         | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_RAG_Evals.ipynb)                             | <ul>        <li>        Video 1</li>                <li>        Video 2</li>                </ul>                                         |\\n| **AI agent evaluations**         | Tutorial on how to build a simple Q&A agent and evaluate tool choice and answer correctness.                                                                                                                                                                                                                                                                                                                                                               | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Agent_Evals.ipynb)                 ',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': '                                                                                                              | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_RAG_Evals.ipynb)                             | <ul>        <li>        Video 1</li>                <li>        Video 2</li>                </ul>                                         |\\n| **AI agent evaluations**         | Tutorial on how to build a simple Q&A agent and evaluate tool choice and answer correctness.                                                                                                                                                                                                                                                                                                                                                               | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Agent_Evals.ipynb)                           | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **Adversarial testing**          | Tutorial on how to run scenario-based risk testing on forbidden topics and brand risks.                                                                                                                                                                                                                                                                                                                                                                    | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Adversarial_Testing.ipynb)                   | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n\\n## More examples\\n\\nYou can also find more examples in the [Example Repository](https://',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': '          | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n| **Adversarial testing**          | Tutorial on how to run scenario-based risk testing on forbidden topics and brand risks.                                                                                                                                                                                                                                                                                                                                                                    | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Adversarial_Testing.ipynb)                   | <ul>        <li>        Video</li>                </ul>                                                                                   |\\n\\n## More examples\\n\\nYou can also find more examples in the [Example Repository](https://github.com/evidentlyai/community-examples).',\n",
       "  'title': 'Tutorials and guides',\n",
       "  'description': 'End-to-end code examples.',\n",
       "  'filename': 'docs-main/examples/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"## ⚠️ Breaking Change Notice\\n\\nWe’ve launched **Evidently Cloud v2** – a major update that brings significant improvements and **breaking changes** to our cloud platform. Please read this carefully to ensure compatibility.\\n\\n## 🚀 What’s New\\n\\n- **Redesigned dashboard** – faster, cleaner, and more intuitive.\\n- **Improved performance** – lighter and more efficient calculations.\\n- **Better LLM evaluation support** – including new features like descriptor calculation directly in the cloud.\\n\\n## 🆕 Who Gets Cloud v2?\\n\\n- **All new users** are automatically enrolled in **Evidently Cloud v2**.\\n- **Existing Cloud v1 users** can manually **switch** to the new version.\\n\\n<Warning>\\n**Breaking changes:** Cloud v2 is **not compatible** with Evidently library versions below `0.7.0`.\\n</Warning>\\n\\n## 🧩 SDK Compatibility Matrix\\n\\n| Cloud Version | Required Evidently library Version |\\n| ------------- | ---------------------------------- |\\n| **Cloud v2**  | `evidently>=0.7.0`                 |\\n| **Cloud v1**  | `evidently<0.7.0`                  |\\n\\nMake sure you use the matching version of the Evidently Python library for your Cloud environment.\\n\\n## 🔄 Switching Between Versions\\n\\nYou can switch back to **Cloud v1** from your **Account Settings** if needed.  However, we **highly recommend** using **Cloud v2** for the latest and most powerful features.\\n\\n<Warning>\\n  **Deprecation Notice: Free users will have access to Evidently Cloud v1 until May 31, 2025.** Please make sure you migrate to Cloud v2 and corresponding SDK version within this period to be able to continue sending data without interruptions. After that, Cloud v1 will enter **read-only mode**.\\n</Warning>\\n\\n## 📦 Need Help Migrating?\\n\\nIf you're a **paying customer** and need assistance with:\\n\\n- Migrating assets\\n- Updating your code\\n- Any technical support\\n\\n📧 Reach out to us at [**support@evidentlyai.com**](mailto:support@evidentlyai.com)\",\n",
       "  'title': 'Evidently Cloud v2',\n",
       "  'description': 'A new version of Evidently Cloud available starting April 10, 2025.',\n",
       "  'filename': 'docs-main/faq/cloud_v2.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '## Discord\\n\\nJoin our [Discord community](https://discord.gg/xZjKRaNp8b) to chat and connect.\\n\\n## GitHub\\n\\nOpen an issue on [GitHub](https://github.com/evidentlyai/evidently) to report bugs and ask questions.\\n\\n## Blog\\n\\nRead our [blog](https://evidentlyai.com/blog), [guides](https://www.evidentlyai.com/mlops-guides) and [tutorials](https://www.evidentlyai.com/mlops-tutorials) for tutorials and content.\\n\\n## Newsletter\\n\\n[Sign up](https://www.evidentlyai.com/sign-up) for our news, content and product updates.\\n\\n## Twitter\\n\\nFollow and connect with us on [Twitter](https://twitter.com/EvidentlyAI).\\n\\n## Email\\n\\nFor general inquiries: [*hello@evidentlyai.com*](mailto:hello@evidentlyai.com)\\n\\n<Note>\\n  Note: we do not provide open-source support via email. If you need help, please ask in the Discord community or open an issue on GitHub.\\n</Note>',\n",
       "  'title': 'Contact us',\n",
       "  'description': 'How to connect with Evidently team.',\n",
       "  'filename': 'docs-main/faq/contact.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Card title=\"Evidently Cloud - Migration guide\" href=\"/faq/cloud_v2\">\\n  What\\'s new in Evidently Cloud v2.\\n</Card>\\n\\n<Card title=\"Evidently library - Migration guide\" href=\"/faq/migration\">\\n  How to migrate to a new Evidently 0.6 version and above.\\n</Card>\\n\\n<Card title=\"OSS vs Cloud\" href=\"/faq/oss_vs_cloud\">\\n  Understand feature availability.\\n</Card>\\n\\nUse the menu on the left to explore other questions.',\n",
       "  'title': 'Frequently Asked Questions',\n",
       "  'description': 'Popular questions.',\n",
       "  'filename': 'docs-main/faq/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"This guide explains the key changes introduced in Evidently 0.6 and above. It is meant for **existing users** who used earlier version of Evidently library prior to 2025.\\n\\n<Check>\\n  If you're new to Evidently, skip this page and head directly to the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n</Check>\\n\\n## What happened?\\n\\nHere is a summary of changes to the Evidently Python library.\\n\\n| Change                                       | Date         | Description                                                                                                                                                                                                                                                                          |\\n| -------------------------------------------- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| **New API.** Version 0.6                     | January 2025 | Evidently 0.6 brought an updated core API with a new Report object. You can learn more in [the blog](https://www.evidentlyai.com/blog/evidently-api-change).  <ul><li>To use the new API, import components from `evidently.future`. For example:`from evidently.future import Report`  </li> </ul>   |\\n| **Transition period.** Versions 0.6 to 0.6.7 | -            | During the transition period, both APIs co-existed in the library. For version between 0.6 and 0.6.7, you can choose either option: <ul><li> Use the new API importing it as `from evidently.future.` </li><li>   Use the legacy API, as documented in [old docs](https://docs-old.evidentlyai.com/). </li> </ul>              |\\n| **Breaking change.** Version 0.7.            | April 2025   | Evidently 0.7 release makes the new API the default.<ul><li> You can import it as `from evidently im\",\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'docs-main/faq/migration.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '---------------------------------------------------------------------------------------- |\\n| **New API.** Version 0.6                     | January 2025 | Evidently 0.6 brought an updated core API with a new Report object. You can learn more in [the blog](https://www.evidentlyai.com/blog/evidently-api-change).  <ul><li>To use the new API, import components from `evidently.future`. For example:`from evidently.future import Report`  </li> </ul>   |\\n| **Transition period.** Versions 0.6 to 0.6.7 | -            | During the transition period, both APIs co-existed in the library. For version between 0.6 and 0.6.7, you can choose either option: <ul><li> Use the new API importing it as `from evidently.future.` </li><li>   Use the legacy API, as documented in [old docs](https://docs-old.evidentlyai.com/). </li> </ul>              |\\n| **Breaking change.** Version 0.7.            | April 2025   | Evidently 0.7 release makes the new API the default.<ul><li> You can import it as `from evidently import Report`. </li><li>    This is coupled with updates to the [Evidently platform](cloud_v2).  </li> </ul>                                                                                                    |\\n<Info>\\n  If you still need the old API, pin your Evidently version to `0.6.7` or earlier.\\n</Info>\\n\\n## What changed?\\n\\nThese updates bring various improvements and changes to the core library. You can also learn more in the [release blog](https://www.evidentlyai.com/blog/evidently-api-change).\\n\\n### Data Definition\\n\\nWe replaced `column_mapping` with `data_definition`. Now, you also need to explicitly create an Evidently `Dataset` object instead of just passing a dataframe when running a Report. Each `Dataset` object has an associated `DataDefinition`.\\n\\nWhile similar to column mapping, this new structure lets you cleanly map input columns based on their **type** (e.g., categorical, numerical, datetime, text) and **role** (e.g., target, prediction, timestamp). A column can have both a ',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'docs-main/faq/migration.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'port Report`. </li><li>    This is coupled with updates to the [Evidently platform](cloud_v2).  </li> </ul>                                                                                                    |\\n<Info>\\n  If you still need the old API, pin your Evidently version to `0.6.7` or earlier.\\n</Info>\\n\\n## What changed?\\n\\nThese updates bring various improvements and changes to the core library. You can also learn more in the [release blog](https://www.evidentlyai.com/blog/evidently-api-change).\\n\\n### Data Definition\\n\\nWe replaced `column_mapping` with `data_definition`. Now, you also need to explicitly create an Evidently `Dataset` object instead of just passing a dataframe when running a Report. Each `Dataset` object has an associated `DataDefinition`.\\n\\nWhile similar to column mapping, this new structure lets you cleanly map input columns based on their **type** (e.g., categorical, numerical, datetime, text) and **role** (e.g., target, prediction, timestamp). A column can have both a type and role.\\n\\nYou can also now map **multiple targets and predictions** inside the same table: e.g., if you deal with multiple regression or have several classification results in one table.\\n\\nAutomated column type/role mapping is still available. Additionally, new mappings for LLM use cases, like RAG, will be supported.\\n\\n<Card title=\"Data Definition\" href=\"/docs/library/data_definition\">\\n  Docs on mapping the input data.\\n</Card>\\n\\n### Descriptors\\n\\nDescriptors provide row-level text evaluations, ranging from basic checks (e.g., text length) to LLM-based evals (e.g., checking for contradictions). With the increasing focus on LLM-related metrics, we’ve updated the text descriptors API to make it more logical and easier to use.\\n\\nDescriptor computation is now split into **two steps**:\\n\\n**1. Compute Descriptors**. Add them to the source table containing inputs and outputs. You can do this together with data definition. For example:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFram',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'docs-main/faq/migration.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'type and role.\\n\\nYou can also now map **multiple targets and predictions** inside the same table: e.g., if you deal with multiple regression or have several classification results in one table.\\n\\nAutomated column type/role mapping is still available. Additionally, new mappings for LLM use cases, like RAG, will be supported.\\n\\n<Card title=\"Data Definition\" href=\"/docs/library/data_definition\">\\n  Docs on mapping the input data.\\n</Card>\\n\\n### Descriptors\\n\\nDescriptors provide row-level text evaluations, ranging from basic checks (e.g., text length) to LLM-based evals (e.g., checking for contradictions). With the increasing focus on LLM-related metrics, we’ve updated the text descriptors API to make it more logical and easier to use.\\n\\nDescriptor computation is now split into **two steps**:\\n\\n**1. Compute Descriptors**. Add them to the source table containing inputs and outputs. You can do this together with data definition. For example:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n\\n**2. Aggregate results or run conditional checks**. Use these descriptors like any other dataset column when creating a Report. For example, here is how you summarize all descriptors and check that the text length is under 100 symbols.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nThis decoupling means you can reuse descriptor outputs for multiple tests or aggregations without recomputation. It’s especially useful for LLM evaluations.\\n\\n<Card title=\"Descriptors\" href=\"/docs/library/descriptors\">\\n  Docs on adding descriptors.\\n</Card>\\n\\n### New Reports API\\n\\nAs you may have noticed in the example above, we made the changes to the core Report ',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'docs-main/faq/migration.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'e(df),\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n\\n**2. Aggregate results or run conditional checks**. Use these descriptors like any other dataset column when creating a Report. For example, here is how you summarize all descriptors and check that the text length is under 100 symbols.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nThis decoupling means you can reuse descriptor outputs for multiple tests or aggregations without recomputation. It’s especially useful for LLM evaluations.\\n\\n<Card title=\"Descriptors\" href=\"/docs/library/descriptors\">\\n  Docs on adding descriptors.\\n</Card>\\n\\n### New Reports API\\n\\nAs you may have noticed in the example above, we made the changes to the core Report API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\"Reports\" href=\"/docs/library/report\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \"Group by\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If ',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'docs-main/faq/migration.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\"Reports\" href=\"/docs/library/report\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \"Group by\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If you choose to enable Tests, their results appear as a separate tab in the same HTML file. This eliminated duplication and the need to switch between separate files or Reports.\\n\\nFor example, here is how you add a Test on max length that will appear in the same Report as all data / column statistics.\\n\\n```python\\nreport = Report([\\n     DataSummaryPreset(),\\n     MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nYou can still use auto-generated Test conditions based on your reference dataset or define your own expectations.\\n\\n<Card title=\"Tests\" href=\"/docs/library/tests\">\\n  How to add Tests with conditions.\\n</Card>\\n\\n### Metric redesign\\n\\nThe Metric object has been simplified:\\n\\n- Metrics now produce a single computation result with a fixed structure.\\n- Some visualization types can be specified directly as parameters to the Metric.\\n\\nThis redesign significantly improves JSON result parsing and UI integration, since each Metric has a single or two results only.\\n\\nYou can check the list of new Me',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'docs-main/faq/migration.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'you choose to enable Tests, their results appear as a separate tab in the same HTML file. This eliminated duplication and the need to switch between separate files or Reports.\\n\\nFor example, here is how you add a Test on max length that will appear in the same Report as all data / column statistics.\\n\\n```python\\nreport = Report([\\n     DataSummaryPreset(),\\n     MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nYou can still use auto-generated Test conditions based on your reference dataset or define your own expectations.\\n\\n<Card title=\"Tests\" href=\"/docs/library/tests\">\\n  How to add Tests with conditions.\\n</Card>\\n\\n### Metric redesign\\n\\nThe Metric object has been simplified:\\n\\n- Metrics now produce a single computation result with a fixed structure.\\n- Some visualization types can be specified directly as parameters to the Metric.\\n\\nThis redesign significantly improves JSON result parsing and UI integration, since each Metric has a single or two results only.\\n\\nYou can check the list of new Metrics here:\\n\\n<Card title=\"Metrics\" href=\"/metrics/all-metrics\">\\n  All available Metrics.\\n</Card>\\n\\nTo get a pre-built combination of multiple checks at once, you can still use Presets.\\n\\n### Simplified Dashboard API\\n\\nWith the redesigned Metrics, the Dashboard API is now much, much simpler. You can create new panels and point to specific Metric results with a strictly fixed set of options.\\n\\n<Card title=\"Dashboard\" href=\"/docs/platform/dashboard_add_panels\">\\n  How to add Dashboard panels.\\n</Card>\\n\\nAdditional improvement: custom metrics with custom renders are now viewable in the UI, which was not previously supported.',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'docs-main/faq/migration.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '## Evidently ecosystem\\xa0\\n\\nEvidently AI develops several products:\\n\\n* Evidently library (OSS).\\n\\n* Tracely library (OSS).\\n\\n* The Evidently Platform (OSS and Commercial).\\n\\n### **Evidently**\\n\\nThe **Evidently Python library** allows users to run various data and AI evaluations and generate Reports and Test Suites with evaluation results. It is best suited for individual data scientists, AI, and ML engineers analyzing the quality of AI systems in a Python environment. The library is open-source and available under the **Apache 2.0** license.\\n\\n### **Tracely**\\n\\nThe **Tracely Python library** lets users capture near real-time data from their AI applications. It is based on OpenTelemetry. The library is open-source (**Apache 2.0**).\\n\\n### Evidently Platform\\n\\nThe **Evidently Platform** is a web application designed for AI testing and observability. It is tailored for teams looking to collaborate on AI quality from experiments to production monitoring. It natively integrates with Evidently and Tracely and has two options:\\n\\n* **Open-source edition**. A basic version of the platform is included in the open-source Evidently library. It has a limited feature set in favor of a very lightweight deployment.\\n\\n* **Commercial edition**. Offers additional advanced features for AI quality workflows, collaboration, and scalability. Two deployment options are available:\\n\\n  * **Evidently Cloud**. The recommended and easiest way to start. Evidently Cloud is hosted and managed by Evidently AI.\\n\\n  * **Evidently Enterprise (Self-Hosted)**. Designed for teams with strict security requirements. This version offers a full-featured platform equivalent to Evidently Cloud that can be deployed in private clouds or on-premises.\\n\\n## OSS vs. Cloud / Enterprise\\n\\nPlatform editions differ in features, level of support, and maintenance costs.\\n\\n### Feature availability\\n\\n| Category             | Feature                   | Open-source | Cloud and Enterprise |\\n| -------------------- | ------------------------- | --',\n",
       "  'title': 'Open-source vs. Cloud',\n",
       "  'description': 'Deployment options and feature overview.',\n",
       "  'filename': 'docs-main/faq/oss_vs_cloud.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'ly and has two options:\\n\\n* **Open-source edition**. A basic version of the platform is included in the open-source Evidently library. It has a limited feature set in favor of a very lightweight deployment.\\n\\n* **Commercial edition**. Offers additional advanced features for AI quality workflows, collaboration, and scalability. Two deployment options are available:\\n\\n  * **Evidently Cloud**. The recommended and easiest way to start. Evidently Cloud is hosted and managed by Evidently AI.\\n\\n  * **Evidently Enterprise (Self-Hosted)**. Designed for teams with strict security requirements. This version offers a full-featured platform equivalent to Evidently Cloud that can be deployed in private clouds or on-premises.\\n\\n## OSS vs. Cloud / Enterprise\\n\\nPlatform editions differ in features, level of support, and maintenance costs.\\n\\n### Feature availability\\n\\n| Category             | Feature                   | Open-source | Cloud and Enterprise |\\n| -------------------- | ------------------------- | ----------- | -------------------- |\\n| **Core features**    | Tracing (instrumentation) | +           | +                    |\\n|                      | Evaluations (100+ checks) | +           | +                    |\\n|                      | Reports and Test Suites   | +           | +                    |\\n|                      | Monitoring dashboard      | +           | +                    |\\n|                      | Custom metrics            | +           | +                    |\\n|                      | Report (JSON) storage     | +           | +                    |\\n|                      | API access                | +           | +                    |\\n| **Premium features** | Trace viewer              | -           | +                    |\\n|                      | No-code evaluations       | -           | +                    |\\n|                      | Scheduled evaluations     | -           | +                    |\\n|                      | Dataset management        | -           ',\n",
       "  'title': 'Open-source vs. Cloud',\n",
       "  'description': 'Deployment options and feature overview.',\n",
       "  'filename': 'docs-main/faq/oss_vs_cloud.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '--------- | -------------------- |\\n| **Core features**    | Tracing (instrumentation) | +           | +                    |\\n|                      | Evaluations (100+ checks) | +           | +                    |\\n|                      | Reports and Test Suites   | +           | +                    |\\n|                      | Monitoring dashboard      | +           | +                    |\\n|                      | Custom metrics            | +           | +                    |\\n|                      | Report (JSON) storage     | +           | +                    |\\n|                      | API access                | +           | +                    |\\n| **Premium features** | Trace viewer              | -           | +                    |\\n|                      | No-code evaluations       | -           | +                    |\\n|                      | Scheduled evaluations     | -           | +                    |\\n|                      | Dataset management        | -           | +                    |\\n|                      | Synthetic data generation | -           | +                    |\\n|                      | Adversarial testing | -           | +                    |\\n|                      | AI agent testing | -           | +                    |\\n|                      | No-code dashboards        | -           | +                    |\\n|                      | Alerts                    | -           | +                    |\\n|                      | Raw data storage          | -           | +                    |\\n| **Access control**   | Authentication            | -           | +                    |\\n|                      | Role-based access control | -           | +                    |\\n\\nSee full details on the commercial plans on the [Pricing page](https://www.evidentlyai.com/pricing).\\n\\nIn summary:\\n\\n* All the core evaluation features are open-source.\\n\\n* The OSS version of the Evidently Platform offers a lightweight deployment with a base feature set f',\n",
       "  'title': 'Open-source vs. Cloud',\n",
       "  'description': 'Deployment options and feature overview.',\n",
       "  'filename': 'docs-main/faq/oss_vs_cloud.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': '| +                    |\\n|                      | Synthetic data generation | -           | +                    |\\n|                      | Adversarial testing | -           | +                    |\\n|                      | AI agent testing | -           | +                    |\\n|                      | No-code dashboards        | -           | +                    |\\n|                      | Alerts                    | -           | +                    |\\n|                      | Raw data storage          | -           | +                    |\\n| **Access control**   | Authentication            | -           | +                    |\\n|                      | Role-based access control | -           | +                    |\\n\\nSee full details on the commercial plans on the [Pricing page](https://www.evidentlyai.com/pricing).\\n\\nIn summary:\\n\\n* All the core evaluation features are open-source.\\n\\n* The OSS version of the Evidently Platform offers a lightweight deployment with a base feature set for storing and visualizing the evaluation results.\\n\\n* The commercial version of the Platform includes additional functionality related to dataset management, collaboration and no-code managed workflows. It also includes security features like role-based access control and comes with a scalable backend.\\n\\n### Support\\n\\nThe commercial platform version includes dedicated support.\\n\\n* **Evidently OSS.** We provide documentation and a Discord community forum, but as a small team, we can’t offer extensive support to open-source users. If you’re running Evidently OSS in production, you must be comfortable troubleshooting and resolving issues on your own. For more hands-on support, we recommend upgrading to Evidently Cloud / Enterprise.\\n\\n* **Evidently Cloud / Enterprise**. We offer varying tiers of support based on the selected Plan. In any scenario, you get direct access to the developers who built the Platform. We help resolve any issues and bugs and provide ongoing assistance on feature config',\n",
       "  'title': 'Open-source vs. Cloud',\n",
       "  'description': 'Deployment options and feature overview.',\n",
       "  'filename': 'docs-main/faq/oss_vs_cloud.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'or storing and visualizing the evaluation results.\\n\\n* The commercial version of the Platform includes additional functionality related to dataset management, collaboration and no-code managed workflows. It also includes security features like role-based access control and comes with a scalable backend.\\n\\n### Support\\n\\nThe commercial platform version includes dedicated support.\\n\\n* **Evidently OSS.** We provide documentation and a Discord community forum, but as a small team, we can’t offer extensive support to open-source users. If you’re running Evidently OSS in production, you must be comfortable troubleshooting and resolving issues on your own. For more hands-on support, we recommend upgrading to Evidently Cloud / Enterprise.\\n\\n* **Evidently Cloud / Enterprise**. We offer varying tiers of support based on the selected Plan. In any scenario, you get direct access to the developers who built the Platform. We help resolve any issues and bugs and provide ongoing assistance on feature configuration and use. For the Enterprise Plan, we also offer onboarding and training sessions.\\n\\n### Hosting and maintenance\\n\\nMaintenance\\xa0requirements depend on the setup you choose.\\n\\n* **Evidently OSS**. You’re responsible for deploying and managing the Platform within your environment, including backups, upgrades, and scaling. While the software is free, you need engineering resources for maintenance and cloud resources for storage and compute.\\n\\n* **Evidently Cloud**. The Platform is fully managed by the Evidently team, so that you can focus on building your AI products without worrying about infrastructure. This includes automatic updates, security patches, and scalability, making it the most cost-effective option. The Platform is instantly available upon signup.\\xa0\\n\\n* **Evidently Enterprise (Self-Hosted)**. For large organizations that need to keep data on-premises. The Enterprise version comes with dedicated implementation support, but your team must still manage ongoing configuration and',\n",
       "  'title': 'Open-source vs. Cloud',\n",
       "  'description': 'Deployment options and feature overview.',\n",
       "  'filename': 'docs-main/faq/oss_vs_cloud.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'uration and use. For the Enterprise Plan, we also offer onboarding and training sessions.\\n\\n### Hosting and maintenance\\n\\nMaintenance\\xa0requirements depend on the setup you choose.\\n\\n* **Evidently OSS**. You’re responsible for deploying and managing the Platform within your environment, including backups, upgrades, and scaling. While the software is free, you need engineering resources for maintenance and cloud resources for storage and compute.\\n\\n* **Evidently Cloud**. The Platform is fully managed by the Evidently team, so that you can focus on building your AI products without worrying about infrastructure. This includes automatic updates, security patches, and scalability, making it the most cost-effective option. The Platform is instantly available upon signup.\\xa0\\n\\n* **Evidently Enterprise (Self-Hosted)**. For large organizations that need to keep data on-premises. The Enterprise version comes with dedicated implementation support, but your team must still manage ongoing configuration and maintenance.',\n",
       "  'title': 'Open-source vs. Cloud',\n",
       "  'description': 'Deployment options and feature overview.',\n",
       "  'filename': 'docs-main/faq/oss_vs_cloud.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"Telemetry refers to the collection of usage data. We collect some data to understand how many users we have and how they interact with Evidently open-source.\\n\\nThis helps us improve the tool and prioritize implementing the new features. Below we describe what is collected, how to opt out and why we'd appreciate if you keep the telemetry on.\\n\\n## **What data is collected?**\\n\\nTelemetry is collected in Evidently starting from **version 0.4.0**.\\n\\nWe only collect telemetry when you use **Evidently Monitoring UI**. We DO NOT collect any telemetry when you use the tool as a library, for instance, run in a Jupyter notebook or in a Python script to generate Evidently Reports.\\n\\nWe only collect **anonymous** usage data. We DO NOT collect personal data.\\n\\nWe only collect data about **environment** and **service** use. Our telemetry is intentionally limited in scope. We DO NOT collect any sensitive information or data about the datasets you process. We DO NOT have access to the dataset schema, parameters, variable names, or anything related to the contents of the data or your code.\\n\\nWe collect the following types of data.\\n\\n**Environment data**. Basic information about the environment in which you run Evidently:\\n\\n* `timestamp`\\n\\n* `user_id`\\n\\n* `os_name`\\n\\n* `os_version`\\n\\n* `python_version`\\n\\n* `tool_name`\\n\\n* `tool_version`\\n\\n* `source_ip`\\n\\n<Info>\\n  The `source_ip` is NOT your IP address. We use `jitsu`, an [open-source tool](https://github.com/jitsucom/jitsu) for event collection. We always use strict `ip_policy` which obscures the exact IP. You can read more in Jitsu [docs](https://classic.jitsu.com/docs/sending-data/js-sdk/reference/parameters-reference).\\n</Info>\\n\\n<Info>\\n  The `user_ID` is anonymized and only allows matching that actions are performed by the same user.\\n</Info>\\n\\n**Service usage data.** Data about the following actions performed in the service to understand features being used:\\n\\n* `Startup`\\n\\n* `Index`\\n\\n* `List_projects`\\n\\n* `Get_project_info`\\n\\n* `Project_dashboard`\\n\\n* `L\",\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'ers, variable names, or anything related to the contents of the data or your code.\\n\\nWe collect the following types of data.\\n\\n**Environment data**. Basic information about the environment in which you run Evidently:\\n\\n* `timestamp`\\n\\n* `user_id`\\n\\n* `os_name`\\n\\n* `os_version`\\n\\n* `python_version`\\n\\n* `tool_name`\\n\\n* `tool_version`\\n\\n* `source_ip`\\n\\n<Info>\\n  The `source_ip` is NOT your IP address. We use `jitsu`, an [open-source tool](https://github.com/jitsucom/jitsu) for event collection. We always use strict `ip_policy` which obscures the exact IP. You can read more in Jitsu [docs](https://classic.jitsu.com/docs/sending-data/js-sdk/reference/parameters-reference).\\n</Info>\\n\\n<Info>\\n  The `user_ID` is anonymized and only allows matching that actions are performed by the same user.\\n</Info>\\n\\n**Service usage data.** Data about the following actions performed in the service to understand features being used:\\n\\n* `Startup`\\n\\n* `Index`\\n\\n* `List_projects`\\n\\n* `Get_project_info`\\n\\n* `Project_dashboard`\\n\\n* `List_reports`\\n\\n* `List_test_suites`\\n\\n* `Get_snapshot_download`\\n\\n* `Add_project`\\n\\n* `Search_projects`\\n\\n* `Update_project_info`\\n\\n* `Get_snapshot_graph_data`\\n\\n* `Get_snapshot_data`\\n\\n* `List_project_dashboard_panels`\\n\\n* `Add_snapshot`\\n\\n## How to enable/disable telemetry?\\n\\nBy default, telemetry is enabled.\\n\\nAfter starting up the service, you will see the following message in the terminal:\\n\\n```\\nAnonymous usage reporting is enabled. To disable it, set env variable {DO_NOT_TRACK_ENV} to any value\\n```\\n\\nTo disable telemetry, use the environment variable: `DO_NOT_TRACK`\\n\\nSet it to any value, for instance:\\n\\n```\\nexport DO_NOT_TRACK=1\\n```\\n\\nAfter doing that and starting the service, you will see the message:\\n\\n```\\nAnonymous usage reporting is disabled.\\n```\\n\\nTo enable telemetry back, unset the environment variable:\\n\\n```\\nunset DO_NOT_TRACK\\n```\\n\\n## Event log examples\\n\\n<AccordionGroup>\\n  <Accordion title=\"Action: startup\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:08:44.332528Z\",\\n    \"action\": \"startu',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ist_reports`\\n\\n* `List_test_suites`\\n\\n* `Get_snapshot_download`\\n\\n* `Add_project`\\n\\n* `Search_projects`\\n\\n* `Update_project_info`\\n\\n* `Get_snapshot_graph_data`\\n\\n* `Get_snapshot_data`\\n\\n* `List_project_dashboard_panels`\\n\\n* `Add_snapshot`\\n\\n## How to enable/disable telemetry?\\n\\nBy default, telemetry is enabled.\\n\\nAfter starting up the service, you will see the following message in the terminal:\\n\\n```\\nAnonymous usage reporting is enabled. To disable it, set env variable {DO_NOT_TRACK_ENV} to any value\\n```\\n\\nTo disable telemetry, use the environment variable: `DO_NOT_TRACK`\\n\\nSet it to any value, for instance:\\n\\n```\\nexport DO_NOT_TRACK=1\\n```\\n\\nAfter doing that and starting the service, you will see the message:\\n\\n```\\nAnonymous usage reporting is disabled.\\n```\\n\\nTo enable telemetry back, unset the environment variable:\\n\\n```\\nunset DO_NOT_TRACK\\n```\\n\\n## Event log examples\\n\\n<AccordionGroup>\\n  <Accordion title=\"Action: startup\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:08:44.332528Z\",\\n    \"action\": \"startup\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"cfcc182d-5a2d-47d6-89dd-37590ec7b08a\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n      \"major\": 3,\\n      \"minor\": 9,\\n      \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: index\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:10:54.355143Z\",\\n    \"action\": \"index\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"96029e42-d2fc-4372-a1b5-b15e4d2ec6a0\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n    \"major\": ',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'p\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"cfcc182d-5a2d-47d6-89dd-37590ec7b08a\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n      \"major\": 3,\\n      \"minor\": 9,\\n      \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: index\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:10:54.355143Z\",\\n    \"action\": \"index\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"96029e42-d2fc-4372-a1b5-b15e4d2ec6a0\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n    \"major\": 3,\\n    \"minor\": 9,\\n    \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: list_projects\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-07T14:08:44.687956Z\",\\n      \"action\": \"list_projects\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"12ac8fe3-0396-430b-b035-e984a3ed2663\",\\n      \"extra\": {\\n        \"project_count\": 1\\n      },\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"78.163.128.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accord',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '3,\\n    \"minor\": 9,\\n    \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: list_projects\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-07T14:08:44.687956Z\",\\n      \"action\": \"list_projects\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"12ac8fe3-0396-430b-b035-e984a3ed2663\",\\n      \"extra\": {\\n        \"project_count\": 1\\n      },\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"78.163.128.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: get_project_info\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:10:54.474555Z\",\\n    \"action\": \"get_project_info\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"52bf5758-4b4c-4379-b2e6-0c1b123f3ce3\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n     \"major\": 3,\\n     \"minor\": 9,\\n     \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: project_dashboard\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:08:46.260846Z\",\\n    \"action\": \"project_dashboard\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"2dc109d4-f322-42de-8db2-d9ce86787b',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'ion>\\n\\n  <Accordion title=\"Action: get_project_info\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:10:54.474555Z\",\\n    \"action\": \"get_project_info\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"52bf5758-4b4c-4379-b2e6-0c1b123f3ce3\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n     \"major\": 3,\\n     \"minor\": 9,\\n     \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: project_dashboard\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-07T14:08:46.260846Z\",\\n    \"action\": \"project_dashboard\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"2dc109d4-f322-42de-8db2-d9ce86787b8b\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n      \"major\": 3,\\n      \"minor\": 9,\\n      \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: add_project\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-18T13:15:16.138786Z\",\\n    \"action\": \"add_project\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"ac3d9bf3-8b26-406e-b781-30936c31da87\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n     \"major\": 3,\\n     \"minor\": 9,\\n     \"patch\": 16\\n    },\\n    \"source_ip\": \"88.225.219.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_versi',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': '8b\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n      \"major\": 3,\\n      \"minor\": 9,\\n      \"patch\": 16\\n    },\\n    \"source_ip\": \"78.163.128.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: add_project\">\\n    ```\\n    {\\n    \"_timestamp\": \"2023-07-18T13:15:16.138786Z\",\\n    \"action\": \"add_project\",\\n    \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n    \"error\": null,\\n    \"eventn_ctx_event_id\": \"ac3d9bf3-8b26-406e-b781-30936c31da87\",\\n    \"extra\": {},\\n    \"group_id\": null,\\n    \"interface\": \"service_backend\",\\n    \"os_name\": \"mac\",\\n    \"os_version\": \"13.0.1\",\\n    \"python_version\": {\\n     \"major\": 3,\\n     \"minor\": 9,\\n     \"patch\": 16\\n    },\\n    \"source_ip\": \"88.225.219.1\",\\n    \"src\": \"api\",\\n    \"tool_name\": \"evidently\",\\n    \"tool_version\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: list_reports\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-07T14:08:51.369513Z\",\\n      \"action\": \"list_reports\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"826b5208-aae1-400f-acc6-0fb2ea91c967\",\\n      \"extra\": {\\n        \"reports_count\": 19\\n      },\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"78.163.128.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: list_test_suite\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-07T14:08:46.902323Z\",\\n      \"action\": \"list_',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'on\": \"0.3.3\",\\n    \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: list_reports\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-07T14:08:51.369513Z\",\\n      \"action\": \"list_reports\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"826b5208-aae1-400f-acc6-0fb2ea91c967\",\\n      \"extra\": {\\n        \"reports_count\": 19\\n      },\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"78.163.128.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: list_test_suite\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-07T14:08:46.902323Z\",\\n      \"action\": \"list_test_suites\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"540b1e8e-06cb-4e76-958d-6d49fed7f86e\",\\n      \"extra\": {},\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"78.163.128.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: get_snapshot_data\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-18T12:53:01.609245Z\",\\n      \"action\": \"get_snapshot_data\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"0426ef98-b35c-4040-bada-4e4b9380f4d5\",\\n      \"extra\": {\\n        \"metric_generators\": [],\\n        \"metric_presets\": [],\\n  ',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'test_suites\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"540b1e8e-06cb-4e76-958d-6d49fed7f86e\",\\n      \"extra\": {},\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"78.163.128.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n\\n  <Accordion title=\"Action: get_snapshot_data\">\\n    ```\\n    {\\n      \"_timestamp\": \"2023-07-18T12:53:01.609245Z\",\\n      \"action\": \"get_snapshot_data\",\\n      \"api_key\": \"s2s.5xmxpip2ax4ut5rrihfjhb.uqcoh71nviknmzp77ev6rd\",\\n      \"error\": null,\\n      \"eventn_ctx_event_id\": \"0426ef98-b35c-4040-bada-4e4b9380f4d5\",\\n      \"extra\": {\\n        \"metric_generators\": [],\\n        \"metric_presets\": [],\\n        \"metrics\": [\\n          \"DatasetDriftMetric\",\\n          \"DatasetMissingValuesMetric\",\\n          \"ColumnDriftMetric\",\\n          \"ColumnQuantileMetric\",\\n          \"ColumnDriftMetric\",\\n          \"ColumnQuantileMetric\"\\n        ],\\n        \"snapshot_type\": \"report\",\\n        \"test_generators\": [],\\n        \"test_presets\": [],\\n        \"tests\": []\\n      },\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"88.225.219.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n</AccordionGroup>\\n\\n## **Should I opt out?**\\n\\nBeing open-source, we have no visibility into the tool usage unless someone actively reaches out to us or opens a GitHub issue.\\n\\nWe’d be grateful if you keep the telemetry on since it hel',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '      \"metrics\": [\\n          \"DatasetDriftMetric\",\\n          \"DatasetMissingValuesMetric\",\\n          \"ColumnDriftMetric\",\\n          \"ColumnQuantileMetric\",\\n          \"ColumnDriftMetric\",\\n          \"ColumnQuantileMetric\"\\n        ],\\n        \"snapshot_type\": \"report\",\\n        \"test_generators\": [],\\n        \"test_presets\": [],\\n        \"tests\": []\\n      },\\n      \"group_id\": null,\\n      \"interface\": \"service_backend\",\\n      \"os_name\": \"mac\",\\n      \"os_version\": \"13.0.1\",\\n      \"python_version\": {\\n        \"major\": 3,\\n        \"minor\": 9,\\n        \"patch\": 16\\n      },\\n      \"source_ip\": \"88.225.219.1\",\\n      \"src\": \"api\",\\n      \"tool_name\": \"evidently\",\\n      \"tool_version\": \"0.3.3\",\\n      \"user_id\": \"16d5bb6f-0400-4e2c-90f3-c3b31c95a1d3\"\\n    }\\n    ```\\n  </Accordion>\\n</AccordionGroup>\\n\\n## **Should I opt out?**\\n\\nBeing open-source, we have no visibility into the tool usage unless someone actively reaches out to us or opens a GitHub issue.\\n\\nWe’d be grateful if you keep the telemetry on since it helps us answer questions like:\\n\\n* How many people are actively using the tool?\\n\\n* Which features are being used most?\\n\\n* What is the environment you run Evidently in?\\n\\nIt helps us prioritize the development of new features and make sure we test the performance in the most popular environments.\\n\\nWe understand that you might still prefer not to share any telemetry data, and we respect this wish. Follow the steps above to disable the data collection.',\n",
       "  'title': 'Telemetry',\n",
       "  'description': 'What data is collected when you use Evidently open-source.',\n",
       "  'filename': 'docs-main/faq/telemetry.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"We’re building Evidently AI to help teams ship reliable AI products: whether it’s an ML model, an LLM app, or a complex agent workflow.\\n\\nOur tools are model-, framework-, and application-agnostic, so you can build and evaluate AI systems your way without limitations.\\n\\n## We are open-source\\n\\n[**Evidently**](https://github.com/evidentlyai/evidently) is an open-source library with over 25 million downloads, 5000+ GitHub stars, and a thriving community. It's licensed under Apache 2.0. This gives full transparency - you can see exactly how every metric works and trust the implementation. It also delivers an intuitive API designed for a great developer experience.\\n\\nThe **Evidently Platform** builds on the library with additional UI features and workflows for team collaboration. For enterprise users, we offer both Cloud and self-hosted options for full data privacy and control.\\n\\n## Evidently is very modular\\xa0\\n\\nEvidently is built to adapt to your needs without lock-ins or complex setups. It’s modular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\\n\\n* Start with local ad hoc checks.\\xa0\\n\\n* Want to share results? Add a UI to track evaluations over time.\\xa0\\n\\n* When you run evals, choose to upload raw data or only evaluation results. It’s up to you.\\xa0\\n\\n* Add monitoring as you are ready to move to production workflows.\\n\\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \\n\\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\\xa0\\n\\n## 100+ built-in evaluations\\n\\nEvidently puts evaluations and quality testing first.\\xa0\\n\\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML an\",\n",
       "  'title': 'Why Evidently?',\n",
       "  'description': 'Why choose Evidently.',\n",
       "  'filename': 'docs-main/faq/why_evidently.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"odular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\\n\\n* Start with local ad hoc checks.\\xa0\\n\\n* Want to share results? Add a UI to track evaluations over time.\\xa0\\n\\n* When you run evals, choose to upload raw data or only evaluation results. It’s up to you.\\xa0\\n\\n* Add monitoring as you are ready to move to production workflows.\\n\\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \\n\\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\\xa0\\n\\n## 100+ built-in evaluations\\n\\nEvidently puts evaluations and quality testing first.\\xa0\\n\\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML and LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we’ve done the hard work by implementing metrics and ways to visualize them.\\xa0You can also easily extend Evidently by adding custom metrics.\\xa0\\n\\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\\n\\n## Complete feature set\\n\\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\\n\\nGet the [Platform overview](/docs/platform/overview).\\n\\n![](/images/dashboard_llm_tabs.gif)\\n\\n## Loved by community\\n\\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\\n\\nWe’re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlya\",\n",
       "  'title': 'Why Evidently?',\n",
       "  'description': 'Why choose Evidently.',\n",
       "  'filename': 'docs-main/faq/why_evidently.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': \"d LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we’ve done the hard work by implementing metrics and ways to visualize them.\\xa0You can also easily extend Evidently by adding custom metrics.\\xa0\\n\\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\\n\\n## Complete feature set\\n\\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\\n\\nGet the [Platform overview](/docs/platform/overview).\\n\\n![](/images/dashboard_llm_tabs.gif)\\n\\n## Loved by community\\n\\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\\n\\nWe’re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlyai.com/llm-evaluations-course), open-source [ML observability course](https://www.evidentlyai.com/ml-observability-course), [guides](https://www.evidentlyai.com/mlops-guides), and [blogs](https://www.evidentlyai.com/blog).\\n\\n## Handles both ML and LLM\\xa0\\n\\nEvidently supports both ML and LLM tasks. We believe this matters even if you’re focused solely on LLMs and not training your models.\\n\\nReal-world AI systems are rarely just one thing, and two types of workflows overlap. For example:\\n\\n* an LLM-based chatbot may need **classification** steps like detecting user intent.\\n\\n* if you are building with RAG, you are solving a **ranking** problem first.\\n\\nThe Evidently Platform supports both complex nested workflows and structured tabular data, providing relevant metrics and views for each. This means you won't be locked into a single approach - or have to reinvent the wheel to measure things like Hit Rate or Precision over traces.\\n\\n## Built for collaboration\\n\\nEvidently started as an open-source pro\",\n",
       "  'title': 'Why Evidently?',\n",
       "  'description': 'Why choose Evidently.',\n",
       "  'filename': 'docs-main/faq/why_evidently.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': \"i.com/llm-evaluations-course), open-source [ML observability course](https://www.evidentlyai.com/ml-observability-course), [guides](https://www.evidentlyai.com/mlops-guides), and [blogs](https://www.evidentlyai.com/blog).\\n\\n## Handles both ML and LLM\\xa0\\n\\nEvidently supports both ML and LLM tasks. We believe this matters even if you’re focused solely on LLMs and not training your models.\\n\\nReal-world AI systems are rarely just one thing, and two types of workflows overlap. For example:\\n\\n* an LLM-based chatbot may need **classification** steps like detecting user intent.\\n\\n* if you are building with RAG, you are solving a **ranking** problem first.\\n\\nThe Evidently Platform supports both complex nested workflows and structured tabular data, providing relevant metrics and views for each. This means you won't be locked into a single approach - or have to reinvent the wheel to measure things like Hit Rate or Precision over traces.\\n\\n## Built for collaboration\\n\\nEvidently started as an open-source project loved by data scientists and AI/ML engineers. But we’re building more than a developer tool - we’re building a platform where domain experts and engineers can work together easily.\\n\\nReliable AI systems require teams to work together: on curating test data, gathering feedback, and running evaluations. We build our platform with this in mind: combine **no-code** workflows for non-technical users with an intuitive **API**. Everyone gets what they need to do their best work.\\n\\n## Trusted partner\\xa0\\n\\nFounded in 2021, Evidently AI is built by a team with 10+ years of experience deploying AI in high-scale, critical scenarios. We are backed by world-class investors like Y Combinator, Fly Ventures, Runa Capital, Nauta Capital and angel investors. Our core Evidently library has a stable history of development and earned trust from the community and enterprise users alike.\",\n",
       "  'title': 'Why Evidently?',\n",
       "  'description': 'Why choose Evidently.',\n",
       "  'filename': 'docs-main/faq/why_evidently.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'images for the changelog page',\n",
       "  'filename': 'docs-main/images/changelog/readme.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'illustrations used for the key conceptual guides (Library overview, Platform overview)',\n",
       "  'filename': 'docs-main/images/concepts/readme.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'images for dashboard panel design',\n",
       "  'filename': 'docs-main/images/dashboard/readme.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'screenshots for specific guides or cookbook examples',\n",
       "  'filename': 'docs-main/images/examples/readme.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'images for the evaluations/metrics in the docs',\n",
       "  'filename': 'docs-main/images/metrics/readme.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'images for datagen section',\n",
       "  'filename': 'docs-main/images/synthetic/readme.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Evidently helps evaluate, test, and monitor data and AI-powered systems.\\n\\n- Evidently is an **open-source Python library** with over 25 million downloads. It provides 100\\\\+ evaluation metrics, a declarative testing API, and a lightweight visual interface to explore the results.\\n- **Evidently Cloud platform** offers a complete toolkit for AI testing and observability. It includes tracing, synthetic data generation, dataset management, eval orchestration, alerting and a no-code interface for domain experts to collaborate on AI quality.\\n\\nOur goal is to help teams build and maintain reliable, high-performing AI products: from predictive ML models to complex LLM-powered systems.\\n\\n## Get started\\n\\nRun your first evaluation in a couple of minutes.\\n\\n<CardGroup cols=\"2\">\\n  <Card title=\"LLM evaluation\" icon=\"comment-text\" href=\"/quickstart_llm\">\\n    Evaluate the quality of LLM system outputs.\\n  </Card>\\n  <Card title=\"ML monitoring\" icon=\"table\" href=\"/quickstart_ml\">\\n    Test tabular data quality and data drift.\\n  </Card>\\n</CardGroup>\\n\\n## Feature overview\\n\\nWhat you can do with Evidently.\\n\\n<CardGroup cols=\"2\">\\n  <Card title=\"Evidently Platform\" href=\"/docs/platform/overview\" img=\"/images/platform_small-min.png\">\\n    Key features of the AI observability platform.\\n  </Card>\\n  <Card title=\"Evidently library\" href=\"docs/library/overview\" img=\"/images/library_small-min.png\">\\n    How the Python evaluation library works.\\n  </Card>\\n</CardGroup>\\n\\n## Learn more\\n\\n<CardGroup cols=\"2\">\\n  <Card title=\"Metrics\" icon=\"chart-simple\" href=\"/metrics\">\\n    Browse the catalogue of 100\\\\+ evaluations.\\n  </Card>\\n  <Card title=\"Cookbook\" icon=\"code\" href=\"/examples\">\\n    End-to-end code tutorials and examples.\\n  </Card>\\n</CardGroup>',\n",
       "  'title': 'What is Evidently?',\n",
       "  'description': 'Welcome to the Evidently documentation.',\n",
       "  'mode': 'wide',\n",
       "  'filename': 'docs-main/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Info>\\n  For an intro, read about [Core Concepts](/docs/library/overview) and check the [LLM Quickstart](/quickstart_llm). For a reference code example, see this [Descriptor cookbook](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/descriptors.ipynb).\\n</Info>\\n\\n## Deterministic evals\\n\\nProgrammatic and heuristics-based evaluations.\\n\\n### Pattern match\\n\\nCheck for general pattern matching.\\n\\n| Name             | Description                                                                                                                                                                                   | Parameters                                                                                                                  |\\n| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |\\n| **ExactMatch()** | <ul><li>Checks if the column contents matches between two provided columns.</li><li>Returns True/False for every input.</li><li>Example: `ExactMatch(columns=[\"answer\", \"target\"])`</li></ul> | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias` </li></ul>                                           |\\n| **RegExp()**     | <ul><li>Matches the text against a set regular expression.</li><li>Returns True/False for every input.</li><li>Example: `RegExp(reg_exp=r\"^I\")`</li></ul>                                     | **Required:** <ul><li>`reg_exp`</li></ul>**Optional:** <ul><li> `alias`</li></ul>                                           |\\n| **BeginsWith()** | <ul><li>Checks if the text starts with a given combination. </li><li>Returns True/False for every input.</li><li>Example: `BeginsWith(prefix=\"How\")`</li></ul>                                | **Required:** <ul><li',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '------------------------------------------------------------------------------------- |\\n| **ExactMatch()** | <ul><li>Checks if the column contents matches between two provided columns.</li><li>Returns True/False for every input.</li><li>Example: `ExactMatch(columns=[\"answer\", \"target\"])`</li></ul> | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias` </li></ul>                                           |\\n| **RegExp()**     | <ul><li>Matches the text against a set regular expression.</li><li>Returns True/False for every input.</li><li>Example: `RegExp(reg_exp=r\"^I\")`</li></ul>                                     | **Required:** <ul><li>`reg_exp`</li></ul>**Optional:** <ul><li> `alias`</li></ul>                                           |\\n| **BeginsWith()** | <ul><li>Checks if the text starts with a given combination. </li><li>Returns True/False for every input.</li><li>Example: `BeginsWith(prefix=\"How\")`</li></ul>                                | **Required:** <ul><li>`prefix`</li></ul>**Optional:** <ul><li>`alias`</li><li> `case_sensitive = True` or `False`</li></ul> |\\n| **EndsWith()**   | <ul><li>Checks if the text ends with a given combination.</li><li>Returns True/False for every input.</li><li>Example: `EndsWith(suffix=\"Thank you.\"`)</li></ul>                              | **Required:** <ul><li>`suffix`</li></ul>**Optional:** <ul><li>`alias`</li><li>`case_sensitive = True` or `False`</li></ul>  |\\n\\n### Content checks\\n\\nVerify presence of specific words, items or components.\\n\\n| Name                 | Description                                                                                                                                                                                                                                                                                                      | Parameters                                                                                                                                        ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '>`prefix`</li></ul>**Optional:** <ul><li>`alias`</li><li> `case_sensitive = True` or `False`</li></ul> |\\n| **EndsWith()**   | <ul><li>Checks if the text ends with a given combination.</li><li>Returns True/False for every input.</li><li>Example: `EndsWith(suffix=\"Thank you.\"`)</li></ul>                              | **Required:** <ul><li>`suffix`</li></ul>**Optional:** <ul><li>`alias`</li><li>`case_sensitive = True` or `False`</li></ul>  |\\n\\n### Content checks\\n\\nVerify presence of specific words, items or components.\\n\\n| Name                 | Description                                                                                                                                                                                                                                                                                                      | Parameters                                                                                                                                                             |\\n| -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **Contains()**       | <ul><li>Checks if the text contains **any** or **all** specified items (e.g., competitor names).</li><li>Returns True/False for every input.</li><li>Example: `Contains(items=[\"chatgpt\"])`</li></ul>                                                                                                            | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias`</li><li> `mode = any` or `all` </li><li> `case_sensitive = True`',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': '                     |\\n| -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **Contains()**       | <ul><li>Checks if the text contains **any** or **all** specified items (e.g., competitor names).</li><li>Returns True/False for every input.</li><li>Example: `Contains(items=[\"chatgpt\"])`</li></ul>                                                                                                            | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias`</li><li> `mode = any` or `all` </li><li> `case_sensitive = True` or `False`</li></ul>  |\\n| **DoesNotContain()** | <ul><li>Checks if the text does not contain the specified items (e.g., forbidden expressions).</li><li>Returns True/False for every input.</li><li>Example: `DoesNotContain(items=[\"as a large language model\"])`</li></ul>                                                                                      | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li>`mode = all` </li><li> `case_sensitive = True` or `False`</li></ul>           |\\n| **IncludesWords()**  | <ul><li>Checks if the text includes **any** or **all** specified words. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `IncludesWords(words_list=[\\'booking\\', \\'hotel\\', \\'flight\\'])`</li></ul>                                                              | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = any` or `all` </li><li> `lemmatize = Tr',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': ' or `False`</li></ul>  |\\n| **DoesNotContain()** | <ul><li>Checks if the text does not contain the specified items (e.g., forbidden expressions).</li><li>Returns True/False for every input.</li><li>Example: `DoesNotContain(items=[\"as a large language model\"])`</li></ul>                                                                                      | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li>`mode = all` </li><li> `case_sensitive = True` or `False`</li></ul>           |\\n| **IncludesWords()**  | <ul><li>Checks if the text includes **any** or **all** specified words. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `IncludesWords(words_list=[\\'booking\\', \\'hotel\\', \\'flight\\'])`</li></ul>                                                              | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = any` or `all` </li><li> `lemmatize = True` or `False`</li></ul> |\\n| **ExcludesWords()**  | <ul><li>Checks if the texts excludes all specified words (e.g. profanity lists). </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `ExcludesWords(words_list=[\\'buy\\', \\'sell\\', \\'bet\\'])`</li></ul>                                                             | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li> `lemmatize = True` or `False`</li></ul>          |\\n| **ItemMatch()**      | <ul><li>Checks if the text contains **any** or **all** specified items.</li><li> The item list is specific to each row and provided in a separate column.</li><li>Returns True/False for each row.</li><li>Example: `ItemMatch([\"Answer\", \"Expected_items\"])`</li></ul>                                          | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li> `alias` </li><li> `mode = all` or `any` </li><li> `case_sensitive = True` or',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'ue` or `False`</li></ul> |\\n| **ExcludesWords()**  | <ul><li>Checks if the texts excludes all specified words (e.g. profanity lists). </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `ExcludesWords(words_list=[\\'buy\\', \\'sell\\', \\'bet\\'])`</li></ul>                                                             | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li> `lemmatize = True` or `False`</li></ul>          |\\n| **ItemMatch()**      | <ul><li>Checks if the text contains **any** or **all** specified items.</li><li> The item list is specific to each row and provided in a separate column.</li><li>Returns True/False for each row.</li><li>Example: `ItemMatch([\"Answer\", \"Expected_items\"])`</li></ul>                                          | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li> `alias` </li><li> `mode = all` or `any` </li><li> `case_sensitive = True` or `False`</li></ul>         |\\n| **ItemNoMatch()**    | <ul><li>Checks if the text excludes **all** specified items.</li><li> The item list is specific to each row and provided in a separate column.</li><li>Returns True/False for each row.</li><li>Example: `ItemMatch([\"Answer\", \"Forbidden_items\"])`</li></ul>                                                    | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li>`case_sensitive = True` or `False`</li></ul>                    |\\n| **WordMatch()**      | <ul><li>Checks if the text includes **any** or **all** specified words. </li><li> Word list is specific to each row and provided in a separate column. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `WordMatch([\"Answer\", \"Expected_words\"]`</li></ul> | **Required:**<ul><li> `columns`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = any` or `all` </li><li> `lemmatize = True` or `Fa',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': ' `False`</li></ul>         |\\n| **ItemNoMatch()**    | <ul><li>Checks if the text excludes **all** specified items.</li><li> The item list is specific to each row and provided in a separate column.</li><li>Returns True/False for each row.</li><li>Example: `ItemMatch([\"Answer\", \"Forbidden_items\"])`</li></ul>                                                    | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li>`case_sensitive = True` or `False`</li></ul>                    |\\n| **WordMatch()**      | <ul><li>Checks if the text includes **any** or **all** specified words. </li><li> Word list is specific to each row and provided in a separate column. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `WordMatch([\"Answer\", \"Expected_words\"]`</li></ul> | **Required:**<ul><li> `columns`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = any` or `all` </li><li> `lemmatize = True` or `False`</li></ul>               |\\n| **WordNoMatch()**    | <ul><li>Checks if the text excludes **all** specified words. </li><li> Word list is specific to each row and provided in a separate column. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `WordNoMatch([\"Answer\", \"Forbidden_words\"]`</li></ul>         | **Required:** <ul><li>`columns`str</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li> `lemmatize = True` or `False`</li></ul>                     |\\n| **ContainsLink()**   | <ul><li>Checks if the column contains at least one valid URL.</li><li>Returns True/False for each row.</li></ul>                                                                                                                                                                                                 | **Optional:** <ul><li>`alias`</li></ul>                                                                                                 ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'lse`</li></ul>               |\\n| **WordNoMatch()**    | <ul><li>Checks if the text excludes **all** specified words. </li><li> Word list is specific to each row and provided in a separate column. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `WordNoMatch([\"Answer\", \"Forbidden_words\"]`</li></ul>         | **Required:** <ul><li>`columns`str</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li> `lemmatize = True` or `False`</li></ul>                     |\\n| **ContainsLink()**   | <ul><li>Checks if the column contains at least one valid URL.</li><li>Returns True/False for each row.</li></ul>                                                                                                                                                                                                 | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                |\\n\\n### Syntax validation\\n\\nValidate structured data formats or code syntax.\\n\\n| Name                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                         | Parameters                                                                                                                                                                       |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': '                               |\\n\\n### Syntax validation\\n\\nValidate structured data formats or code syntax.\\n\\n| Name                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                         | Parameters                                                                                                                                                                       |\\n| --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **IsValidJSON()**     | <ul><li>Checks if the column contains a valid JSON.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                                                           | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\\n| **JSONSche',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **IsValidJSON()**     | <ul><li>Checks if the column contains a valid JSON.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                                                           | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\\n| **JSONSchemaMatch()** | <ul><li>Checks if the column contains a valid JSON object matching the expected **schema**: all keys are present and values are not `None`. </li><li>Exact match mode checks no extra keys are present.</li><li> Optional type validation for each key. </li><li>Returns True/False for each input.</li><li>Example: `JSONSchemaMatch(expected_schema={\"name\": str, \"age\": int}, exact_match=False, validate_types=True)`</li></ul> | **Required:** <ul><li>`expected_schema: Dict[str, type]`</li></ul> **Optional:** <ul><li> `exact_match = True` or `False` </li><li> `validate_types = True` or `False`</li></ul> |\\n| **JSONMatch()**       | <ul><li>Checks if the column contains a valid JSON object matching a JSON provided in a reference column. </li><li>Matches **key-value pairs** irrespective of order.</li><li>Whitespace outside of the actual values (e.g., spaces or newlines) is ignored.</li><li>Returns True/False for every input.</li><li>Example: `JSONMatch(first_column=\"Json1\", second_colum',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'maMatch()** | <ul><li>Checks if the column contains a valid JSON object matching the expected **schema**: all keys are present and values are not `None`. </li><li>Exact match mode checks no extra keys are present.</li><li> Optional type validation for each key. </li><li>Returns True/False for each input.</li><li>Example: `JSONSchemaMatch(expected_schema={\"name\": str, \"age\": int}, exact_match=False, validate_types=True)`</li></ul> | **Required:** <ul><li>`expected_schema: Dict[str, type]`</li></ul> **Optional:** <ul><li> `exact_match = True` or `False` </li><li> `validate_types = True` or `False`</li></ul> |\\n| **JSONMatch()**       | <ul><li>Checks if the column contains a valid JSON object matching a JSON provided in a reference column. </li><li>Matches **key-value pairs** irrespective of order.</li><li>Whitespace outside of the actual values (e.g., spaces or newlines) is ignored.</li><li>Returns True/False for every input.</li><li>Example: `JSONMatch(first_column=\"Json1\", second_column=\"Json2\"),`</li></ul>                                       | **Required:** <ul><li>`first_column`</li><li>`second_column`</li></ul>**Optional:** <ul><li>`alias`</li></ul>                                                                    |\\n| **IsValidPython()**   | <ul><li>Checks if the column contains valid Python code without syntax errors.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                                | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\\n| **IsValidSQL()**      | <ul><li>Checks if the column contains a valid SQL query without executing the query.</li><li>Returns True',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'n=\"Json2\"),`</li></ul>                                       | **Required:** <ul><li>`first_column`</li><li>`second_column`</li></ul>**Optional:** <ul><li>`alias`</li></ul>                                                                    |\\n| **IsValidPython()**   | <ul><li>Checks if the column contains valid Python code without syntax errors.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                                | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\\n| **IsValidSQL()**      | <ul><li>Checks if the column contains a valid SQL query without executing the query.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                          | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\\n\\n### Text stats\\n\\nDescriptive text statistics.\\n\\n| Name                               | Descriptor                                                                                                                                              | Parameters                                                                 |\\n| :--------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': '/False for every input.</li></ul>                                                                                                                                                                                                                                                                                          | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\\n\\n### Text stats\\n\\nDescriptive text statistics.\\n\\n| Name                               | Descriptor                                                                                                                                              | Parameters                                                                 |\\n| :--------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |\\n| **TextLength()**                   | <ul><li>Measures the length of the text in symbols.</li><li>Returns an absolute number.</li></ul>                                                       | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n| **OOVWordsPercentage()**           | <ul><li>Calculates the percentage of out-of-vocabulary words based on imported NLTK vocabulary.</li><li>Returns a score on a scale: 0 to 100.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`ignore_words: Tuple = ()`</li></ul> |\\n| **NonLetterCharacterPercentage()** | <ul><li>Calculates the percentage of non-letter characters.</li><li>Returns a score on a scale: 0 to 100.</li></ul>                                     | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n| **SentenceCount()**                | <ul><li>Counts the number of sentences in the text.</li><li>Returns ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': '--- | -------------------------------------------------------------------------- |\\n| **TextLength()**                   | <ul><li>Measures the length of the text in symbols.</li><li>Returns an absolute number.</li></ul>                                                       | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n| **OOVWordsPercentage()**           | <ul><li>Calculates the percentage of out-of-vocabulary words based on imported NLTK vocabulary.</li><li>Returns a score on a scale: 0 to 100.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`ignore_words: Tuple = ()`</li></ul> |\\n| **NonLetterCharacterPercentage()** | <ul><li>Calculates the percentage of non-letter characters.</li><li>Returns a score on a scale: 0 to 100.</li></ul>                                     | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n| **SentenceCount()**                | <ul><li>Counts the number of sentences in the text.</li><li>Returns an absolute number.</li></ul>                                                       | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n| **WordCount()**                    | <ul><li>Counts the number of words in the text.</li><li>Returns an absolute number.</li></ul>                                                           | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n\\n### Custom\\n\\nImplement your own programmatic checks.\\n\\n| Name                          | Descriptor                                                                                                                                                                                                                                                             | Parameters                                                                                                                            |\\n| :---------------------------- | --------------------------------------------',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': 'an absolute number.</li></ul>                                                       | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n| **WordCount()**                    | <ul><li>Counts the number of words in the text.</li><li>Returns an absolute number.</li></ul>                                                           | **Optional:** <ul><li>`alias`</li></ul>                                    |\\n\\n### Custom\\n\\nImplement your own programmatic checks.\\n\\n| Name                          | Descriptor                                                                                                                                                                                                                                                             | Parameters                                                                                                                            |\\n| :---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\\n| **CustomDescriptor()**        | <ul><li>Implements a custom check for specific column(s) as a Python function.</li><li>Use it to run your own programmatic checks. </li><li>Returns score and/or label as specified.</li><li>Can accept and return multiple columns. </li></ul>                        | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |\\n| **CustomColumnsDescriptor()** | <ul><li>Implements a custom check as a Python function that can be applied to any column in the dataset.</li><li>Use it to run your own programmatic checks. </li><li>Returns ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\\n| **CustomDescriptor()**        | <ul><li>Implements a custom check for specific column(s) as a Python function.</li><li>Use it to run your own programmatic checks. </li><li>Returns score and/or label as specified.</li><li>Can accept and return multiple columns. </li></ul>                        | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |\\n| **CustomColumnsDescriptor()** | <ul><li>Implements a custom check as a Python function that can be applied to any column in the dataset.</li><li>Use it to run your own programmatic checks. </li><li>Returns score and/or label as specified.</li><li>Accepts and returns a single column. </li></ul> | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |\\n\\n## LLM-based evals\\n\\nUsing an external LLMs with an evaluation prompt. You can specify the LLM to use as an evaluator.\\n\\n### Custom\\n\\nLLM judge templates.\\n\\n| Name          | Descriptor                                                                                                                                                                                                   | Parameters                                                                                                                                                                                                |\\n| :------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': 'score and/or label as specified.</li><li>Accepts and returns a single column. </li></ul> | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |\\n\\n## LLM-based evals\\n\\nUsing an external LLMs with an evaluation prompt. You can specify the LLM to use as an evaluator.\\n\\n### Custom\\n\\nLLM judge templates.\\n\\n| Name          | Descriptor                                                                                                                                                                                                   | Parameters                                                                                                                                                                                                |\\n| :------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **LLMEval()** | <ul><li>Scores the text using user-defined criteria.</li><li>You must specify provider, model and use prompt template to formulate the criteria. </li><li>Returns score and/or label as specified.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`template`</li><li>`provider`</li><li>`model`</li><li>`additional_columns: dict`</li><li>See [custom LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\\n\\n### RAG\\n\\nRAG-specific evals for retrieval and generation. ([Tutorial](/examples/LLM_rag_evals)).\\n\\n| Name                        | Descriptor                                                                                                                                                                                               ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': '-------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **LLMEval()** | <ul><li>Scores the text using user-defined criteria.</li><li>You must specify provider, model and use prompt template to formulate the criteria. </li><li>Returns score and/or label as specified.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`template`</li><li>`provider`</li><li>`model`</li><li>`additional_columns: dict`</li><li>See [custom LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\\n\\n### RAG\\n\\nRAG-specific evals for retrieval and generation. ([Tutorial](/examples/LLM_rag_evals)).\\n\\n| Name                        | Descriptor                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Parameters                                                                                                                                                                                                                                                                                                                                          |\\n| :-------------------------- | -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': '                                                                                                                                                                                                                                                                         | Parameters                                                                                                                                                                                                                                                                                                                                          |\\n| :-------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **ContextQualityLLMEval()** | <ul><li>Evaluates if the context provides sufficient information to answer the question.</li><li>Returns a label (VALID or INVALID) or a score.</li><li>Run over the \"context\" column and pass the `question` column as a parameter.</li><li>Example: `ContextQualityLLMEval(\"Context\", question=\"Question\")`</li></ul>                                                                                                                                                           |  **Required:** <ul><li>`question`</li></ul> **Opt',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': '---------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **ContextQualityLLMEval()** | <ul><li>Evaluates if the context provides sufficient information to answer the question.</li><li>Returns a label (VALID or INVALID) or a score.</li><li>Run over the \"context\" column and pass the `question` column as a parameter.</li><li>Example: `ContextQualityLLMEval(\"Context\", question=\"Question\")`</li></ul>                                                                                                                                                           |  **Required:** <ul><li>`question`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                                                     |\\n| **ContextRelevance()**      | <ul><li>Checks if the context is relevant to the given question (0 to 1) for multiple context chunks.</li><li>Pass all context chunks as a list in the `context` column.</li><li>Uses semantic similarity (default) or LLM. </li><li>Aggregates relevance: `mean` (default) or `hit` (at least one chunk is relevant). </li><li>Example: `ContextRelevance(\"Question\", \"Context\", output_scores=True, aggregation_method=\"hit\", method=\"llm\")`</li></ul> | **Required:** <ul><li>`input`</li><li>`contexts`</li></ul> **Optional:** <ul><li>`output_scores`: `False` or `True` </li><li>`method`: `semantic_similarity` or `llm`</li><li>`aggregation_method`: `mean` or `',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': 'ional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                                                     |\\n| **ContextRelevance()**      | <ul><li>Checks if the context is relevant to the given question (0 to 1) for multiple context chunks.</li><li>Pass all context chunks as a list in the `context` column.</li><li>Uses semantic similarity (default) or LLM. </li><li>Aggregates relevance: `mean` (default) or `hit` (at least one chunk is relevant). </li><li>Example: `ContextRelevance(\"Question\", \"Context\", output_scores=True, aggregation_method=\"hit\", method=\"llm\")`</li></ul> | **Required:** <ul><li>`input`</li><li>`contexts`</li></ul> **Optional:** <ul><li>`output_scores`: `False` or `True` </li><li>`method`: `semantic_similarity` or `llm`</li><li>`aggregation_method`: `mean` or `hit` </li><li>`aggregation_method_params={\"threshold\":0.95}` (set the relevance threshold as greater or equal, 0.8 by default)</li><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\\n| **FaithfulnessLLMEval()**   | <ul><li>Assesses whether the response stays faithful to the given context.Checks for hallucinations or unsupported claims.</li><li>Returns a label (FAITHFUL or UNFAITHFUL) or a score.</li><li>Run over the \"response\" column and pass the `context` column as a parameter.</li><li>Example: `FaithfulnessLLMEval(\"Response\", context=\"Context\")`</li></ul>                                                                                                             | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                       ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 21000,\n",
       "  'chunk': 'hit` </li><li>`aggregation_method_params={\"threshold\":0.95}` (set the relevance threshold as greater or equal, 0.8 by default)</li><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\\n| **FaithfulnessLLMEval()**   | <ul><li>Assesses whether the response stays faithful to the given context.Checks for hallucinations or unsupported claims.</li><li>Returns a label (FAITHFUL or UNFAITHFUL) or a score.</li><li>Run over the \"response\" column and pass the `context` column as a parameter.</li><li>Example: `FaithfulnessLLMEval(\"Response\", context=\"Context\")`</li></ul>                                                                                                             | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                              |\\n| **CompletenessLLMEval()**   | <ul><li>Determines whether the response fully uses the information provided in the context.</li><li>Returns a label (COMPLETE or INCOMPLETE) or a score.</li><li>Run over the \"response\" column and pass the `context` column as a parameter.</li><li>Example: `CompletenessLLMEval(\"Response\", context=\"Context\")`</li></ul>                                                                                                                                                     | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                              |\\n\\n### Generation\\n\\nEvals for varied generation scenarios.\\n\\n| Name      ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 22000,\n",
       "  'chunk': '                                                                                       |\\n| **CompletenessLLMEval()**   | <ul><li>Determines whether the response fully uses the information provided in the context.</li><li>Returns a label (COMPLETE or INCOMPLETE) or a score.</li><li>Run over the \"response\" column and pass the `context` column as a parameter.</li><li>Example: `CompletenessLLMEval(\"Response\", context=\"Context\")`</li></ul>                                                                                                                                                     | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                              |\\n\\n### Generation\\n\\nEvals for varied generation scenarios.\\n\\n| Name                     | Descriptor                                                                                                                                                                                                                                                                                                                                                                                                          | Parameters                                                                                                                                                                                   |\\n| :----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 23000,\n",
       "  'chunk': '               | Descriptor                                                                                                                                                                                                                                                                                                                                                                                                          | Parameters                                                                                                                                                                                   |\\n| :----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **CorrectnessLLMEval()** | <ul><li>Evaluates the correctness of a response by comparing it with the target output.</li><li>Useful for RAG or any LLM generation where you have a ground truth output</li><li>Returns a label (CORRECT or INCORRECT) or a score.</li><li>Run over the \"response\" column and pass the `target_output` column as a parameter.</li><li>Example: `CorrectnessLLMEval(\"Response\", target_output=\"Target\")`</li></ul> | **Required:** <ul><li>`target_output`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\\n| **DeclineLLMEval()**     | <ul><li>Detects if the text contains a refusal or rejection. </li><li>Useful to detect instances where an',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 24000,\n",
       "  'chunk': '---------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **CorrectnessLLMEval()** | <ul><li>Evaluates the correctness of a response by comparing it with the target output.</li><li>Useful for RAG or any LLM generation where you have a ground truth output</li><li>Returns a label (CORRECT or INCORRECT) or a score.</li><li>Run over the \"response\" column and pass the `target_output` column as a parameter.</li><li>Example: `CorrectnessLLMEval(\"Response\", target_output=\"Target\")`</li></ul> | **Required:** <ul><li>`target_output`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\\n| **DeclineLLMEval()**     | <ul><li>Detects if the text contains a refusal or rejection. </li><li>Useful to detect instances where an LLM denies the user response.</li><li>Returns a label (DECLINE or OK) or a score.</li></ul>                                                                                                                                                                                                               | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **PIILLMEval()**         | <ul><li>Detects texts containing PII (Personally Identifiable Information).</li><li>Returns a label (PII or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                               | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 25000,\n",
       "  'chunk': ' LLM denies the user response.</li><li>Returns a label (DECLINE or OK) or a score.</li></ul>                                                                                                                                                                                                               | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **PIILLMEval()**         | <ul><li>Detects texts containing PII (Personally Identifiable Information).</li><li>Returns a label (PII or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                               | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **NegativityLLMEval()**  | <ul><li>Detects negative texts.</li><li>Returns a label (NEGATIVE or POSITIVE) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **BiasLLMEval()**        | <ul><li>Detects biased texts.</li><li>Returns a label (BIAS or OK) or a score.</li></ul>                                                                                                                                           ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 26000,\n",
       "  'chunk': 'See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **NegativityLLMEval()**  | <ul><li>Detects negative texts.</li><li>Returns a label (NEGATIVE or POSITIVE) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **BiasLLMEval()**        | <ul><li>Detects biased texts.</li><li>Returns a label (BIAS or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                            | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **ToxicityLLMEval()**    | <ul><li>Detects toxic texts.</li><li>Returns a label (TOXICITY or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                         | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n\\n## ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 27000,\n",
       "  'chunk': '                                                                                                                                                                                 | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n| **ToxicityLLMEval()**    | <ul><li>Detects toxic texts.</li><li>Returns a label (TOXICITY or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                         | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |\\n\\n## ML-based evals\\n\\nUse pre-trained machine learning or embedding models.\\n\\n| Name                      | Descriptor                                                                                                                                                                                                                                                                                                                                                                             | Parameters                                                                                                                           |\\n| :------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 28000,\n",
       "  'chunk': 'ML-based evals\\n\\nUse pre-trained machine learning or embedding models.\\n\\n| Name                      | Descriptor                                                                                                                                                                                                                                                                                                                                                                             | Parameters                                                                                                                           |\\n| :------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\\n| **SemanticSimilarity()**  | <ul><li>Calculates pairwise semantic similarity (Cosine Similarity) between two columns using a sentence embeddings model [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).</li><li>Returns a score from 0 to 1: (0: different, 0.5: unrelated, 1: identical) </li><li>Example use: `SemanticSimilarity(columns=[\"Question\", \"Answer\"])`.</li></ul> | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li> `alias`</li></ul>                                                    |\\n| **BERTScore()**           | <ul><li>Calculates similarity between two text columns based on token embeddings.</li><li>Returns [BERTScore](https://arxiv.org/pdf/1904.09675) (F1 Score).</li><li>Example use: `BERTScore(columns=[\"Answer\", \"Target\"])`.</li></ul>                                            ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 29000,\n",
       "  'chunk': '----------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\\n| **SemanticSimilarity()**  | <ul><li>Calculates pairwise semantic similarity (Cosine Similarity) between two columns using a sentence embeddings model [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).</li><li>Returns a score from 0 to 1: (0: different, 0.5: unrelated, 1: identical) </li><li>Example use: `SemanticSimilarity(columns=[\"Question\", \"Answer\"])`.</li></ul> | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li> `alias`</li></ul>                                                    |\\n| **BERTScore()**           | <ul><li>Calculates similarity between two text columns based on token embeddings.</li><li>Returns [BERTScore](https://arxiv.org/pdf/1904.09675) (F1 Score).</li><li>Example use: `BERTScore(columns=[\"Answer\", \"Target\"])`.</li></ul>                                                                                                                                                  | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`model`</li><li>`tfidf_weighted`</li><li>`alias`</li></ul>            |\\n| **Sentiment()**           | <ul><li>Analyzes text sentiment using a word-based model from NLTK.</li><li>Returns a score: -1 (negative) to 1 (positive).</li></ul>                                                                                                                                                                                                                                                            | **Optional:** <ul><li>`alias`</li></ul>                                                                                              |\\n| **HuggingFace()**         | <ul><li>Scores the text using a user-selected HuggingFace model.</li><li>See [HuggingFace descriptor docs](/metrics/customize_hf_descriptor) for example models.</li></ul>         ',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 30000,\n",
       "  'chunk': '                                                                                                      | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`model`</li><li>`tfidf_weighted`</li><li>`alias`</li></ul>            |\\n| **Sentiment()**           | <ul><li>Analyzes text sentiment using a word-based model from NLTK.</li><li>Returns a score: -1 (negative) to 1 (positive).</li></ul>                                                                                                                                                                                                                                                            | **Optional:** <ul><li>`alias`</li></ul>                                                                                              |\\n| **HuggingFace()**         | <ul><li>Scores the text using a user-selected HuggingFace model.</li><li>See [HuggingFace descriptor docs](/metrics/customize_hf_descriptor) for example models.</li></ul>                                                                                                                                                                                                             | **Optional:** <ul><li>`alias`</li><li>See [docs](/metrics/customize_hf_descriptor).</li></ul>                                        |\\n| **HuggingFaceToxicity()** | <ul><li>Detects hate speech using a [`roberta-hate-speech`](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) model.</li><li>Returns predicted probability for the “hate” label. Scale: 0 to 1.</li></ul>                                                                                                                                                       | **Optional:** <ul><li>`toxic_label`(default: `hate`)</li><li>`alias`</li><li>See [docs](/metrics/customize_hf_descriptor).</li></ul> |',\n",
       "  'title': 'All Descriptors',\n",
       "  'description': 'Reference page for all row-level text and LLM evals.',\n",
       "  'filename': 'docs-main/metrics/all_descriptors.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Info>\\n  For an intro, read [Core Concepts](/docs/library/overview) and check quickstarts for [LLMs](docs/quickstart_llm) or [ML](docs/quickstart_ml). For a reference code example, see this [Metric cookbook](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb).\\n</Info>\\n\\n<Accordion title=\"How to read the tables\" defaultOpen={false}>\\n  * **Metric**: the name of Metric or Preset you can pass to `Report`.\\n\\n  * **Description:** what it does. Complex Metrics link to explainer pages.\\n\\n  * **Parameters:** available options. You can also add conditional `tests` to any Metric with standard operators like `eq` (equal), `gt` (greater than), etc. [How Tests work](/docs/library/tests).\\n\\n  * **Test defaults** are conditions that apply when you invoke Tests but do not set a pass/fail condition yourself.\\n\\n    * **With reference**: if you provide a reference dataset during the Report `run`, the conditions are set relative to reference.\\n\\n    * **No reference**: if you do not provide a reference, Tests will use fixed heuristics (like expect no missing values).\\n</Accordion>\\n\\n## Text Evals\\n\\nSummarizes results of text or LLM evals. To score individual inputs, first use [descriptors](/metrics/all_descriptors).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map text columns.\\n</Info>\\n\\n| Metric          | Description                                                                                                                                                                                                                                                                                             | Parameters                                 | Test Defaults                          |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 't provide a reference, Tests will use fixed heuristics (like expect no missing values).\\n</Accordion>\\n\\n## Text Evals\\n\\nSummarizes results of text or LLM evals. To score individual inputs, first use [descriptors](/metrics/all_descriptors).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map text columns.\\n</Info>\\n\\n| Metric          | Description                                                                                                                                                                                                                                                                                             | Parameters                                 | Test Defaults                          |\\n| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ | -------------------------------------- |\\n| **TextEvals()** | <ul><li>Large Preset.</li><li>Shows `ValueStats` for all descriptors.</li><li>You must specify descriptors ([see how](/docs/library/descriptors) and [all descriptors](/metrics/all_descriptors)).</li><li>Metric result: for all Metrics.</li><li>[Preset page](/metrics/preset_text_evals).</li></ul> | **Optional**:  <ul><li>`columns`</li></ul> | As in Metrics included in `ValueStats` |\\n\\n## Columns\\n\\nUse to aggregate descriptor results or check data quality on column level.\\n\\n<Info>\\n  You may need to map column types using [Data definition](/docs/library/data_definition).\\n</Info>\\n\\n### Value stats\\n\\nDescriptive statistics.\\n\\n| Metric                                                                                                         | Description                                                                                    ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '----------------------------------------------------------- | ------------------------------------------ | -------------------------------------- |\\n| **TextEvals()** | <ul><li>Large Preset.</li><li>Shows `ValueStats` for all descriptors.</li><li>You must specify descriptors ([see how](/docs/library/descriptors) and [all descriptors](/metrics/all_descriptors)).</li><li>Metric result: for all Metrics.</li><li>[Preset page](/metrics/preset_text_evals).</li></ul> | **Optional**:  <ul><li>`columns`</li></ul> | As in Metrics included in `ValueStats` |\\n\\n## Columns\\n\\nUse to aggregate descriptor results or check data quality on column level.\\n\\n<Info>\\n  You may need to map column types using [Data definition](/docs/library/data_definition).\\n</Info>\\n\\n### Value stats\\n\\nDescriptive statistics.\\n\\n| Metric                                                                                                         | Description                                                                                                                                                                                                                                 | Parameters                                                                                                                                        | Test Defaults                                                                                                                        |\\n| -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': '                                                                                                                                             | Parameters                                                                                                                                        | Test Defaults                                                                                                                        |\\n| -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\\n| **ValueStats()**                                                                                               | <ul><li>Small Preset, column-level. </li><li>Computes various descriptive stats. Included Metrics: `UniqueValueCount`, `MissingValueCount`, `MinValue`, `MaxValue`, `MeanValue`, `StdValue`, `QuantileValue` (0.25, 0.5, 0.75).</li><li>Returns different stats based on the column type.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. As in individual Metrics.</li><li>**With reference**. As in indiviudal Metrics.</li></ul>                  |\\n| **MinValue()**                                                                                                 | <ul><li>Column-level.</li><li>Returns min value for a given numerical column.</li><li>Metric result: `value`.</li></ul>            ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '----------------------------------------------------------- |\\n| **ValueStats()**                                                                                               | <ul><li>Small Preset, column-level. </li><li>Computes various descriptive stats. Included Metrics: `UniqueValueCount`, `MissingValueCount`, `MinValue`, `MaxValue`, `MeanValue`, `StdValue`, `QuantileValue` (0.25, 0.5, 0.75).</li><li>Returns different stats based on the column type.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. As in individual Metrics.</li><li>**With reference**. As in indiviudal Metrics.</li></ul>                  |\\n| **MinValue()**                                                                                                 | <ul><li>Column-level.</li><li>Returns min value for a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                                     | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if Min Value is differs by more than 10% (+/-).</li></ul>           |\\n| **StdValue()**                                                                                                 | <ul><li>Column-level.</li><li>Computes the standard deviation of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                        | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the standard deviation differs by more th',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': '                                                                                                         | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if Min Value is differs by more than 10% (+/-).</li></ul>           |\\n| **StdValue()**                                                                                                 | <ul><li>Column-level.</li><li>Computes the standard deviation of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                        | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the standard deviation differs by more than 10% (+/-).</li></ul> |\\n| **MeanValue()**                                                                                                | <ul><li>Column-level.</li><li>Computes the mean value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                                | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the mean value differs by more than 10%.</li></ul>               |\\n| **MaxValue()**                                                                                                 | <ul><li>Column-level.</li><li>Computes the max value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                    ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'an 10% (+/-).</li></ul> |\\n| **MeanValue()**                                                                                                | <ul><li>Column-level.</li><li>Computes the mean value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                                | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the mean value differs by more than 10%.</li></ul>               |\\n| **MaxValue()**                                                                                                 | <ul><li>Column-level.</li><li>Computes the max value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                                 | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the max value is higher than in the reference.</li></ul>         |\\n| **MedianValue()**                                                                                              | <ul><li>Column-level.</li><li>Computes the median value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                              | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the median value differs by more than 10% (+/-).</li></ul>       |\\n| **QuantileValue()**                                             ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': '             | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the max value is higher than in the reference.</li></ul>         |\\n| **MedianValue()**                                                                                              | <ul><li>Column-level.</li><li>Computes the median value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                              | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the median value differs by more than 10% (+/-).</li></ul>       |\\n| **QuantileValue()**                                                                                            | <ul><li>Column-level.</li><li>Computes the quantile value of a given numerical column.</li><li>Defaults to 0.5 if no quantile is specified.</li><li>Metric result: `value`.</li></ul>                                                       | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>`quantile` (default: 0.5)</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if quantile value differs by more than 10% (+/-).</li></ul>         |\\n| **CategoryCount()** <br /><br /> Example: <br /> `CategoryCount(`<br />`column=\"city\",`<br />` category=\"NY\")` | <ul><li>Column-level.</li><li>Counts occurrences of the specified category or categories. </li><li> To check the joint share of several categories, pass the list `categories=[\"a\", \"b\"]`.</li><li>Metric result: `count`, `share`.</li></ul>                                                                              ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': '                                               | <ul><li>Column-level.</li><li>Computes the quantile value of a given numerical column.</li><li>Defaults to 0.5 if no quantile is specified.</li><li>Metric result: `value`.</li></ul>                                                       | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>`quantile` (default: 0.5)</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if quantile value differs by more than 10% (+/-).</li></ul>         |\\n| **CategoryCount()** <br /><br /> Example: <br /> `CategoryCount(`<br />`column=\"city\",`<br />` category=\"NY\")` | <ul><li>Column-level.</li><li>Counts occurrences of the specified category or categories. </li><li> To check the joint share of several categories, pass the list `categories=[\"a\", \"b\"]`.</li><li>Metric result: `count`, `share`.</li></ul>                                                                                                              | **Required**: <ul><li>`column`</li><li>`category`</li><li>`categories`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the specified category is not present.</li></ul>                 |\\n\\n### Column data quality\\n\\nColumn-level data quality metrics.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types.\\n</Info>\\n\\n| Metric                                                                                                                         | Description                                                                                                                                          | Parameters                                                                                                                                     | Test Defaults                                                          ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '                                | **Required**: <ul><li>`column`</li><li>`category`</li><li>`categories`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the specified category is not present.</li></ul>                 |\\n\\n### Column data quality\\n\\nColumn-level data quality metrics.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types.\\n</Info>\\n\\n| Metric                                                                                                                         | Description                                                                                                                                          | Parameters                                                                                                                                     | Test Defaults                                                                                                                                                      |\\n| ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| **MissingValueCount()**                                                                                                        | <ul><li>Column-level.</li><li>Counts the number and share of missing values.</li><li>Metric result: `count`, `share`.</li></ul>                      | **Required**: <ul><li>`column`</',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': '                                                                                            |\\n| ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| **MissingValueCount()**                                                                                                        | <ul><li>Column-level.</li><li>Counts the number and share of missing values.</li><li>Metric result: `count`, `share`.</li></ul>                      | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if there are missing values.</li><li>**With reference**: Fails if share of missing values is >10% higher.</li></ul>                |\\n| **InRangeValueCount()**  <br /><br /> Example: <br /> `InRangeValueCount(`<br />`column=\"age\",`<br />`left=\"1\", right=\"18\")`   | <ul><li>Column-level.</li><li>Counts the number and share of values in the set range.</li><li>Metric result: `count`, `share`.</li></ul>             | **Required**: <ul><li>`column`</li><li>`left`</li><li>`right`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if column contains values out of the min-max reference range.</li></ul>                           |\\n| **OutRangeValueCount()**                                                                                                       |',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if there are missing values.</li><li>**With reference**: Fails if share of missing values is >10% higher.</li></ul>                |\\n| **InRangeValueCount()**  <br /><br /> Example: <br /> `InRangeValueCount(`<br />`column=\"age\",`<br />`left=\"1\", right=\"18\")`   | <ul><li>Column-level.</li><li>Counts the number and share of values in the set range.</li><li>Metric result: `count`, `share`.</li></ul>             | **Required**: <ul><li>`column`</li><li>`left`</li><li>`right`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if column contains values out of the min-max reference range.</li></ul>                           |\\n| **OutRangeValueCount()**                                                                                                       | <ul><li>Column-level.</li><li>Counts the number and share of values out of the set range.</li><li>Metric result: `count`, `share`.</li></ul>         | **Required**: <ul><li>`column`</li><li>`left`</li><li>`right`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of min-max reference range.</li></ul>                                         |\\n| **InListValueCount()**                                                                                                         | <ul><li>Column-level.</li><li>Counts the number and share of values in the set list.</li><li>Metric result: `count`, `share`.</li></ul>              | **Required**: <ul><li>`column`</li><li>`values`</li></ul>**Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of list.</li></ul>        ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': ' <ul><li>Column-level.</li><li>Counts the number and share of values out of the set range.</li><li>Metric result: `count`, `share`.</li></ul>         | **Required**: <ul><li>`column`</li><li>`left`</li><li>`right`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of min-max reference range.</li></ul>                                         |\\n| **InListValueCount()**                                                                                                         | <ul><li>Column-level.</li><li>Counts the number and share of values in the set list.</li><li>Metric result: `count`, `share`.</li></ul>              | **Required**: <ul><li>`column`</li><li>`values`</li></ul>**Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of list.</li></ul>                                                            |\\n| **OutListValueCount()**  <br /><br /> Example: <br /> `OutListValueCount(`<br />`column=\"city\",`<br />` values=[\"Lon\", \"NY\"])` | <ul><li>Column-level.</li><li>Counts the number and share of values out of the set list.</li><li>Metric result: `count`, `share`.</li></ul>          | **Required**: <ul><li>`column`</li><li>`values`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>               | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of list.</li></ul>                                                            |\\n| **UniqueValueCount()**                                                                                                         | <ul><li>Column-level.</li><li>Counts the number and share of unique values.</li><li>Metric result: `values` (dict with `count, share`).</li></ul>    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test con',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': '                                                    |\\n| **OutListValueCount()**  <br /><br /> Example: <br /> `OutListValueCount(`<br />`column=\"city\",`<br />` values=[\"Lon\", \"NY\"])` | <ul><li>Column-level.</li><li>Counts the number and share of values out of the set list.</li><li>Metric result: `count`, `share`.</li></ul>          | **Required**: <ul><li>`column`</li><li>`values`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>               | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of list.</li></ul>                                                            |\\n| **UniqueValueCount()**                                                                                                         | <ul><li>Column-level.</li><li>Counts the number and share of unique values.</li><li>Metric result: `values` (dict with `count, share`).</li></ul>    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if the share of unique values differs by >10% (+/-).</li></ul>                                    |\\n\\n## Dataset\\n\\nUse for exploratory data analysis and data quality checks.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.\\n</Info>\\n\\n### Dataset stats\\n\\nDescriptive statistics.\\n\\n| Metric                  | Description                                                                                                                                                                                                         | Parameters                                                             | Test Defaults                                                                                                  |\\n| ----------------------- | ---------------------------------------------------------------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': 'ditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if the share of unique values differs by >10% (+/-).</li></ul>                                    |\\n\\n## Dataset\\n\\nUse for exploratory data analysis and data quality checks.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.\\n</Info>\\n\\n### Dataset stats\\n\\nDescriptive statistics.\\n\\n| Metric                  | Description                                                                                                                                                                                                         | Parameters                                                             | Test Defaults                                                                                                  |\\n| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |\\n| **DataSummaryPreset()** | <ul><li>Large Preset.</li><li>Combines `DatasetStats` and `ValueStats` for all or specified columns.</li><li>Metric result: for all Metrics.</li><li>[Preset page](/metrics/preset_data_summary)</li></ul>          | **Optional**: <ul><li>`columns`</li></ul>                              | As in individual Metrics.                                                                                      |\\n| **DatasetStats()**      | <ul><li>Small preset. </li><li> Dataset-level.</li><li>Calculates descriptive dataset stats, including columns by type, rows, missing values, empty columns, etc.</li><li>Metric result: for all Metrics.</li></ul> |',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': '---------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |\\n| **DataSummaryPreset()** | <ul><li>Large Preset.</li><li>Combines `DatasetStats` and `ValueStats` for all or specified columns.</li><li>Metric result: for all Metrics.</li><li>[Preset page](/metrics/preset_data_summary)</li></ul>          | **Optional**: <ul><li>`columns`</li></ul>                              | As in individual Metrics.                                                                                      |\\n| **DatasetStats()**      | <ul><li>Small preset. </li><li> Dataset-level.</li><li>Calculates descriptive dataset stats, including columns by type, rows, missing values, empty columns, etc.</li><li>Metric result: for all Metrics.</li></ul> | None                                                                   | <ul><li>**No reference**: As in included Metrics</li><li>**With reference**: As in included Metrics.</li></ul> |\\n| **RowCount()**          | <ul><li> Dataset-level.</li><li>Counts the number of rows.</li><li>Metric result: `value`.</li></ul>                                                                                                                | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if row count differs by >10%.</li></ul>       |\\n| **ColumnCount()**       | <ul><li> Dataset-level.</li><li>Counts the number of columns.</li><li>Metric result:  `value`.</li></ul>                                                                                                            | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if no',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': ' None                                                                   | <ul><li>**No reference**: As in included Metrics</li><li>**With reference**: As in included Metrics.</li></ul> |\\n| **RowCount()**          | <ul><li> Dataset-level.</li><li>Counts the number of rows.</li><li>Metric result: `value`.</li></ul>                                                                                                                | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if row count differs by >10%.</li></ul>       |\\n| **ColumnCount()**       | <ul><li> Dataset-level.</li><li>Counts the number of columns.</li><li>Metric result:  `value`.</li></ul>                                                                                                            | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if not equal to reference.</li></ul>          |\\n\\n### Dataset data quality\\n\\nDataset-level data quality metrics.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.\\n</Info>\\n\\n| Metric                                          | Description                                                                                                                                                                            | Parameters                                                                                                      | Test Defaults                                                                                                                                                          |\\n| ----------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': 't equal to reference.</li></ul>          |\\n\\n### Dataset data quality\\n\\nDataset-level data quality metrics.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.\\n</Info>\\n\\n| Metric                                          | Description                                                                                                                                                                            | Parameters                                                                                                      | Test Defaults                                                                                                                                                          |\\n| ----------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **ConstantColumnsCount()**                      | <ul><li> Dataset-level.</li><li>Counts the number of constant columns.</li><li>Metric result: `value`.</li></ul>                                                                       | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one constant column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>            |\\n| **EmptyRowsCount()**                            | <ul><li> Dataset-level.</li><li>Counts the number of empty rows.</li><li>Metric result: `value`.</li></ul>                                                       ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': '----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **ConstantColumnsCount()**                      | <ul><li> Dataset-level.</li><li>Counts the number of constant columns.</li><li>Metric result: `value`.</li></ul>                                                                       | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one constant column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>            |\\n| **EmptyRowsCount()**                            | <ul><li> Dataset-level.</li><li>Counts the number of empty rows.</li><li>Metric result: `value`.</li></ul>                                                                             | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one empty row.</li><li>**With reference**: Fails if share differs by >10%.</li></ul>                              |\\n| **EmptyColumnsCount()**                         | <ul><li> Dataset-level.</li><li>Counts the number of empty columns.</li><li>Metric result: `value`.</li></ul>                                                                          | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>               |\\n| **DuplicatedRowCount()**                        | <ul><li> Dataset-level.</li><li>Counts the number of duplicated rows.</li><li>Metric result: `value`.</li></ul>          ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': '                      | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one empty row.</li><li>**With reference**: Fails if share differs by >10%.</li></ul>                              |\\n| **EmptyColumnsCount()**                         | <ul><li> Dataset-level.</li><li>Counts the number of empty columns.</li><li>Metric result: `value`.</li></ul>                                                                          | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>               |\\n| **DuplicatedRowCount()**                        | <ul><li> Dataset-level.</li><li>Counts the number of duplicated rows.</li><li>Metric result: `value`.</li></ul>                                                                        | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one duplicated row.</li><li>**With reference**: Fails if share differs by >10% (+/-).</li></ul>                   |\\n| **DuplicatedColumnsCount()**                    | <ul><li> Dataset-level.</li><li>Counts the number of duplicated columns.</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one duplicated column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>          |\\n| **DatasetMissingValueCount()**                  | <ul><li> Dataset-level.</li><li>Calculates the number and share of missing values',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': '                                                              | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one duplicated row.</li><li>**With reference**: Fails if share differs by >10% (+/-).</li></ul>                   |\\n| **DuplicatedColumnsCount()**                    | <ul><li> Dataset-level.</li><li>Counts the number of duplicated columns.</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one duplicated column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>          |\\n| **DatasetMissingValueCount()**                  | <ul><li> Dataset-level.</li><li>Calculates the number and share of missing values.</li><li>Displays the number of missing values per column.</li><li>Metric result: `share`, `count`.</li></ul> | **Required**: <ul><li>`columns`</li></ul>**Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if there are missing values.</li><li>**With reference**: Fails if share is >10% higher than reference (+/-).</li></ul>                 |\\n| **AlmostConstantColumnsCount()**                | <ul><li> Dataset-level.</li><li>Counts almost constant columns (95% identical values).</li><li>Metric result: `value`.</li></ul>                                                       | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost constant column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>     |\\n| **ColumnsWithMissingValuesCount()**             | <ul><li> Dataset-level.</li><li>',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 21000,\n",
       "  'chunk': '.</li><li>Displays the number of missing values per column.</li><li>Metric result: `share`, `count`.</li></ul> | **Required**: <ul><li>`columns`</li></ul>**Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if there are missing values.</li><li>**With reference**: Fails if share is >10% higher than reference (+/-).</li></ul>                 |\\n| **AlmostConstantColumnsCount()**                | <ul><li> Dataset-level.</li><li>Counts almost constant columns (95% identical values).</li><li>Metric result: `value`.</li></ul>                                                       | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost constant column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>     |\\n| **ColumnsWithMissingValuesCount()**             | <ul><li> Dataset-level.</li><li>Counts columns with missing values.</li><li>Metric result: `value`.</li></ul>                                                                          | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one column with missing values.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul> |\\n\\n## Data Drift\\n\\nUse to detect distribution drift for text and tabular data or over computed text descriptors. Checks 20+ drift methods listed separately: [text and tabular](/metrics/customize_data_drift).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.\\n</Info>\\n\\n<Info>\\n  [Metrics explainers](/metrics/explainer_drift). Understand how data drift\\xa0works.\\n</Info>\\n\\n| Metric                                | Description                                                                                      ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 22000,\n",
       "  'chunk': 'Counts columns with missing values.</li><li>Metric result: `value`.</li></ul>                                                                          | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one column with missing values.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul> |\\n\\n## Data Drift\\n\\nUse to detect distribution drift for text and tabular data or over computed text descriptors. Checks 20+ drift methods listed separately: [text and tabular](/metrics/customize_data_drift).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.\\n</Info>\\n\\n<Info>\\n  [Metrics explainers](/metrics/explainer_drift). Understand how data drift\\xa0works.\\n</Info>\\n\\n| Metric                                | Description                                                                                                                                                                                                                                                                                                                                | Parameters                                                                                                                                                                                                                                                                             | Test Defaults                                                                                                                            |\\n| ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 23000,\n",
       "  'chunk': '                                                                                                                                                                                                                                          | Parameters                                                                                                                                                                                                                                                                             | Test Defaults                                                                                                                            |\\n| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\\n| **DataDriftPreset()**                 | <ul><li>Large Preset. </li><li> Requires reference.</li><li>Calculates data drift for all or set columns. </li><li> Uses the default or set method.</li><li>Returns drift score for each column.</li><li>Visualizes all distributions.</li><li>Metric result: all Metrics.</li><li>[Preset page](/metrics/customize_data_drift).</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method`</li><li>`cat_method`</li><li>`num_method`</li><li>`per_column_method`</li><li>`threshold`</li><li>`cat_threshold`</li><li>`nu',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 24000,\n",
       "  'chunk': '---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\\n| **DataDriftPreset()**                 | <ul><li>Large Preset. </li><li> Requires reference.</li><li>Calculates data drift for all or set columns. </li><li> Uses the default or set method.</li><li>Returns drift score for each column.</li><li>Visualizes all distributions.</li><li>Metric result: all Metrics.</li><li>[Preset page](/metrics/customize_data_drift).</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method`</li><li>`cat_method`</li><li>`num_method`</li><li>`per_column_method`</li><li>`threshold`</li><li>`cat_threshold`</li><li>`num_threshold`</li><li>`per_column_threshold`</li></ul>See [drift options](/metrics/customize_data_drift). | <ul><li>**With reference**: Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).</li></ul> |\\n| **DriftedColumnsCount()**             | <ul><li> Dataset-level. </li><li> Requires reference.</li><li>Calculates the number and share of drifted columns in the dataset.</li><li>Each column is tested for drift using the default algorithm or set method.</li><li>Returns only the total number of drifted columns.</li><li>Metric result: `count`, `share`.</li></ul>           | **Optional**: <ul><li>`columns`</li><li>`method`</li><li>`cat_method`</li><li>`num_method`</li><li>`per_column_method`</li><li>`threshold`</li><li>`cat_threshold`</li><li>`num_threshold`</li><li>`per_column_threshold`</li></ul>See [drift options](/metrics/customize_data_drift). | <ul><li>**With reference**: Fails if 50% of columns are drifted.</li></ul>                        ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 25000,\n",
       "  'chunk': 'm_threshold`</li><li>`per_column_threshold`</li></ul>See [drift options](/metrics/customize_data_drift). | <ul><li>**With reference**: Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).</li></ul> |\\n| **DriftedColumnsCount()**             | <ul><li> Dataset-level. </li><li> Requires reference.</li><li>Calculates the number and share of drifted columns in the dataset.</li><li>Each column is tested for drift using the default algorithm or set method.</li><li>Returns only the total number of drifted columns.</li><li>Metric result: `count`, `share`.</li></ul>           | **Optional**: <ul><li>`columns`</li><li>`method`</li><li>`cat_method`</li><li>`num_method`</li><li>`per_column_method`</li><li>`threshold`</li><li>`cat_threshold`</li><li>`num_threshold`</li><li>`per_column_threshold`</li></ul>See [drift options](/metrics/customize_data_drift). | <ul><li>**With reference**: Fails if 50% of columns are drifted.</li></ul>                                                               |\\n| **ValueDrift()**                      | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for a defined column (num, cat, text).</li><li>Visualizes distributions.</li><li>Metric result: `value`.</li></ul>                                                                                                                                        | **Required**: <ul><li>`column`</li></ul>**Optional:** <ul><li>`method`</li><li>`threshold`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                | <ul><li>**With reference**: Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).</li></ul> |\\n\\n\\n## Classification\\n\\nUse to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).\\n\\n<Info>\\n  [Data definition](/docs/',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 26000,\n",
       "  'chunk': '                                       |\\n| **ValueDrift()**                      | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for a defined column (num, cat, text).</li><li>Visualizes distributions.</li><li>Metric result: `value`.</li></ul>                                                                                                                                        | **Required**: <ul><li>`column`</li></ul>**Optional:** <ul><li>`method`</li><li>`threshold`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                | <ul><li>**With reference**: Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).</li></ul> |\\n\\n\\n## Classification\\n\\nUse to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction, target columns and classification type.\\n</Info>\\n\\n### General\\n\\nUse for binary classification and aggregated results for multi-class.\\n\\n| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                                                                       | Test Defaults                                                                                                                                                                        |\\n| ------------------------------ | -----------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 27000,\n",
       "  'chunk': 'library/data_definition). You may need to map prediction, target columns and classification type.\\n</Info>\\n\\n### General\\n\\nUse for binary classification and aggregated results for multi-class.\\n\\n| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                                                                       | Test Defaults                                                                                                                                                                        |\\n| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| **ClassificationPreset()**     | <ul><li>Large Preset with many classification Metrics and visuals.</li><li>See [Preset page](/metrics/preset_classification).</li><li>Metric result: all Metrics.</li></ul> | Optional: `probas_threshold`  .                                                                                                                                                                                                                                           ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 28000,\n",
       "  'chunk': '---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| **ClassificationPreset()**     | <ul><li>Large Preset with many classification Metrics and visuals.</li><li>See [Preset page](/metrics/preset_classification).</li><li>Metric result: all Metrics.</li></ul> | Optional: `probas_threshold`  .                                                                                                                                                                                                                                                  | As in individual Metrics.                                                                                                                                                            |\\n| **ClassificationQuality()**    | <ul><li>Small Preset.</li><li>Summarizes quality Metrics in a single widget.</li><li>Metric result: all Metrics.</li></ul>                                                  | Optional: `probas_threshold`                                                                                                                                                                                                                                                     | As in individual Metrics.                                                                                                                                                            |\\n| **Accuracy()**                 | <ul><li>Calculates accuracy.</li><li>Metric result: `value`.</li></ul>                                    ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 29000,\n",
       "  'chunk': '       | As in individual Metrics.                                                                                                                                                            |\\n| **ClassificationQuality()**    | <ul><li>Small Preset.</li><li>Summarizes quality Metrics in a single widget.</li><li>Metric result: all Metrics.</li></ul>                                                  | Optional: `probas_threshold`                                                                                                                                                                                                                                                     | As in individual Metrics.                                                                                                                                                            |\\n| **Accuracy()**                 | <ul><li>Calculates accuracy.</li><li>Metric result: `value`.</li></ul>                                                                                                      | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                                                                                           | <ul><li>**No reference**: Fails if lower than dummy model accuracy.</li><li>**With reference**: Fails if accuracy differs by >20%.</li></ul>                                         |\\n| **Precision()**                | <ul><li>Calculates precision.</li><li>Visualizations available: Confusion Matrix, PR Curve, PR Table.</li><li>Metric result: `value`.</li></ul>                             | **Required**: <ul><li>Set at least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold` (default: None or 0.5 for probabilistic classification)</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</l',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 30000,\n",
       "  'chunk': '                                                                  | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                                                                                           | <ul><li>**No reference**: Fails if lower than dummy model accuracy.</li><li>**With reference**: Fails if accuracy differs by >20%.</li></ul>                                         |\\n| **Precision()**                | <ul><li>Calculates precision.</li><li>Visualizations available: Confusion Matrix, PR Curve, PR Table.</li><li>Metric result: `value`.</li></ul>                             | **Required**: <ul><li>Set at least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold` (default: None or 0.5 for probabilistic classification)</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Precision is lower than the dummy model.</li><li>**With reference**: Fails if Precision differs by >20%.</li></ul>                                |\\n| **Recall()**                   | <ul><li>Calculates recall.</li><li>Visualizations available: Confusion Matrix, PR Curve, PR Table.</li><li>Metric result: `value`.</li></ul>                                | **Required**: <ul><li>Set at least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                         | <ul><li>**No reference**: Fails if lower than dummy model recall.</li><li>**With reference**: Fails if Recall differs by >20%.</li></ul>                                             |\\n| **F1Score()**                  | <ul><li>Calculates F1 Score.</li><li>Metric result: `value`.</li></ul>                                   ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 31000,\n",
       "  'chunk': 'i></ul> | <ul><li>**No reference**: Fails if Precision is lower than the dummy model.</li><li>**With reference**: Fails if Precision differs by >20%.</li></ul>                                |\\n| **Recall()**                   | <ul><li>Calculates recall.</li><li>Visualizations available: Confusion Matrix, PR Curve, PR Table.</li><li>Metric result: `value`.</li></ul>                                | **Required**: <ul><li>Set at least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                         | <ul><li>**No reference**: Fails if lower than dummy model recall.</li><li>**With reference**: Fails if Recall differs by >20%.</li></ul>                                             |\\n| **F1Score()**                  | <ul><li>Calculates F1 Score.</li><li>Metric result: `value`.</li></ul>                                                                                                      | **Required**: <ul><li>Set at least one visualization: `conf_matrix`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                 | <ul><li>**No reference**: Fails if lower than dummy model F1.</li><li>**With reference**: Fails if F1 differs by >20%.</li></ul>                                                     |\\n| **TPR()**                      | <ul><li>Calculates True Positive Rate (TPR).</li><li>Metric result: `value`.</li></ul>                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                           ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 32000,\n",
       "  'chunk': '                                                                   | **Required**: <ul><li>Set at least one visualization: `conf_matrix`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                 | <ul><li>**No reference**: Fails if lower than dummy model F1.</li><li>**With reference**: Fails if F1 differs by >20%.</li></ul>                                                     |\\n| **TPR()**                      | <ul><li>Calculates True Positive Rate (TPR).</li><li>Metric result: `value`.</li></ul>                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if TPR is lower than the dummy model.</li><li>**With reference**: Fails if TPR differs by >20%.</li></ul>                                            |\\n| **TNR()**                      | <ul><li>Calculates True Negative Rate (TNR).</li><li>Metric result: `value`.</li></ul>                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if TNR is lower than the dummy model.</li><li>**With reference**: Fails if TNR differs by >20%.</li></ul>                                            |\\n| **FPR()**                      | <ul><li>Calculates False Positive Rate (FPR).</li><li>Metric result: `value`.</li></ul>                 ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 33000,\n",
       "  'chunk': '         | <ul><li>**No reference**: Fails if TPR is lower than the dummy model.</li><li>**With reference**: Fails if TPR differs by >20%.</li></ul>                                            |\\n| **TNR()**                      | <ul><li>Calculates True Negative Rate (TNR).</li><li>Metric result: `value`.</li></ul>                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if TNR is lower than the dummy model.</li><li>**With reference**: Fails if TNR differs by >20%.</li></ul>                                            |\\n| **FPR()**                      | <ul><li>Calculates False Positive Rate (FPR).</li><li>Metric result: `value`.</li></ul>                                                                                     | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if FPR is higher than the dummy model.</li><li>**With reference**: Fails if FPR differs by >20%.</li></ul>                                           |\\n| **FNR()**                      | <ul><li>Calculates False Negative Rate (FNR).</li><li>Metric result: `value`.</li></ul>                                                                                     | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                          ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 34000,\n",
       "  'chunk': '                                                                    | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if FPR is higher than the dummy model.</li><li>**With reference**: Fails if FPR differs by >20%.</li></ul>                                           |\\n| **FNR()**                      | <ul><li>Calculates False Negative Rate (FNR).</li><li>Metric result: `value`.</li></ul>                                                                                     | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if FNR is higher than the dummy model.</li><li>**With reference**: Fails if FNR differs by >20%.</li></ul>                                           |\\n| **LogLoss()**                  | <ul><li>Calculates Log Loss.</li><li>Metric result: `value`.</li></ul>                                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                               | <ul><li>**No reference**: Fails if LogLoss is higher than the dummy model (equals 0.5 for a constant model).</li><li>**With reference**: Fails if LogLoss differs by >20%.</li></ul> |\\n| **RocAUC()**                   | <ul><li>Calculates ROC AUC.</li><li>Can visualize PR curve or table.</li><li>Metric result: `value`.</l',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 35000,\n",
       "  'chunk': '          | <ul><li>**No reference**: Fails if FNR is higher than the dummy model.</li><li>**With reference**: Fails if FNR differs by >20%.</li></ul>                                           |\\n| **LogLoss()**                  | <ul><li>Calculates Log Loss.</li><li>Metric result: `value`.</li></ul>                                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                               | <ul><li>**No reference**: Fails if LogLoss is higher than the dummy model (equals 0.5 for a constant model).</li><li>**With reference**: Fails if LogLoss differs by >20%.</li></ul> |\\n| **RocAUC()**                   | <ul><li>Calculates ROC AUC.</li><li>Can visualize PR curve or table.</li><li>Metric result: `value`.</li></ul>                                                              | **Required**: <ul><li>Set at least one visualization: `pr_table`, `roc_curve`.</li></ul> **Optional**: <ul><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                  | <ul><li>**No reference**: Fails if ROC AUC is ≤ 0.5.</li><li>**With reference**: Fails if ROC AUC differs by >20%.</li></ul>                                                         |\\n\\nDummy metrics:\\n\\n<Accordion title=\"Dummy model quality\" defaultOpen={false}>\\n  Use these Metics to get the quality of a dummy model created on the same data (based on heuristics). You can compare your model quality to verify that it\\'s better than random. These Metrics serve as a baseline in automated testing.\\n\\n  | Metric                           | Description                                                                                             | Parameters | T',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 36000,\n",
       "  'chunk': 'i></ul>                                                              | **Required**: <ul><li>Set at least one visualization: `pr_table`, `roc_curve`.</li></ul> **Optional**: <ul><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                  | <ul><li>**No reference**: Fails if ROC AUC is ≤ 0.5.</li><li>**With reference**: Fails if ROC AUC differs by >20%.</li></ul>                                                         |\\n\\nDummy metrics:\\n\\n<Accordion title=\"Dummy model quality\" defaultOpen={false}>\\n  Use these Metics to get the quality of a dummy model created on the same data (based on heuristics). You can compare your model quality to verify that it\\'s better than random. These Metrics serve as a baseline in automated testing.\\n\\n  | Metric                           | Description                                                                                             | Parameters | Test Defaults |\\n  | -------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------- | ------------- |\\n  | **ClassificationDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul> | N/A        | N/A           |\\n  | **DummyPrecision()**             | <ul><li>Calculates precision for a dummy model.</li><li>Metric result: `value`.</li></ul>               | N/A        | N/A           |\\n  | **DummyRecall()**                | <ul><li>Calculates recall for a dummy model.</li><li>Metric result: `value`.</li></ul>                  | N/A        | N/A           |\\n  | **DummyF1()**                    | <ul><li>Calculates F1 Score for a dummy model.</li><li>Metric result: `value`.</li></ul>                | N/A        | N/A           |\\n</Accordion>\\n\\n### By label\\n\\nUse when you have multiple classes and want to evaluate quality separately.\\n\\n| Metric  ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 37000,\n",
       "  'chunk': 'est Defaults |\\n  | -------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------- | ------------- |\\n  | **ClassificationDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul> | N/A        | N/A           |\\n  | **DummyPrecision()**             | <ul><li>Calculates precision for a dummy model.</li><li>Metric result: `value`.</li></ul>               | N/A        | N/A           |\\n  | **DummyRecall()**                | <ul><li>Calculates recall for a dummy model.</li><li>Metric result: `value`.</li></ul>                  | N/A        | N/A           |\\n  | **DummyF1()**                    | <ul><li>Calculates F1 Score for a dummy model.</li><li>Metric result: `value`.</li></ul>                | N/A        | N/A           |\\n</Accordion>\\n\\n### By label\\n\\nUse when you have multiple classes and want to evaluate quality separately.\\n\\n| Metric                             | Description                                                                                                                  | Parameters                                                                                                        | Test Defaults                                                                                                                                         |   |\\n| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | - |\\n| **ClassificationQualityByLabel()** | <ul><li>Small Preset summarizing classification quality Metrics by label.</li><li>Metric result',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 38000,\n",
       "  'chunk': '                           | Description                                                                                                                  | Parameters                                                                                                        | Test Defaults                                                                                                                                         |   |\\n| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | - |\\n| **ClassificationQualityByLabel()** | <ul><li>Small Preset summarizing classification quality Metrics by label.</li><li>Metric result: all Metrics.</li></ul>      | None                                                                                                              | As in individual Metrics.                                                                                                                             |   |\\n| **PrecisionByLabel()**             | <ul><li>Calculates precision by label in multiclass classification.</li><li>Metric result (dict): `label: value`. </li></ul> | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Precision is lower than the dummy model.</li><li>**With reference**: Fails if Precision differs by >20%.</li></ul> |   |\\n| **F1ByLabel()**                    | <ul><li>Calculates F1 Score by label in multiclass classification.</li><li>Metric result (dict): `label: value`.</li></ul>  | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/libr',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 39000,\n",
       "  'chunk': ': all Metrics.</li></ul>      | None                                                                                                              | As in individual Metrics.                                                                                                                             |   |\\n| **PrecisionByLabel()**             | <ul><li>Calculates precision by label in multiclass classification.</li><li>Metric result (dict): `label: value`. </li></ul> | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Precision is lower than the dummy model.</li><li>**With reference**: Fails if Precision differs by >20%.</li></ul> |   |\\n| **F1ByLabel()**                    | <ul><li>Calculates F1 Score by label in multiclass classification.</li><li>Metric result (dict): `label: value`.</li></ul>  | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if F1 is lower than the dummy model.</li><li>**With reference**: Fails if F1 differs by >20%.</li></ul>               |   |\\n| **RecallByLabel()**                | <ul><li>Calculates recall by label in multiclass classification.</li><li>Metric result (dict): `label: value`</li></ul>     | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Recall is lower than the dummy model.</li><li>**With reference**: Fails if Recall differs by >20%.</li></ul>       |   |\\n| **RocAUCByLabel()**                | <ul><li>Calculates ROC AUC by label in multiclass classification.</li><li>Metric result (dict): `label: value`</li></ul>     | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if ROC AUC is ≤ 0.5.</li><li>**With reference**: Fails if ROC AUC diff',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 40000,\n",
       "  'chunk': 'ary/tests)</li></ul> | <ul><li>**No reference**: Fails if F1 is lower than the dummy model.</li><li>**With reference**: Fails if F1 differs by >20%.</li></ul>               |   |\\n| **RecallByLabel()**                | <ul><li>Calculates recall by label in multiclass classification.</li><li>Metric result (dict): `label: value`</li></ul>     | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Recall is lower than the dummy model.</li><li>**With reference**: Fails if Recall differs by >20%.</li></ul>       |   |\\n| **RocAUCByLabel()**                | <ul><li>Calculates ROC AUC by label in multiclass classification.</li><li>Metric result (dict): `label: value`</li></ul>     | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if ROC AUC is ≤ 0.5.</li><li>**With reference**: Fails if ROC AUC differs by >20%.</li></ul>                          |   |\\n\\n## Regression\\n\\nUse to evaluate the quality of a regression model.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns.\\n</Info>\\n\\n| Metric                | Description                                                                                                                                                                                                    | Parameters                                                                                                                                                                             | Test Defaults                                                                                                                                                                                   |\\n| --------------------- | -----------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 41000,\n",
       "  'chunk': 'ers by >20%.</li></ul>                          |   |\\n\\n## Regression\\n\\nUse to evaluate the quality of a regression model.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns.\\n</Info>\\n\\n| Metric                | Description                                                                                                                                                                                                    | Parameters                                                                                                                                                                             | Test Defaults                                                                                                                                                                                   |\\n| --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RegressionPreset**  | <ul><li>Large Preset. </li><li> Includes a wide range of regression metrics with rich visuals.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_regression).</li></ul>            | None.                                                                                                                                                                                  | As in individual metrics.                                                                                      ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 42000,\n",
       "  'chunk': '--------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RegressionPreset**  | <ul><li>Large Preset. </li><li> Includes a wide range of regression metrics with rich visuals.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_regression).</li></ul>            | None.                                                                                                                                                                                  | As in individual metrics.                                                                                                                                                                       |\\n| **RegressionQuality** | <ul><li>Small Preset. </li><li> Summarizes key regression metrics in a single widget.</li><li>Metric result: all metrics.</li></ul>                                                                            | None.                                                                                                                                                                                  | As in individual metrics.                                                                                                                                                                       |\\n| **MeanError()**       | <ul><li>Calculates the mean error.</li><li>Visualizations available: Error Plot, Error Distribution, Error Normality.</li><li>Metric result: `mean`, `std`.</li></ul>                              | **Required**: <ul><li>Set at least one visualization: `error_plot`, `error_distr',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 43000,\n",
       "  'chunk': '                                                                                 |\\n| **RegressionQuality** | <ul><li>Small Preset. </li><li> Summarizes key regression metrics in a single widget.</li><li>Metric result: all metrics.</li></ul>                                                                            | None.                                                                                                                                                                                  | As in individual metrics.                                                                                                                                                                       |\\n| **MeanError()**       | <ul><li>Calculates the mean error.</li><li>Visualizations available: Error Plot, Error Distribution, Error Normality.</li><li>Metric result: `mean`, `std`.</li></ul>                              | **Required**: <ul><li>Set at least one visualization: `error_plot`, `error_distr`, `error_normality`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests). Use  `mean_tests` and `std_tests`. </li></ul> | <ul><li>**No reference/With reference**: Expect ME to be near zero. Fails if Mean Error is skewed and condition is violated: `eq = approx(absolute=0.1 * error_std)`.</li></ul>                 |\\n| **MAE()**             | <ul><li>Calculates Mean Absolute Error (MAE).</li><li>Visualizations available: Error Plot, Error Distribution, Error Normality.</li><li>Metric result: `mean`, `std`.</li></ul> | **Required**: <ul><li>Set at least one visualization: `error_plot`, `error_distr`, `error_normality`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests). Use  `mean_tests` and `std_tests`. </li></ul> | <ul><li>**No reference**: Fails if MAE is higher than the dummy model predicting the median target value.</li><li>**With reference**: Fails if MAE differs by >10%.</li></ul>                   |\\n| **RMSE()**            | <ul><li>Calculate',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 44000,\n",
       "  'chunk': '`, `error_normality`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests). Use  `mean_tests` and `std_tests`. </li></ul> | <ul><li>**No reference/With reference**: Expect ME to be near zero. Fails if Mean Error is skewed and condition is violated: `eq = approx(absolute=0.1 * error_std)`.</li></ul>                 |\\n| **MAE()**             | <ul><li>Calculates Mean Absolute Error (MAE).</li><li>Visualizations available: Error Plot, Error Distribution, Error Normality.</li><li>Metric result: `mean`, `std`.</li></ul> | **Required**: <ul><li>Set at least one visualization: `error_plot`, `error_distr`, `error_normality`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests). Use  `mean_tests` and `std_tests`. </li></ul> | <ul><li>**No reference**: Fails if MAE is higher than the dummy model predicting the median target value.</li><li>**With reference**: Fails if MAE differs by >10%.</li></ul>                   |\\n| **RMSE()**            | <ul><li>Calculates Root Mean Square Error (RMSE).</li><li>Metric result: `value`.</li></ul>                                                                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if RMSE is higher than the dummy model predicting the mean target value.</li><li>**With reference**: Fails if RMSE differs by >10%.</li></ul>                   |\\n| **MAPE()**            | <ul><li>Calculates Mean Absolute Percentage Error (MAPE).</li><li>Visualizations available: Percentage Error Plot.</li><li>Metric result: `mean`, `std`.</li></ul>     | **Required**: <ul><li>Set at least one visualization: `perc_error_plot`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                              | <ul><li>**No reference**: Fails if MAPE is highe',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 45000,\n",
       "  'chunk': 's Root Mean Square Error (RMSE).</li><li>Metric result: `value`.</li></ul>                                                                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if RMSE is higher than the dummy model predicting the mean target value.</li><li>**With reference**: Fails if RMSE differs by >10%.</li></ul>                   |\\n| **MAPE()**            | <ul><li>Calculates Mean Absolute Percentage Error (MAPE).</li><li>Visualizations available: Percentage Error Plot.</li><li>Metric result: `mean`, `std`.</li></ul>     | **Required**: <ul><li>Set at least one visualization: `perc_error_plot`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                              | <ul><li>**No reference**: Fails if MAPE is higher than the dummy model predicting the weighted median target value.</li><li>**With reference**: Fails if MAPE differs by >10%.</li></ul>        |\\n| **R2Score()**         | <ul><li>Calculates R² (Coefficient of Determination).</li><li>Metric result: `value`.</li></ul>                                                                                                              | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if R² ≤ 0.</li><li>**With reference**: Fails if R² differs by >10%.</li></ul>                                                                                   |\\n| **AbsMaxError()**     | <ul><li>Calculates Absolute Maximum Error.</li><li>Metric result: `value`.</li></ul>                                                                                                                   | **Optional**: <',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 46000,\n",
       "  'chunk': 'r than the dummy model predicting the weighted median target value.</li><li>**With reference**: Fails if MAPE differs by >10%.</li></ul>        |\\n| **R2Score()**         | <ul><li>Calculates R² (Coefficient of Determination).</li><li>Metric result: `value`.</li></ul>                                                                                                              | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if R² ≤ 0.</li><li>**With reference**: Fails if R² differs by >10%.</li></ul>                                                                                   |\\n| **AbsMaxError()**     | <ul><li>Calculates Absolute Maximum Error.</li><li>Metric result: `value`.</li></ul>                                                                                                                   | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if absolute maximum error is higher than the dummy model predicting the median target value.</li><li>**With reference**: Fails if it differs by >10%.</li></ul> |\\n\\nDummy metrics:\\n\\n<Accordion title=\"Dummy model quality\" defaultOpen={false}>\\n  Use these Metics to get the baseline quality for regression: they use optimal constants (varies by the Metric). These Metrics serve as a baseline in automated testing.\\n\\n  | Metric                       | Description                                                                                                                                                         | Parameters | Test Defaults |\\n  | ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 47000,\n",
       "  'chunk': 'ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if absolute maximum error is higher than the dummy model predicting the median target value.</li><li>**With reference**: Fails if it differs by >10%.</li></ul> |\\n\\nDummy metrics:\\n\\n<Accordion title=\"Dummy model quality\" defaultOpen={false}>\\n  Use these Metics to get the baseline quality for regression: they use optimal constants (varies by the Metric). These Metrics serve as a baseline in automated testing.\\n\\n  | Metric                       | Description                                                                                                                                                         | Parameters | Test Defaults |\\n  | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ------------- |\\n  | **RegressionDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul>                                                             | N/A        | N/A           |\\n  | **DummyMeanError()**         | <ul><li>Calculates Mean Error for a dummy model.</li><li>Metric result: `mean_error`, `error std`.</li></ul>                                                        | N/A        | N/A           |\\n  | **DummyMAE()**               | <ul><li>Calculates Mean Absolute Error (MAE) for a dummy model.</li><li>Metric result: `mean_absolute_error`, `absolute_error_std`.</li></ul>                       | N/A        | N/A           |\\n  | **DummyMAPE()**              | <ul><li>Calculates Mean Absolute Percentage Error (MAPE) for a dummy model.</li><li>Metric result: `mean_perc_absolute_error`, `perc_absolute_error std`.</li></ul> | N/A        | N/A           |\\n  | **Du',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 48000,\n",
       "  'chunk': '---------------------------------------- | ---------- | ------------- |\\n  | **RegressionDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul>                                                             | N/A        | N/A           |\\n  | **DummyMeanError()**         | <ul><li>Calculates Mean Error for a dummy model.</li><li>Metric result: `mean_error`, `error std`.</li></ul>                                                        | N/A        | N/A           |\\n  | **DummyMAE()**               | <ul><li>Calculates Mean Absolute Error (MAE) for a dummy model.</li><li>Metric result: `mean_absolute_error`, `absolute_error_std`.</li></ul>                       | N/A        | N/A           |\\n  | **DummyMAPE()**              | <ul><li>Calculates Mean Absolute Percentage Error (MAPE) for a dummy model.</li><li>Metric result: `mean_perc_absolute_error`, `perc_absolute_error std`.</li></ul> | N/A        | N/A           |\\n  | **DummyRMSE()**              | <ul><li>Calculates Root Mean Square Error (RMSE) for a dummy model.</li><li>Metric result: `rmse`.</li></ul>                                                        | N/A        | N/A           |\\n  | **DummyR2()**                | <ul><li>Calculates Calculates R² (Coefficient of Determination) for a dummy model.</li><li>Metric result: `r2score`.</li></ul>                                      | N/A        | N/A           |\\n</Accordion>\\n\\n## Ranking\\n\\nUse to evaluate ranking, search / retrieval or recommendations.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type.\\n</Info>\\n\\n<Info>\\n  [**Metric explainers**](/metrics/explainer_recsys)**.** Check ranking metrics explainers.\\n</Info>\\n\\n| Metric                              | Description                                                                                                                                                                  ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 49000,\n",
       "  'chunk': 'mmyRMSE()**              | <ul><li>Calculates Root Mean Square Error (RMSE) for a dummy model.</li><li>Metric result: `rmse`.</li></ul>                                                        | N/A        | N/A           |\\n  | **DummyR2()**                | <ul><li>Calculates Calculates R² (Coefficient of Determination) for a dummy model.</li><li>Metric result: `r2score`.</li></ul>                                      | N/A        | N/A           |\\n</Accordion>\\n\\n## Ranking\\n\\nUse to evaluate ranking, search / retrieval or recommendations.\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type.\\n</Info>\\n\\n<Info>\\n  [**Metric explainers**](/metrics/explainer_recsys)**.** Check ranking metrics explainers.\\n</Info>\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RecallTopK()**                    | <ul><li>Calculates Recall at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                  ',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 50000,\n",
       "  'chunk': '       | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RecallTopK()**                    | <ul><li>Calculates Recall at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                   | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if recall > 0.</li><li>**With reference**: Fails if Recall differs by >10%.</li></ul>                   |\\n| **FBetaTopK()**                     | <ul><li>Calculates F-beta score at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if F-beta > 0.</li><li>**With reference**: Fails if F-beta differs by >10%.</li></ul>                   |\\n| **PrecisionTopK()**                 | <ul><li>Calculates Precision at the top K retrieved items.</li><li>Metric result: `value`',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 51000,\n",
       "  'chunk': '                                                 | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if recall > 0.</li><li>**With reference**: Fails if Recall differs by >10%.</li></ul>                   |\\n| **FBetaTopK()**                     | <ul><li>Calculates F-beta score at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if F-beta > 0.</li><li>**With reference**: Fails if F-beta differs by >10%.</li></ul>                   |\\n| **PrecisionTopK()**                 | <ul><li>Calculates Precision at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if Precision > 0.</li><li>**With reference**: Fails if Precision differs by >10%.</li></ul>             |\\n| **MAP()**                           | <ul><li>Calculates Mean Average Precision at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                   | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MAP > 0.</li><li>**With reference**: Fails if MAP differs by >10%.</li></ul>                         |\\n| **NDCG()**                          | <ul><li>Calculates Normalized Discounted Cumula',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 52000,\n",
       "  'chunk': '.</li></ul>                                                                                | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if Precision > 0.</li><li>**With reference**: Fails if Precision differs by >10%.</li></ul>             |\\n| **MAP()**                           | <ul><li>Calculates Mean Average Precision at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                   | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MAP > 0.</li><li>**With reference**: Fails if MAP differs by >10%.</li></ul>                         |\\n| **NDCG()**                          | <ul><li>Calculates Normalized Discounted Cumulative Gain at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if NDCG > 0.</li><li>**With reference**: Fails if NDCG differs by >10%.</li></ul>                       |\\n| **MRR()**                           | <ul><li>Calculates Mean Reciprocal Rank at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MRR > 0.</li><li>**With reference**: Fails if MRR differs by >10%.</li></ul>                         |\\n| **HitRate()**                       | <ul><',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 53000,\n",
       "  'chunk': 'tive Gain at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if NDCG > 0.</li><li>**With reference**: Fails if NDCG differs by >10%.</li></ul>                       |\\n| **MRR()**                           | <ul><li>Calculates Mean Reciprocal Rank at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MRR > 0.</li><li>**With reference**: Fails if MRR differs by >10%.</li></ul>                         |\\n| **HitRate()**                       | <ul><li>Calculates Hit Rate at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                 | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if Hit Rate > 0.</li><li>**With reference**: Fails if Hit Rate differs by >10%.</li></ul>               |\\n| **ScoreDistribution()**             | <ul><li>Computes the predicted score entropy (KL divergence). </li><li> Applies only when the recommendations\\\\_type is a score..</li><li>Metric result: `value`.</li></ul>          | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**:`value`</li><li>**With reference**: `value`.</li></ul>                                                         |',\n",
       "  'title': 'All Metrics',\n",
       "  'description': 'Reference page for all dataset-level evals.',\n",
       "  'filename': 'docs-main/metrics/all_metrics.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'These are pre-built evaluation templates that are easy to run without setup. They are great for a start: you can create a custom setup later.\\n\\n<Note>\\n  Note that Presets apply on the **dataset level**. If you looking at row-level evaluations (e.g. scoring relevance, correcteness, etc. for LLM outputs and RAG), it\\'s best to explore [built-in descriptors](/metrics/all_descriptors).\\n</Note>\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Text Evals\" icon=\"comments\" href=\"/metrics/preset_text_evals\">\\n    Evals for text and LLMs.\\n  </Card>\\n  <Card title=\"Data Drift\" icon=\"chart-simple\" href=\"/metrics/preset_data_drift\">\\n    Data distribution drift detection.\\n  </Card>\\n</CardGroup>\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Data Summary\" icon=\"file-chart-column\" href=\"/metrics/preset_data_summary\">\\n    Dataset overview and statistics .\\n  </Card>\\n  <Card title=\"Classification\" icon=\"dice-two\" href=\"/metrics/preset_classification\">\\n    Quality for classification tasks.\\n  </Card>\\n</CardGroup>\\n\\n<CardGroup cols={2}>\\n  <Card title=\"Regression\" icon=\"chart-line\" href=\"/metrics/preset_regression\">\\n    Quality for regression tasks.\\n  </Card>\\n</CardGroup>',\n",
       "  'title': 'Overview',\n",
       "  'description': 'All available Presets.',\n",
       "  'filename': 'docs-main/metrics/all_presets.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Pending implementation for the new API.',\n",
       "  'title': 'Add text comments [UNPUBLISHED]',\n",
       "  'description': 'How to add text widgets to the Report.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_add_text.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Pending implementation for the new API.',\n",
       "  'title': 'Change colors [UNPUBLISHED]',\n",
       "  'description': 'How to change color schema of the Report.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_colors.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'All Metrics and Presets that evaluate shift in data distributions use the default [Data Drift algorithm](/metrics/explainer_drift). It automatically selects the drift detection method based on the column type (text, categorical, numerical) and volume.\\n\\nYou can override the defaults by passing a custom parameter to the chosen Metric or Preset. You can modify the drift detection method (choose from 20+ available), thresholds, or both.&#x20;\\n\\nYou can also implement fully custom drift detection methods.\\n\\n**Pre-requisites**:\\n\\n* You know how to use [Data Definition ](/docs/library/data_definition)to map column types.\\n\\n* You know how to create [Reports](/docs/library/report) and run [Tests](/docs/library/tests).\\n\\n## Data drift parameters\\n\\n<Note>\\n  Setting conditions for data drift works differently from the usual Test  API (with `gt`, `lt`, etc.) This accounts for nuances like varying role of thresholds across drift detection methods, where \"greater\"  can be better or worse depending on the method.&#x20;\\n</Note>\\n\\n### Dataset-level\\n\\n**Dataset drift share**. You can set the share of drifting columns that signals **dataset drift** (default: 0.5) in the relevant Metrics or Presets. For example, to set it at 70%:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(drift_share=0.7)\\n]\\n```\\n\\nThis will detect dataset drift if over 70% columns are drifting, using defaults for each column.\\n\\n**Drift methods**. You can also specify the drift detection methods used on the column level. For example, to use PSI (Population Stability Index) for all columns in the dataset:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(drift_share=0.7, method=\"psi\")\\n]\\n```\\n\\nThis will check if over 70% columns are drifting, using PSI method with default thresholds.\\n\\n<Tip>\\n  See all available methods in the table below.\\n</Tip>\\n\\n**Drift thresholds**. You can set thresholds for each method. For example, use PSI with a threshold of 0.3 for categorical columns.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(cat_me',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'ethod.&#x20;\\n</Note>\\n\\n### Dataset-level\\n\\n**Dataset drift share**. You can set the share of drifting columns that signals **dataset drift** (default: 0.5) in the relevant Metrics or Presets. For example, to set it at 70%:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(drift_share=0.7)\\n]\\n```\\n\\nThis will detect dataset drift if over 70% columns are drifting, using defaults for each column.\\n\\n**Drift methods**. You can also specify the drift detection methods used on the column level. For example, to use PSI (Population Stability Index) for all columns in the dataset:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(drift_share=0.7, method=\"psi\")\\n]\\n```\\n\\nThis will check if over 70% columns are drifting, using PSI method with default thresholds.\\n\\n<Tip>\\n  See all available methods in the table below.\\n</Tip>\\n\\n**Drift thresholds**. You can set thresholds for each method. For example, use PSI with a threshold of 0.3 for categorical columns.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(cat_method=\"psi\", cat_threshold=\"0.3\")\\n]\\n```\\n\\nIn this case, if PSI is ≥ 0.3 for any categorical column, drift will be detected for that column. The rest of the checks will use defaults: default methods for numerical and text columns (if present), and 50% as the `drift_share` threshold.\\n\\n### Column-level\\n\\nFor column-level metrics, you can set the drift method/threshold directly for each column:\\n\\n```python\\nreport = Report([\\n    ValueDrift(column=\"Salary\", method=\"psi\"),\\n]\\n```\\n\\n### All parameters\\n\\nUse the following parameters to pass chosen drift methods. See methods and their defaults below.\\n\\n| Parameter                                                  | Description                                                                                                                                                                                                                | Applies To                                                   |\\n| ---------------------------------------------------------- ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'thod=\"psi\", cat_threshold=\"0.3\")\\n]\\n```\\n\\nIn this case, if PSI is ≥ 0.3 for any categorical column, drift will be detected for that column. The rest of the checks will use defaults: default methods for numerical and text columns (if present), and 50% as the `drift_share` threshold.\\n\\n### Column-level\\n\\nFor column-level metrics, you can set the drift method/threshold directly for each column:\\n\\n```python\\nreport = Report([\\n    ValueDrift(column=\"Salary\", method=\"psi\"),\\n]\\n```\\n\\n### All parameters\\n\\nUse the following parameters to pass chosen drift methods. See methods and their defaults below.\\n\\n| Parameter                                                  | Description                                                                                                                                                                                                                | Applies To                                                   |\\n| ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |\\n| `method`                                                 | Defines the drift detection method for a given column (if one column is tested), or all columns in the dataset (if multiple columns are tested and the method can apply to all columns).                                   | `ValueDrift()`, `DriftedColumnsCount()`, `DataDriftPreset()` |\\n| `threshold`                                       | Sets the drift threshold in a given column or all columns.<br /><br />The threshold meaning varies based on the drift detection method, e.g., it can be the value of a distance metric or a p-value of a statistical test. | `ValueDrift()`, `DriftedColumnsCount()`, `DataDriftPreset()` |\\n| `drift_share`                ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': '| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |\\n| `method`                                                 | Defines the drift detection method for a given column (if one column is tested), or all columns in the dataset (if multiple columns are tested and the method can apply to all columns).                                   | `ValueDrift()`, `DriftedColumnsCount()`, `DataDriftPreset()` |\\n| `threshold`                                       | Sets the drift threshold in a given column or all columns.<br /><br />The threshold meaning varies based on the drift detection method, e.g., it can be the value of a distance metric or a p-value of a statistical test. | `ValueDrift()`, `DriftedColumnsCount()`, `DataDriftPreset()` |\\n| `drift_share`                                              | Defines the share of drifting columns as a condition for Dataset Drift. Default: 0.5                                                                                                                                       | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `cat_method` <br />`cat_threshold`              | Sets the drift method and/or threshold for all categorical columns.                                                                                                                                                        | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `num_method` <br />`num_threshold`              | Sets the drift method and/or threshold for all numerical columns.                                                                                                                                                          | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `per_colum',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '                              | Defines the share of drifting columns as a condition for Dataset Drift. Default: 0.5                                                                                                                                       | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `cat_method` <br />`cat_threshold`              | Sets the drift method and/or threshold for all categorical columns.                                                                                                                                                        | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `num_method` <br />`num_threshold`              | Sets the drift method and/or threshold for all numerical columns.                                                                                                                                                          | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `per_column_method`<br />`per_column_threshold` | Sets the drift method and/or threshold for the listed columns (accepts a dictionary).                                                                                                                                      | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `text_method` <br /> `text_threshold`           | Defines the drift detection method and threshold for all text columns.                                                                                                                                                     | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n\\n## Data drift detection methods\\n\\n### Tabular data\\n\\nThe following methods apply to **tabular** data: numerical or categorical columns in data definition. Pass them using the `stattest` (or `num_stattest`, etc.) parameter.\\n\\n| StatTest                                          | Applicable to                                                  ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'n_method`<br />`per_column_threshold` | Sets the drift method and/or threshold for the listed columns (accepts a dictionary).                                                                                                                                      | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `text_method` <br /> `text_threshold`           | Defines the drift detection method and threshold for all text columns.                                                                                                                                                     | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n\\n## Data drift detection methods\\n\\n### Tabular data\\n\\nThe following methods apply to **tabular** data: numerical or categorical columns in data definition. Pass them using the `stattest` (or `num_stattest`, etc.) parameter.\\n\\n| StatTest                                          | Applicable to                                                                                                       | Drift score                                                                                          |\\n| ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\\n| `ks`<br />Kolmogorov–Smirnov (K-S) test           | tabular data<br />only numerical <br /><br />**Default method for numerical data, if ≤ 1000 objects**               | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `chisquare`<br />Chi-Square test                  | tabular data<br />only categorical<br /><br />**Default method for categorical with > 2 labels, if ≤ 1000 objects** | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `z`<br /> Z-tes',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': '                                                     | Drift score                                                                                          |\\n| ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\\n| `ks`<br />Kolmogorov–Smirnov (K-S) test           | tabular data<br />only numerical <br /><br />**Default method for numerical data, if ≤ 1000 objects**               | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `chisquare`<br />Chi-Square test                  | tabular data<br />only categorical<br /><br />**Default method for categorical with > 2 labels, if ≤ 1000 objects** | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `z`<br /> Z-test                                  | tabular data<br />only categorical<br /><br />**Default method for binary data, if ≤ 1000 objects**                 | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `wasserstein`<br /> Wasserstein distance (normed) | tabular data<br />only numerical<br /><br />**Default method for numerical data, if > 1000 objects**                | returns `distance`<br />drift detected when `distance` ≥ `threshold`<br />default threshold: 0.1     |\\n| `kl_div`<br />Kullback-Leibler divergence         | tabular data<br />numerical and categorical                                                                         | returns `divergence`<br />drift detected when `divergence` ≥ `threshold`<br />default threshold: 0.1 |\\n| `psi`<br /> Population Stability Index (PSI)      | tabular data<br />numerical and categorical                                                                         | returns `psi_value`<',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': \"t                                  | tabular data<br />only categorical<br /><br />**Default method for binary data, if ≤ 1000 objects**                 | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `wasserstein`<br /> Wasserstein distance (normed) | tabular data<br />only numerical<br /><br />**Default method for numerical data, if > 1000 objects**                | returns `distance`<br />drift detected when `distance` ≥ `threshold`<br />default threshold: 0.1     |\\n| `kl_div`<br />Kullback-Leibler divergence         | tabular data<br />numerical and categorical                                                                         | returns `divergence`<br />drift detected when `divergence` ≥ `threshold`<br />default threshold: 0.1 |\\n| `psi`<br /> Population Stability Index (PSI)      | tabular data<br />numerical and categorical                                                                         | returns `psi_value`<br />drift detected when `psi_value` ≥ `threshold`<br />default threshold: 0.1   |\\n| `jensenshannon`<br /> Jensen-Shannon distance     | tabular data<br />numerical and categorical<br /><br />**Default method for categorical, if > 1000 objects**        | returns `distance`<br />drift detected when `distance` ≥ `threshold`<br />default threshold: 0.1     |\\n| `anderson`<br /> Anderson-Darling test            | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `fisher_exact`<br /> Fisher's Exact test          | tabular data<br />only categorical                                                                                  | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `cramer_von_mises`<br /> Cramer-Von-Mises test    | tabular data<br />only numerical      \",\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': \"br />drift detected when `psi_value` ≥ `threshold`<br />default threshold: 0.1   |\\n| `jensenshannon`<br /> Jensen-Shannon distance     | tabular data<br />numerical and categorical<br /><br />**Default method for categorical, if > 1000 objects**        | returns `distance`<br />drift detected when `distance` ≥ `threshold`<br />default threshold: 0.1     |\\n| `anderson`<br /> Anderson-Darling test            | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `fisher_exact`<br /> Fisher's Exact test          | tabular data<br />only categorical                                                                                  | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `cramer_von_mises`<br /> Cramer-Von-Mises test    | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `g-test`<br /> G-test                             | tabular data<br />only categorical                                                                                  | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `hellinger`<br /> Hellinger Distance (normed)     | tabular data<br />numerical and categorical                                                                         | returns `distance`<br />drift detected when `distance` >= `threshold`<br />default threshold: 0.1    |\\n| `mannw`<br /> Mann-Whitney U-rank test            | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05  \",\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '                                                                              | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `g-test`<br /> G-test                             | tabular data<br />only categorical                                                                                  | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `hellinger`<br /> Hellinger Distance (normed)     | tabular data<br />numerical and categorical                                                                         | returns `distance`<br />drift detected when `distance` >= `threshold`<br />default threshold: 0.1    |\\n| `mannw`<br /> Mann-Whitney U-rank test            | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `ed`<br /> Energy distance                        | tabular data<br />only numerical                                                                                    | returns `distance`<br />drift detected when `distance >= threshold`<br />default threshold: 0.1      |\\n| `es`<br /> Epps-Singleton test                    | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `t_test`<br /> T-Test                             | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `empirical_mmd`<br /> Empirical-MMD               | tabular data<br />only numerical                                                                                 ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': '      |\\n| `ed`<br /> Energy distance                        | tabular data<br />only numerical                                                                                    | returns `distance`<br />drift detected when `distance >= threshold`<br />default threshold: 0.1      |\\n| `es`<br /> Epps-Singleton test                    | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `t_test`<br /> T-Test                             | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `empirical_mmd`<br /> Empirical-MMD               | tabular data<br />only numerical                                                                                    | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `TVD`<br /> Total-Variation-Distance              | tabular data<br />only categorical                                                                                  | returns `p_value`<br />drift detected when `p_value` \\\\< `threshold`<br />default threshold: 0.05     |\\n\\n### Text data\\n\\nText drift detection applies to columns with **raw text data**, as specified in data definition. Pass them using the `stattest` (or `text_stattest`) parameter.\\n\\n| StatTest                                                                                                    | Description                                                                                                                                                                       | Drift score                                                                                                                                         ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': '   | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `TVD`<br /> Total-Variation-Distance              | tabular data<br />only categorical                                                                                  | returns `p_value`<br />drift detected when `p_value` \\\\< `threshold`<br />default threshold: 0.05     |\\n\\n### Text data\\n\\nText drift detection applies to columns with **raw text data**, as specified in data definition. Pass them using the `stattest` (or `text_stattest`) parameter.\\n\\n| StatTest                                                                                                    | Description                                                                                                                                                                       | Drift score                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\\n| ----------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': '                                                                                                                                                                                                                                                                                                                  |\\n| ----------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `perc_text_content_drift`<br /> Text content drift (domain classifier, with statistical hypothesis testing) | Applies only to text data. Trains a classifier model to distinguish between text in “current” and “reference” datasets.<br /><br />**Default for text data ≤ 1000 objects.**      | <ul><li>returns `roc_auc` of the classifier as a `drift_score`</li><li>drift detected when `roc_auc` > possible ROC AUC of the random classifier at a set percentile</li><li>`threshold` sets the percentile of the possible ROC AUC values of the random classifier to compare against</li><li>default threshold: 0.95 (95th percentile)</li><li> `roc_auc` values can be 0 to 1 (typically 0.5 to 1); a higher value means more confident drift detection</li></ul> |\\n| `abs_text_content_drift`<br /> Text content drift (domain classifier)                                       | Applies only to text data. Trains a classifier model to distinguish between text in ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': '----------------------------------------------------- |\\n| `perc_text_content_drift`<br /> Text content drift (domain classifier, with statistical hypothesis testing) | Applies only to text data. Trains a classifier model to distinguish between text in “current” and “reference” datasets.<br /><br />**Default for text data ≤ 1000 objects.**      | <ul><li>returns `roc_auc` of the classifier as a `drift_score`</li><li>drift detected when `roc_auc` > possible ROC AUC of the random classifier at a set percentile</li><li>`threshold` sets the percentile of the possible ROC AUC values of the random classifier to compare against</li><li>default threshold: 0.95 (95th percentile)</li><li> `roc_auc` values can be 0 to 1 (typically 0.5 to 1); a higher value means more confident drift detection</li></ul> |\\n| `abs_text_content_drift`<br /> Text content drift (domain classifier)                                       | Applies only to text data. Trains a classifier model to distinguish between text in “current” and “reference” datasets.<br /><br />**Default for text data when > 1000 objects.** | <ul><li>returns `roc_auc` of the classifier as a `drift_score`</li><li>drift detected when `roc_auc` > `threshold` </li><li>`threshold` sets the ROC AUC threshold</li><li>default threshold: 0.55</li><li> `roc_auc` values can be 0 to 1 (typically 0.5 to 1); a higher value means more confident drift detection</li></ul>                                                                                                                                        |\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n## Add a custom method\\n\\nIf you do not find a suitable drift detection method, you can implement a custom function:\\n\\n```',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': '“current” and “reference” datasets.<br /><br />**Default for text data when > 1000 objects.** | <ul><li>returns `roc_auc` of the classifier as a `drift_score`</li><li>drift detected when `roc_auc` > `threshold` </li><li>`threshold` sets the ROC AUC threshold</li><li>default threshold: 0.55</li><li> `roc_auc` values can be 0 to 1 (typically 0.5 to 1); a higher value means more confident drift detection</li></ul>                                                                                                                                        |\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n## Add a custom method\\n\\nIf you do not find a suitable drift detection method, you can implement a custom function:\\n\\n```python\\nimport pandas as pd\\nfrom scipy.stats import anderson_ksamp\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import ColumnType\\nfrom evidently.metrics import ValueDrift\\nfrom evidently.metrics import DriftedColumnsCount\\nfrom evidently.legacy.calculations.stattests import register_stattest\\nfrom evidently.legacy.calculations.stattests import StatTest\\n\\n#toy data \\ndata = pd.DataFrame(data={\\n    \"column_1\": [1, 2, 3, 4, -1, 5],\\n    \"target\": [1, 1, 0, 0, 1, 1],\\n    \"prediction\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\\n})\\n\\ndefinition = DataDefinition(\\n    numerical_columns=[\"column_1\", \"target\", \"prediction\"],\\n    )\\ndataset = Dataset.from_pandas(\\n    data,\\n    data_definition=definition,\\n)\\n\\n#implement method\\ndef _addd(\\n    reference_data: pd.Series,\\n    current_data: pd.Series,\\n    feature_type: ColumnType,\\n    threshold: float,\\n):\\n    p_value = anderson_ksamp([reference_data.values, current_data.values])[2]\\n    return p_value, p_val',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': 'python\\nimport pandas as pd\\nfrom scipy.stats import anderson_ksamp\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import ColumnType\\nfrom evidently.metrics import ValueDrift\\nfrom evidently.metrics import DriftedColumnsCount\\nfrom evidently.legacy.calculations.stattests import register_stattest\\nfrom evidently.legacy.calculations.stattests import StatTest\\n\\n#toy data \\ndata = pd.DataFrame(data={\\n    \"column_1\": [1, 2, 3, 4, -1, 5],\\n    \"target\": [1, 1, 0, 0, 1, 1],\\n    \"prediction\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\\n})\\n\\ndefinition = DataDefinition(\\n    numerical_columns=[\"column_1\", \"target\", \"prediction\"],\\n    )\\ndataset = Dataset.from_pandas(\\n    data,\\n    data_definition=definition,\\n)\\n\\n#implement method\\ndef _addd(\\n    reference_data: pd.Series,\\n    current_data: pd.Series,\\n    feature_type: ColumnType,\\n    threshold: float,\\n):\\n    p_value = anderson_ksamp([reference_data.values, current_data.values])[2]\\n    return p_value, p_value < threshold\\n\\n\\nadt = StatTest(\\n    name=\"adt\",\\n    display_name=\"Anderson-Darling\",\\n    allowed_feature_types=[ColumnType.Numerical],\\n    default_threshold=0.1,\\n)\\n\\nregister_stattest(adt, default_impl=_addd)\\n\\n\\nreport = Report([\\n    # ValueDrift(column=\"column_1\"),\\n    ValueDrift(column=\"column_1\", method=\"adt\"),\\n    DriftedColumnsCount(),\\n])\\n\\nsnapshot = report.run(dataset, dataset)\\nsnapshot\\n```\\n\\nWe recommended writing a specific instance of the **StatTest class** for that function. You need:\\n\\n| Parameter               | Type        | Description                                                                          |\\n| ----------------------- | ----------- | ------------------------------------------------------------------------------------ |\\n| `name`                  | `str`       | A short name used to reference the Stat Test from the options (registered globally). |\\n| `display_name`          | `str`       | A long name displayed in the Report.                                    ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': 'ue < threshold\\n\\n\\nadt = StatTest(\\n    name=\"adt\",\\n    display_name=\"Anderson-Darling\",\\n    allowed_feature_types=[ColumnType.Numerical],\\n    default_threshold=0.1,\\n)\\n\\nregister_stattest(adt, default_impl=_addd)\\n\\n\\nreport = Report([\\n    # ValueDrift(column=\"column_1\"),\\n    ValueDrift(column=\"column_1\", method=\"adt\"),\\n    DriftedColumnsCount(),\\n])\\n\\nsnapshot = report.run(dataset, dataset)\\nsnapshot\\n```\\n\\nWe recommended writing a specific instance of the **StatTest class** for that function. You need:\\n\\n| Parameter               | Type        | Description                                                                          |\\n| ----------------------- | ----------- | ------------------------------------------------------------------------------------ |\\n| `name`                  | `str`       | A short name used to reference the Stat Test from the options (registered globally). |\\n| `display_name`          | `str`       | A long name displayed in the Report.                                                 |\\n| `func`                  | `Callable`  | The StatTest function.                                                               |\\n| `allowed_feature_types` | `List[str]` | The list of allowed feature types for this function (`cat`, `num`).                  |\\n\\nThe **StatTest function** itself should match `(reference_data: pd.Series, current_data: pd.Series, threshold: float) -> Tuple[float, bool]` signature.\\n\\nAccepts:\\n\\n* `reference_data: pd.Series` - The reference data series.\\n\\n* `current_data: pd.Series` - The current data series to compare.\\n\\n* `feature_type: str` - The type of feature being analyzed.\\n\\n* `threshold: float` - The test threshold for drift detection.\\n\\nReturns:\\n\\n* `score: float` - Stat Test score (actual value)\\n\\n* `drift_detected: bool` - indicates is drift detected with given threshold',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'To run a check not available in Evidently, you can implement it as a custom function. Use this for building your own programmatic evaluators.\\n\\n<Tip>\\n  You can also customize existing evals with parameters, such as defining custom [LLM judges ](/metrics/customize_llm_judge) or using regex-based metrics like `Contains` for word lists. See [available descriptors](/metrics/all_descriptors).\\n</Tip>\\n\\n**Pre-requisites**:\\n\\n* You know how to use built-in [descriptors](/docs/library/descriptors).\\n\\n## Imports\\n\\n```python\\nimport pandas as pd\\n\\nfrom evidently import Dataset, DataDefinition\\nfrom evidently.core.datasets import DatasetColumn\\nfrom evidently.descriptors import CustomColumnDescriptor, CustomDescriptor\\n```\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  To generate toy data and create a Dataset object:\\n\\n  ```python\\n  data = [\\n      [\"Can fish fly?\", \"no\", \"\"],\\n      [\"Is the sky blue?\", \"yes\", \"yes\"],\\n      [\"Is milk liquid??\", \"yes\", \"yes\"]\\n  ]\\n\\n  columns = [\"question\", \"target_answer\", \"answer\"]\\n\\n  df = pd.DataFrame(data, columns=columns)\\n\\n  eval_df = Dataset.from_pandas(\\n      df,\\n      data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Single column check\\n\\nYou can define a `CustomColumnDescriptor` that will:\\n\\n* take any column from your dataset to evaluate each value inside it\\n\\n* return a single column with numerical (`num`) scores or categorical (`cat`) labels.\\n\\nImplement it as a Python function that takes a Pandas Series as input and return a transformed Series. For example, to check if the column is empty:\\n\\n```python\\ndef is_empty(data: DatasetColumn) -> DatasetColumn:\\n    return DatasetColumn(\\n        type=\"cat\",\\n        data=pd.Series([\\n            \"EMPTY\" if val == \"\" else \"NON EMPTY\"\\n            for val in data.data]))\\n```\\n\\nTo use this descriptor on your data:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    CustomColumnDescriptor(\"answer\", is_empty, alias=\"is_empty\"),\\n])\\n```\\n\\nPublish to a dataframe:\\n\\n```python\\neval_df.as_datafr',\n",
       "  'title': 'Custom Text Descriptor',\n",
       "  'description': 'How to add a custom row-level text evaluator.',\n",
       "  'filename': 'docs-main/metrics/customize_descriptor.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'n\", \"target_answer\", \"answer\"]\\n\\n  df = pd.DataFrame(data, columns=columns)\\n\\n  eval_df = Dataset.from_pandas(\\n      df,\\n      data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Single column check\\n\\nYou can define a `CustomColumnDescriptor` that will:\\n\\n* take any column from your dataset to evaluate each value inside it\\n\\n* return a single column with numerical (`num`) scores or categorical (`cat`) labels.\\n\\nImplement it as a Python function that takes a Pandas Series as input and return a transformed Series. For example, to check if the column is empty:\\n\\n```python\\ndef is_empty(data: DatasetColumn) -> DatasetColumn:\\n    return DatasetColumn(\\n        type=\"cat\",\\n        data=pd.Series([\\n            \"EMPTY\" if val == \"\" else \"NON EMPTY\"\\n            for val in data.data]))\\n```\\n\\nTo use this descriptor on your data:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    CustomColumnDescriptor(\"answer\", is_empty, alias=\"is_empty\"),\\n])\\n```\\n\\nPublish to a dataframe:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\n## Multi-column check\\n\\nYou can alternatively define a `CustomDescriptor` that:\\n\\n* Takes one or many named columns from your dataset,\\n\\n* Returns one or many transformed columns.\\n\\n**Pairwise evaluation**. For example, to check exact match between `target_answer` and `answer` columns, and return a label:\\n\\n```python\\ndef exact_match(dataset: Dataset) -> DatasetColumn:\\n    return DatasetColumn(\\n        type=\"cat\",\\n        data=pd.Series([\\n            \"MATCH\" if val else \"MISMATCH\"\\n            for val in dataset.column(\"target_answer\").data\\n            == dataset.column(\"answer\").data]))\\n```\\n\\nTo use this descriptor on your data:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    CustomDescriptor(exact_match, alias=\"exact\"),\\n])\\n```\\n\\n**Multiple scores**. You can also use `CustomDescriptor` to run evals for multiple columns and return multiple scores.\\n\\nAs a fun example, let\\'s reverse all words in the `question` and `answer` columns:\\n\\n```python\\nfrom typing import Union, Dict\\n\\ndef rev',\n",
       "  'title': 'Custom Text Descriptor',\n",
       "  'description': 'How to add a custom row-level text evaluator.',\n",
       "  'filename': 'docs-main/metrics/customize_descriptor.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ame()\\n```\\n\\n## Multi-column check\\n\\nYou can alternatively define a `CustomDescriptor` that:\\n\\n* Takes one or many named columns from your dataset,\\n\\n* Returns one or many transformed columns.\\n\\n**Pairwise evaluation**. For example, to check exact match between `target_answer` and `answer` columns, and return a label:\\n\\n```python\\ndef exact_match(dataset: Dataset) -> DatasetColumn:\\n    return DatasetColumn(\\n        type=\"cat\",\\n        data=pd.Series([\\n            \"MATCH\" if val else \"MISMATCH\"\\n            for val in dataset.column(\"target_answer\").data\\n            == dataset.column(\"answer\").data]))\\n```\\n\\nTo use this descriptor on your data:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    CustomDescriptor(exact_match, alias=\"exact\"),\\n])\\n```\\n\\n**Multiple scores**. You can also use `CustomDescriptor` to run evals for multiple columns and return multiple scores.\\n\\nAs a fun example, let\\'s reverse all words in the `question` and `answer` columns:\\n\\n```python\\nfrom typing import Union, Dict\\n\\ndef reverse_text(dataset: Dataset) -> Union[DatasetColumn, Dict[str, DatasetColumn]]:\\n    return {\\n        \"reversed_question\": DatasetColumn(\\n            type=\"cat\",\\n            data=pd.Series([\\n                value[::-1] for value in dataset.column(\"question\").data])),\\n        \"reversed_answer\": DatasetColumn(\\n            type=\"cat\",\\n            data=pd.Series([\\n                value[::-1] for value in dataset.column(\"answer\").data]))}\\n```\\n\\nTo use this descriptor on your data:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    CustomDescriptor(reverse_text),\\n])\\n```',\n",
       "  'title': 'Custom Text Descriptor',\n",
       "  'description': 'How to add a custom row-level text evaluator.',\n",
       "  'filename': 'docs-main/metrics/customize_descriptor.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"Pending implementation for the new API.\\n\\n---\\n## Embeddings drift\\n\\n<Warning>\\n  This method is **coming soon** to the new Evidently API! Check the old docs for now.\\n</Warning>\\n\\nThe default embedding drift method is a **classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.\\n\\n* The default for **small data with \\\\<= 1000 observations** detects drift if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile.\\n\\n* The default for **larger data with > 1000 observations** detects drift if the ROC AUC > 0.55.\\n\\n**You can choose other embedding drift detection methods**. You can specify custom thresholds and parameters such as dimensionality reduction and choose from other methods, including Euclidean distance, Cosine Similarity, Maximum Mean Discrepancy, and share of drifted embeddings. You must specify this as a parameter. \\n\\n---\\n\\n**Pre-requisites**:\\n\\n* You know how to generate Reports or Test Suites with default parameters.\\n\\n* You know how to pass custom parameters for Reports or Test Suites.\\n\\n\\n# Default\\n\\nWhen you calculate embeddings drift, Evidently automatically applies the default drift detection method (“model”).\\n\\nIn Reports:\\n\\n```python\\nreport = Report(metrics=[\\n    EmbeddingsDriftMetric('small_subset')\\n])\\n```\\n\\nIn Test Suites:\\n\\n```python\\ntests = TestSuite(tests=[\\n    TestEmbeddingsDrift(embeddings_name='small_subset')\\n])\\n```\\n\\nIt works the same inside presets, like `DataDriftPreset`.\\n\\n# Embedding parameters - Metrics and Tests\\n\\nYou can override the defaults by passing a custom `drift_method` parameter to the relevant Metric or Test. You can define the embeddings drift detection method, the threshold, or both.\\n\\nPass the `drift_method` parameter:\\n\\n```python\\nfrom evidently.metrics.data_drift.embedding_drift_methods import model\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = model()\\n\",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"o generate Reports or Test Suites with default parameters.\\n\\n* You know how to pass custom parameters for Reports or Test Suites.\\n\\n\\n# Default\\n\\nWhen you calculate embeddings drift, Evidently automatically applies the default drift detection method (“model”).\\n\\nIn Reports:\\n\\n```python\\nreport = Report(metrics=[\\n    EmbeddingsDriftMetric('small_subset')\\n])\\n```\\n\\nIn Test Suites:\\n\\n```python\\ntests = TestSuite(tests=[\\n    TestEmbeddingsDrift(embeddings_name='small_subset')\\n])\\n```\\n\\nIt works the same inside presets, like `DataDriftPreset`.\\n\\n# Embedding parameters - Metrics and Tests\\n\\nYou can override the defaults by passing a custom `drift_method` parameter to the relevant Metric or Test. You can define the embeddings drift detection method, the threshold, or both.\\n\\nPass the `drift_method` parameter:\\n\\n```python\\nfrom evidently.metrics.data_drift.embedding_drift_methods import model\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = model()\\n                         )\\n])\\n```\\n\\n# Embedding parameters - Presets\\n\\nWhen you use `NoTargetPerformanceTestPreset`, `DataDriftTestPreset` or `DataDriftPreset` you can specify which subsets of columns with embeddings to include using `embeddings`, and the drift detection method using `embeddings_drift_method`.\\n\\nBy default, the Presets will include all columns mapped as containing embeddings in `column_mapping`.\\n\\nTo exclude columns with embeddings:\\n\\n```python\\nembeddings = []\\n```\\n\\nTo specify which sets of columns to include (with the default drift detection method):\\n\\n```python\\nembeddings = [‘set1’, ‘set2’]\\n```\\n\\nTo specify which sets of columns to include, and specify the method:\\n\\n```python\\nembeddings = [‘set1’, ‘set2’]\\nembeddings_drift_method = {‘set1’: model(), ‘set2’: ratio())}\\n```\\n\\n# Embedding drift detection methods\\n\\nCurrently 4 embeddings drift detection methods are available.\\n\\n| Embeddings drift detection method | Description and default                                               \",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '                         )\\n])\\n```\\n\\n# Embedding parameters - Presets\\n\\nWhen you use `NoTargetPerformanceTestPreset`, `DataDriftTestPreset` or `DataDriftPreset` you can specify which subsets of columns with embeddings to include using `embeddings`, and the drift detection method using `embeddings_drift_method`.\\n\\nBy default, the Presets will include all columns mapped as containing embeddings in `column_mapping`.\\n\\nTo exclude columns with embeddings:\\n\\n```python\\nembeddings = []\\n```\\n\\nTo specify which sets of columns to include (with the default drift detection method):\\n\\n```python\\nembeddings = [‘set1’, ‘set2’]\\n```\\n\\nTo specify which sets of columns to include, and specify the method:\\n\\n```python\\nembeddings = [‘set1’, ‘set2’]\\nembeddings_drift_method = {‘set1’: model(), ‘set2’: ratio())}\\n```\\n\\n# Embedding drift detection methods\\n\\nCurrently 4 embeddings drift detection methods are available.\\n\\n| Embeddings drift detection method | Description and default                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\\n| --------------------------------- | -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': '                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\\n| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `drift_method=model` (Default)    | <ul><li>A binary classifier model to distinguish between embeddings in “current” and “reference” distributions.</li><li>Returns **ROC AUC** as a `drift_score`.</li><li>Drift detected when `drift_score` > `threshold` or when `drift_score` > ROC AUC of the random classifier at a set `quantile_probability`.</li><li>Default threshold: 0.55 (ROC AUC). </li><li>Default quantile\\\\_probability: 0.95. Applies when bootstrap is True; default True if less than or eq to 1000 objects.</li></ul>                                                                                                                  |\\n| `drift_method=ratio`              | <ul><li>Computes the distribution drift between individual embedding components using any of the tabular numerical drift detection methods available in E',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `drift_method=model` (Default)    | <ul><li>A binary classifier model to distinguish between embeddings in “current” and “reference” distributions.</li><li>Returns **ROC AUC** as a `drift_score`.</li><li>Drift detected when `drift_score` > `threshold` or when `drift_score` > ROC AUC of the random classifier at a set `quantile_probability`.</li><li>Default threshold: 0.55 (ROC AUC). </li><li>Default quantile\\\\_probability: 0.95. Applies when bootstrap is True; default True if less than or eq to 1000 objects.</li></ul>                                                                                                                  |\\n| `drift_method=ratio`              | <ul><li>Computes the distribution drift between individual embedding components using any of the tabular numerical drift detection methods available in Evidently. </li><li>Default tabular drift detection method: Wasserstein distance, with the 0.1 threshold.</li><li>Returns the **share of drifted embeddings** as `drift_score`. </li><li>Drift detected when `drift_score` > `threshold` </li><li>Default threshold: 0.2 (share of drifted embedding components).</li></ul>                                                                                                                                    |\\n| `drift_method=distance`           | <ul><li>Computes the distance between average embeddings in “current” and “reference” datasets using a specified distance metric (euclidean, cosine, cityblock, chebyshev). Default: `euclidean`. </li><li>Returns the **distance metric value** as `drift_score`.</li><li>Drift detected when drift\\\\_score > threshold or when `drift_score` > obtained distance in reference at a set `quantile_probability`.</li><li>Default threshold: 0.2 (relevant for Euclidean distance).</li><li>Default quantile\\\\_probability: 0.95. App',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'vidently. </li><li>Default tabular drift detection method: Wasserstein distance, with the 0.1 threshold.</li><li>Returns the **share of drifted embeddings** as `drift_score`. </li><li>Drift detected when `drift_score` > `threshold` </li><li>Default threshold: 0.2 (share of drifted embedding components).</li></ul>                                                                                                                                    |\\n| `drift_method=distance`           | <ul><li>Computes the distance between average embeddings in “current” and “reference” datasets using a specified distance metric (euclidean, cosine, cityblock, chebyshev). Default: `euclidean`. </li><li>Returns the **distance metric value** as `drift_score`.</li><li>Drift detected when drift\\\\_score > threshold or when `drift_score` > obtained distance in reference at a set `quantile_probability`.</li><li>Default threshold: 0.2 (relevant for Euclidean distance).</li><li>Default quantile\\\\_probability: 0.95. Applies when bootstrap is True; default True if less than or eq 1000 objects.</li></ul> |\\n| `drift_method=mmd`                | <ul><li>Computes the Maximum Mean Discrepancy (MMD)</li><li>Returns the **MMD value** as a `drift_score`</li><li>Drift detected when `drift_score` > `threshold` or when `drift_score` >  obtained MMD values in reference at a set `quantile_probability`.</li><li>Default threshold: 0.015 (MMD). </li><li>Default quantile\\\\_probability: 0.95. Applies when bootstrap is True; default True if less than or eq 1000 objects.</li></ul>                                                                                                                                                                              |\\n\\nIf you specify an embedding drift detection method but do not pass additional parameters, defaults will apply.\\n\\nYou can also specify parameters for any chosen method. Since the methods are different, each has a different set of parameters. Note that you should pass the par',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': \"lies when bootstrap is True; default True if less than or eq 1000 objects.</li></ul> |\\n| `drift_method=mmd`                | <ul><li>Computes the Maximum Mean Discrepancy (MMD)</li><li>Returns the **MMD value** as a `drift_score`</li><li>Drift detected when `drift_score` > `threshold` or when `drift_score` >  obtained MMD values in reference at a set `quantile_probability`.</li><li>Default threshold: 0.015 (MMD). </li><li>Default quantile\\\\_probability: 0.95. Applies when bootstrap is True; default True if less than or eq 1000 objects.</li></ul>                                                                                                                                                                              |\\n\\nIf you specify an embedding drift detection method but do not pass additional parameters, defaults will apply.\\n\\nYou can also specify parameters for any chosen method. Since the methods are different, each has a different set of parameters. Note that you should pass the parameters **directly to the chosen drift detection method**, not to the Metric.\\n\\n## Model-based (“model”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = model(\\n                              threshold = 0.55,\\n                              bootstrap = None,\\n                              quantile_probability = 0.05,\\n                              pca_components = None,\\n                          )\\n                         )\\n])\\n```\\n\\n| Parameter                         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                              \",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': \"ameters **directly to the chosen drift detection method**, not to the Metric.\\n\\n## Model-based (“model”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = model(\\n                              threshold = 0.55,\\n                              bootstrap = None,\\n                              quantile_probability = 0.05,\\n                              pca_components = None,\\n                          )\\n                         )\\n])\\n```\\n\\n| Parameter                         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\\n| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `threshold`                       | Sets the threshold for drift detection (ROC AUC). Drift is detected when `drift_score` > \",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': '                                                                                                                                                                                           |\\n| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `threshold`                       | Sets the threshold for drift detection (ROC AUC). Drift is detected when `drift_score` > `threshold`. <br />Applies when `bootstrap` not equal True.<br /><br />**Default: 0.55**.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\\n| `bootstrap` (optional)            | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br />If applied, the ROC AUC of the classifier is compared to the ROC AUC of the random classifier at a set percentile. The calculation is repeated 1000 times with randomly assigned target class probabilities. This produces a distribution of random roc\\\\_auc scores with a mean of 0,5. We then take the 95th perc',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '`threshold`. <br />Applies when `bootstrap` not equal True.<br /><br />**Default: 0.55**.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\\n| `bootstrap` (optional)            | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br />If applied, the ROC AUC of the classifier is compared to the ROC AUC of the random classifier at a set percentile. The calculation is repeated 1000 times with randomly assigned target class probabilities. This produces a distribution of random roc\\\\_auc scores with a mean of 0,5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, data drift is detected.<br /><br />**Default: True if less than or eq 1000 objects, False if > 1000 objects.** |\\n| `quantile_probability` (optional) | Sets the percentile of the possible ROC AUC values of the random classifier to compare against. <br />This applies when bootstrap is True.<br /><br />**Default: 0.95**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\\n| `pca_components` (optional)       | The number of PCA components. If spec',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': \"entile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, data drift is detected.<br /><br />**Default: True if less than or eq 1000 objects, False if > 1000 objects.** |\\n| `quantile_probability` (optional) | Sets the percentile of the possible ROC AUC values of the random classifier to compare against. <br />This applies when bootstrap is True.<br /><br />**Default: 0.95**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\\n| `pca_components` (optional)       | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`.<br /><br />**Default: None.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\\n\\n## Maximum mean discrepancy (“mmd”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = mmd(\\n                              threshold = 0.015,\\n                              bootstrap = None,\\n                              quantile_probability = 0.05,\\n                              pca_components = None,\\n                 \",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': \"ified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`.<br /><br />**Default: None.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\\n\\n## Maximum mean discrepancy (“mmd”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = mmd(\\n                              threshold = 0.015,\\n                              bootstrap = None,\\n                              quantile_probability = 0.05,\\n                              pca_components = None,\\n                          )\\n                         )\\n])\\n```\\n\\n| Parameter                         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\\n| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': '         )\\n                         )\\n])\\n```\\n\\n| Parameter                         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\\n| --------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `threshold`                       | Sets the threshold for drift detection (ROC AUC). Drift is detected when `drift_score` > `threshold`. <br />Applies when `bootstrap` not eq True.<br /><br />**Default: 0.55**.                                                                                                                                                                                                                                                                                                                                                                                   ',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `threshold`                       | Sets the threshold for drift detection (ROC AUC). Drift is detected when `drift_score` > `threshold`. <br />Applies when `bootstrap` not eq True.<br /><br />**Default: 0.55**.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\\n| `bootstrap` (optional)            | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br />If applied, the ROC AUC of the classifier is compared to the ROC AUC of the random classifier at a set percentile. The calculation is repeated 1000 times with randomly assigned target class probabilities. This produces a distribution of random roc\\\\_auc scores with a mean of 0,5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, data drift is detected.<br /><br />**Default: True if less thahn or eq 1000 objects, False if > 1000 objects.** |\\n| `quantile_probability` (optional) | Sets the percentile of the possible ROC AUC values of the random classifier to compare against. <br />This applies when bootstrap is True.<br /><br />**Default: 0.95**         ',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': '                                                                                                   |\\n| `bootstrap` (optional)            | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br />If applied, the ROC AUC of the classifier is compared to the ROC AUC of the random classifier at a set percentile. The calculation is repeated 1000 times with randomly assigned target class probabilities. This produces a distribution of random roc\\\\_auc scores with a mean of 0,5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, data drift is detected.<br /><br />**Default: True if less thahn or eq 1000 objects, False if > 1000 objects.** |\\n| `quantile_probability` (optional) | Sets the percentile of the possible ROC AUC values of the random classifier to compare against. <br />This applies when bootstrap is True.<br /><br />**Default: 0.95**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\\n| `pca_components` (optional)       | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`.<br /><br />**Default: None.**                                                                                                                                                                                                                                                                                                         ',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\\n| `pca_components` (optional)       | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`.<br /><br />**Default: None.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\\n\\n## Share of drifted embedding components (“ratio”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = ratio(\\n                              component_stattest = 'wasserstein',\\n                              component_stattest_threshold = 0.1,\\n                              threshold = 0.2,\\n                              pca_components = None,\\n                          )\\n                         )\\n])\\n```\\n\\n| Parameter                                 | Description                                                                                                                                                                                                                                                                                                                      \",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': \"                                                                                                                                                          |\\n\\n## Share of drifted embedding components (“ratio”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = ratio(\\n                              component_stattest = 'wasserstein',\\n                              component_stattest_threshold = 0.1,\\n                              threshold = 0.2,\\n                              pca_components = None,\\n                          )\\n                         )\\n])\\n```\\n\\n| Parameter                                 | Description                                                                                                                                                                                                                                                                                                                                                                                                  |\\n| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| `component_stattest` (optional)           | Sets the tabular drift detection method (any of the tabular drift detection methods for numerical features available in Evidently).\\\\< br /><br />**Default: Wasserstein**                                                                                                                                                                                                                                    |\\n| `component_stattest_threshold`\",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': '                                                                            |\\n| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| `component_stattest` (optional)           | Sets the tabular drift detection method (any of the tabular drift detection methods for numerical features available in Evidently).\\\\< br /><br />**Default: Wasserstein**                                                                                                                                                                                                                                    |\\n| `component_stattest_threshold` (optional) | Sets the threshold for drift detection for individual embedding components. Drift is detected when `drift_score` > `component_stattest_threshold` in case of distance/divergence metrics where the threshold is the metric value or `drift_score` \\\\< `component_stattest_threshold` in case of statistical tests where the threshold is the p-value.<br /><br />**Default: 0.1** (relevant for Wasserstein). |\\n| `threshold` (optional)                    | Sets the threshold (share of drifted embedding components) for drift detection for the overall dataset. <br /><br />**Default: 0.2**                                                                                                                                                                                                                                                                         |\\n| `pca_components` (optional)               | The number of PCA components. If specified, dimensionality reduction will be applied to project ',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': ' (optional) | Sets the threshold for drift detection for individual embedding components. Drift is detected when `drift_score` > `component_stattest_threshold` in case of distance/divergence metrics where the threshold is the metric value or `drift_score` \\\\< `component_stattest_threshold` in case of statistical tests where the threshold is the p-value.<br /><br />**Default: 0.1** (relevant for Wasserstein). |\\n| `threshold` (optional)                    | Sets the threshold (share of drifted embedding components) for drift detection for the overall dataset. <br /><br />**Default: 0.2**                                                                                                                                                                                                                                                                         |\\n| `pca_components` (optional)               | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`. <br /><br />**Default: None**.                                                                                                                                                                                                          |\\n\\n\\n## Distance-based methods (“distance”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric(\\'small_subset\\', \\n                          drift_method = distance(\\n                              dist = \\'euclidean\\', #\"euclidean\", \"cosine\", \"cityblock\" or \"chebyshev\"\\n                              threshold = 0.2,\\n                              pca_components = None,\\n                              bootstrap = None,\\n                              quantile_probability = 0.05\\n                          )\\n                         )\\n])\\n```\\n\\n| Parameter | Description |\\n|---|---|\\n| `dist` (optional) <br /><br />Available: <br />`euclidean` <br />`cosine`<br />`cityblock` (manhattan distance)<b',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': 'data to n-dimensional space based on the number of `pca_components`. <br /><br />**Default: None**.                                                                                                                                                                                                          |\\n\\n\\n## Distance-based methods (“distance”)\\n\\n```python\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric(\\'small_subset\\', \\n                          drift_method = distance(\\n                              dist = \\'euclidean\\', #\"euclidean\", \"cosine\", \"cityblock\" or \"chebyshev\"\\n                              threshold = 0.2,\\n                              pca_components = None,\\n                              bootstrap = None,\\n                              quantile_probability = 0.05\\n                          )\\n                         )\\n])\\n```\\n\\n| Parameter | Description |\\n|---|---|\\n| `dist` (optional) <br /><br />Available: <br />`euclidean` <br />`cosine`<br />`cityblock` (manhattan distance)<br />`chebyshev` | Sets the distance metric for drift detection. <br /><br />**Default: Euclidean distance** |\\n| `threshold` (optional)  | Sets the threshold for drift detection. Drift is detected when `drift_score` > `threshold`.<br />Applies when bootstrap not equals True<br /><br />**Default: 0.2** (relevant for euclidean distance) |\\n| `bootstrap` (optional)  | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br /><br />If applied, the distance between reference and current is tested against possible distance values in reference. We randomly split the reference data into two parts and compute the distance between them. The calculation is repeated 100 times. This produces a distribution of distance values obtained for a reference dataset. We then take the 95th percentile (default) of this distribution and compare it to the distance between reference and current datasets. If the distance between the reference and current is higher than the 9',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': 'r />`chebyshev` | Sets the distance metric for drift detection. <br /><br />**Default: Euclidean distance** |\\n| `threshold` (optional)  | Sets the threshold for drift detection. Drift is detected when `drift_score` > `threshold`.<br />Applies when bootstrap not equals True<br /><br />**Default: 0.2** (relevant for euclidean distance) |\\n| `bootstrap` (optional)  | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br /><br />If applied, the distance between reference and current is tested against possible distance values in reference. We randomly split the reference data into two parts and compute the distance between them. The calculation is repeated 100 times. This produces a distribution of distance values obtained for a reference dataset. We then take the 95th percentile (default) of this distribution and compare it to the distance between reference and current datasets. If the distance between the reference and current is higher than the 95th percentile of the distance obtained for the reference dataset, the drift is detected. <br /><br />**Default: True if less than or eq 1000 objects, False if > 1000 objects**. |\\n| `quantile_probability` (optional)  | Sets the percentile of the possible distance values in reference to compare against.<br />Applies when `bootstrap` is True.<br /><br />**Default: 0.95**. |\\n| `pca_components` (optional)  | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`. <br /><br />**Default: None**. |',\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'You can score your text by downloading and using ML models from HuggingFace. This lets you apply any criteria from the source model, e.g. classify texts by emotion. There are:\\n\\n* Ready-to-use descriptors that wrap a specific model,\\n\\n* A general interface to call other suitable models you select.\\n\\n**Pre-requisites**:\\n\\n* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data.\\n\\n## Imports\\n\\n```python\\nfrom evidently.descriptors import HuggingFace, HuggingFaceToxicity\\n```\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  To generate toy data and create a Dataset object:\\n\\n  ```python\\n  import pandas as pd\\n\\n  from evidently import Dataset\\n  from evidently import DataDefinition\\n\\n  data = [\\n      [\"Why is the sky blue?\", \\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\", \\n       \"because air scatters blue light more\"],\\n      [\"How do airplanes stay in the air?\", \\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \\n       \"because wings create lift\"],\\n      [\"Why do we have seasons?\", \\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \\n       \"because Earth is tilted\"],\\n      [\"How do magnets work?\", \\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \\n       \"because of magnetic fields\"],\\n      [\"Why does the moon change shape?\", \\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \\n       \"because it rotates\"],\\n      [\"What movie should I watch tonight?\", \\n       \"A movie is a motion picture created to entertain, educate, or infor',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'nes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \\n       \"because wings create lift\"],\\n      [\"Why do we have seasons?\", \\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \\n       \"because Earth is tilted\"],\\n      [\"How do magnets work?\", \\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \\n       \"because of magnetic fields\"],\\n      [\"Why does the moon change shape?\", \\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \\n       \"because it rotates\"],\\n      [\"What movie should I watch tonight?\", \\n       \"A movie is a motion picture created to entertain, educate, or inform viewers through a combination of storytelling, visuals, and sound.\", \\n       \"watch a movie that suits your mood\"]\\n  ]\\n\\n  columns = [\"question\", \"context\", \"response\"]\\n\\n  df = pd.DataFrame(data, columns=columns)\\n\\n  eval_df = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Built-in ML evals\\n\\n<Tip>\\n  **Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#ml-based-evals).\\n</Tip>\\n\\nThere are built-in evaluators for some models. You can call them like any other descriptor:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \\n])\\n```\\n\\n## Custom ML evals\\n\\n<Info>\\n  You can also add any custom checks [directly as a Python function](/metrics/customize_descriptor).\\n</Info>\\n\\nAlternatively, use the general `HuggingFace()` descriptor to call a specific named model. The model you use must return a numerical score or a category fo',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'm viewers through a combination of storytelling, visuals, and sound.\", \\n       \"watch a movie that suits your mood\"]\\n  ]\\n\\n  columns = [\"question\", \"context\", \"response\"]\\n\\n  df = pd.DataFrame(data, columns=columns)\\n\\n  eval_df = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Built-in ML evals\\n\\n<Tip>\\n  **Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#ml-based-evals).\\n</Tip>\\n\\nThere are built-in evaluators for some models. You can call them like any other descriptor:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \\n])\\n```\\n\\n## Custom ML evals\\n\\n<Info>\\n  You can also add any custom checks [directly as a Python function](/metrics/customize_descriptor).\\n</Info>\\n\\nAlternatively, use the general `HuggingFace()` descriptor to call a specific named model. The model you use must return a numerical score or a category for each text in a column.\\n\\nFor example, to evaluate \"curiousity\" expressed in a text:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n   HuggingFace(\"question\",\\n       model=\"SamLowe/roberta-base-go_emotions\", \\n       params={\"label\": \"curiosity\"},\\n       alias=\"Curiousity\"\\n   )\\n])\\n```\\n\\nCall the result as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/hf_descriptor_example_toxicity-min.png)\\n\\n### Sample models\\n\\nHere are some models you can call using the `HuggingFace()` descriptor.\\n\\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'r each text in a column.\\n\\nFor example, to evaluate \"curiousity\" expressed in a text:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n   HuggingFace(\"question\",\\n       model=\"SamLowe/roberta-base-go_emotions\", \\n       params={\"label\": \"curiosity\"},\\n       alias=\"Curiousity\"\\n   )\\n])\\n```\\n\\nCall the result as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/hf_descriptor_example_toxicity-min.png)\\n\\n### Sample models\\n\\nHere are some models you can call using the `HuggingFace()` descriptor.\\n\\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Example use                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '                               | Example use                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| **Emotion classification** <br /> <ul><li> Scores texts by 28 emotions. </li><li> Returns the predicted probability for the chosen emotion label. </li><li> Scale: 0 to 1. </li><li>[HuggingFace Model](https://huggingface.co/SamLowe/roberta-base-go_emotions) </li></ul>                                                                                                                                                                                                                                              | `HuggingFace(\"response\", model=\"SamLowe/roberta-base-go_emotions\", params={\"label\": \"disappointment\"}, alias=\"disappointment\")`                                        | **Required**:<ul><li> `params={\"label\":\"label\"}`</li></ul>**Available labels**:<ul><li>admiration</li><li>amusement</li><li>anger</li><li>annoyance</li><li>approval</li><li>caring</li><li>confusion</li><li>curiosity</li><li>',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': '----------------------------------------------------------------------------------------------- |\\n| **Emotion classification** <br /> <ul><li> Scores texts by 28 emotions. </li><li> Returns the predicted probability for the chosen emotion label. </li><li> Scale: 0 to 1. </li><li>[HuggingFace Model](https://huggingface.co/SamLowe/roberta-base-go_emotions) </li></ul>                                                                                                                                                                                                                                              | `HuggingFace(\"response\", model=\"SamLowe/roberta-base-go_emotions\", params={\"label\": \"disappointment\"}, alias=\"disappointment\")`                                        | **Required**:<ul><li> `params={\"label\":\"label\"}`</li></ul>**Available labels**:<ul><li>admiration</li><li>amusement</li><li>anger</li><li>annoyance</li><li>approval</li><li>caring</li><li>confusion</li><li>curiosity</li><li>desire</li><li>disappointment</li><li>disapproval</li><li>disgust</li><li>embarrassment</li><li>excitement</li><li>fear</li><li>gratitude</li><li>grief</li><li>joy</li><li>love</li><li>nervousness</li><li>optimism</li><li>pride</li><li>realization</li><li>relief</li><li>remorse</li><li>sadness</li><li>surprise</li><li>neutral</li></ul>**Optional**:<ul><li>`alias=\"name\"`</li></ul> |\\n| **Zero-shot classification** <br /><ul><li>A natural language inference model. </li><li>Use it for zero-shot classification by user-provided topics.</li><li> List candidate topics as `labels`. You can provide one or several topics. </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Returns a label. </li><li>[HuggingFace Model](https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)</li></ul> | `HuggingFace(\"response\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\", params={\"labels',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'desire</li><li>disappointment</li><li>disapproval</li><li>disgust</li><li>embarrassment</li><li>excitement</li><li>fear</li><li>gratitude</li><li>grief</li><li>joy</li><li>love</li><li>nervousness</li><li>optimism</li><li>pride</li><li>realization</li><li>relief</li><li>remorse</li><li>sadness</li><li>surprise</li><li>neutral</li></ul>**Optional**:<ul><li>`alias=\"name\"`</li></ul> |\\n| **Zero-shot classification** <br /><ul><li>A natural language inference model. </li><li>Use it for zero-shot classification by user-provided topics.</li><li> List candidate topics as `labels`. You can provide one or several topics. </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Returns a label. </li><li>[HuggingFace Model](https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)</li></ul> | `HuggingFace(\"response\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\", params={\"labels\": [\"science\", \"physics\"], \"threshold\":0.5}, alias=\"Topic\")` | **Required**: <ul><li>`params={\"labels\": [\"label\"]}`</li></ul> **Optional**:<ul><li> `params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\\n| **GPT-2 text detection** <br /><ul><li> Predicts if a text is Real or Fake (generated by a GPT-2 model). </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Note that it is not usable as a detector for more advanced models like ChatGPT.  </li',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': '\": [\"science\", \"physics\"], \"threshold\":0.5}, alias=\"Topic\")` | **Required**: <ul><li>`params={\"labels\": [\"label\"]}`</li></ul> **Optional**:<ul><li> `params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\\n| **GPT-2 text detection** <br /><ul><li> Predicts if a text is Real or Fake (generated by a GPT-2 model). </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Note that it is not usable as a detector for more advanced models like ChatGPT.  </li><li> Returns a label. </li><li> [HuggingFace Model](https://huggingface.co/openai-community/roberta-base-openai-detector)  </li></ul>                                             | `HuggingFace(\"response\", model=\"openai-community/roberta-base-openai-detector\", params={\"score_threshold\": 0.7}, alias=\"fake\")`                                        | **Optional**:<ul><li>`params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\\n\\nThis list is not exhaustive, and the Des',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': '><li> Returns a label. </li><li> [HuggingFace Model](https://huggingface.co/openai-community/roberta-base-openai-detector)  </li></ul>                                             | `HuggingFace(\"response\", model=\"openai-community/roberta-base-openai-detector\", params={\"score_threshold\": 0.7}, alias=\"fake\")`                                        | **Optional**:<ul><li>`params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\\n\\nThis list is not exhaustive, and the Descriptor may support other models published on Hugging Face. The implemented interface generally works for models that:\\n\\n* Output a single number (e.g., predicted score for a label) or a label, **not** an array of values.\\n\\n* Can process raw text input directly.\\n\\n* Name labels using\\xa0`label`\\xa0or\\xa0`labels`\\xa0fields.\\n\\n* Use methods named\\xa0`predict`\\xa0or\\xa0`predict_proba`\\xa0for scoring.\\n\\nHowever, since each model is implemented differently, we cannot provide a complete list of models with a compatible interface. We suggest testing the implementation on your own using trial and error. If you discover useful models, feel free to share them with the community in Discord. You can also open an issue on GitHub to request support for a specific model.',\n",
       "  'title': 'Use HuggingFace models',\n",
       "  'description': 'How to use models from HuggingFace as evaluators.',\n",
       "  'filename': 'docs-main/metrics/customize_hf_descriptor.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'LLM-based descriptors use an external LLM for evaluation. You can:\\n\\n- Use built-in evaluators (with pre-written prompts), or\\n- Configure custom criteria using templates.\\n\\n**Pre-requisites**:\\n\\n- You know how to use [descriptors](/docs/library/descriptors) to evaluate text data.\\n\\n## Imports\\n\\n```python\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate, MulticlassClassificationPromptTemplate \\nfrom evidently.descriptors import LLMEval, ToxicityLLMEval, ContextQualityLLMEval, DeclineLLMEval\\n```\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  To generate toy data and create a Dataset object:\\n\\n  ```python\\n  import pandas as pd\\n  from evidently import DataDefinition\\n  \\n  data = [\\n      [\"Why is the sky blue?\", \\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\", \\n       \"because air scatters blue light more\"],\\n      [\"How do airplanes stay in the air?\", \\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \\n       \"because wings create lift\"],\\n      [\"Why do we have seasons?\", \\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \\n       \"because Earth is tilted\"],\\n      [\"How do magnets work?\", \\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \\n       \"because of magnetic fields\"],\\n      [\"Why does the moon change shape?\", \\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \\n       \"because it rotates\"],\\n      [\"What movie should I watch tonight?\", \\n       \"A movie is a motion picture created to entertain, educate, or inform viewers throu',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \\n       \"because wings create lift\"],\\n      [\"Why do we have seasons?\", \\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \\n       \"because Earth is tilted\"],\\n      [\"How do magnets work?\", \\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \\n       \"because of magnetic fields\"],\\n      [\"Why does the moon change shape?\", \\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \\n       \"because it rotates\"],\\n      [\"What movie should I watch tonight?\", \\n       \"A movie is a motion picture created to entertain, educate, or inform viewers through a combination of storytelling, visuals, and sound.\", \\n       \"watch a movie that suits your mood\"]\\n  ]\\n  \\n  columns = [\"question\", \"context\", \"response\"]\\n  \\n  df = pd.DataFrame(data, columns=columns)\\n  \\n  eval_df = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Built-in LLM judges\\n\\n<Tip>\\n  **Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#llm-based-evals).\\n</Tip>\\n\\nThere are built-in evaluators for popular criteria, like detecting toxicity or if the text contains a refusal. These built-in descriptors:\\n\\n- Default to binary classifiers.\\n- Default to using `gpt-4o-mini` model from OpenAI.\\n- Return a label, the reasoning for the decision, and an optional score.\\n\\n**OpenAI key.** Add the token as the environment variable: [see docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] \\n```\\n\\n**Run a single-c',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'gh a combination of storytelling, visuals, and sound.\", \\n       \"watch a movie that suits your mood\"]\\n  ]\\n  \\n  columns = [\"question\", \"context\", \"response\"]\\n  \\n  df = pd.DataFrame(data, columns=columns)\\n  \\n  eval_df = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Built-in LLM judges\\n\\n<Tip>\\n  **Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#llm-based-evals).\\n</Tip>\\n\\nThere are built-in evaluators for popular criteria, like detecting toxicity or if the text contains a refusal. These built-in descriptors:\\n\\n- Default to binary classifiers.\\n- Default to using `gpt-4o-mini` model from OpenAI.\\n- Return a label, the reasoning for the decision, and an optional score.\\n\\n**OpenAI key.** Add the token as the environment variable: [see docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] \\n```\\n\\n**Run a single-column eval.** For example, to evaluate whether `response`contains any toxicity:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    ToxicityLLMEval(\"response\", alias=\"toxicity\"),\\n])\\n```\\n\\nView the results as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_toxicity-min.png)\\n\\n**Run a multi-column eval.** Some evaluators naturally require two columns. For example, to evaluate Context Quality (\"does it have enough information to answer the question?\"), you must run this evaluation over your `context` column, and pass the `question` column as a parameter.\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    ContextQualityLLMEval(\"context\", alias=\"good_context\", question=\"question\"),\\n])\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_context_quality-min.png)\\n\\n**Parametrize evaluators**. You can switch the output format from `category` to `score` (0 to 1) or exclude the reasoning to get only the label:\\n\\n```python\\neval_df.add_d',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'olumn eval.** For example, to evaluate whether `response`contains any toxicity:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    ToxicityLLMEval(\"response\", alias=\"toxicity\"),\\n])\\n```\\n\\nView the results as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_toxicity-min.png)\\n\\n**Run a multi-column eval.** Some evaluators naturally require two columns. For example, to evaluate Context Quality (\"does it have enough information to answer the question?\"), you must run this evaluation over your `context` column, and pass the `question` column as a parameter.\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    ContextQualityLLMEval(\"context\", alias=\"good_context\", question=\"question\"),\\n])\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_context_quality-min.png)\\n\\n**Parametrize evaluators**. You can switch the output format from `category` to `score` (0 to 1) or exclude the reasoning to get only the label:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"refusal\", include_reasoning=False),\\n    ToxicityLLMEval(\"response\", alias=\"toxicity\", include_category=False),\\n    PIILLMEval(\"response\", alias=\"PII\", include_score=True), \\n])\\n```\\n\\n<Info>\\n  **Column names**. The alias you set defines the column name with the category. If you enable the score result as well, it will get the \"Alias score\" name.\\n</Info>\\n\\n## Change the evaluator LLM\\n\\nOpenAI is the default evalution provider in Evidently, but you can choose any other, including models from Anthropic, Gemini, Mistral, Ollama, etc.\\n\\n### Using parameters\\n\\nYou can pass model and provider parameters to the built-in LLM-based descriptor or to your custom `LLMEval`.\\n\\n**Change the model**. Specify a different model from OpenAI:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"Decline by Turbo\", provider=\"openai\", model=\"gpt-3.5-turbo\"),\\n])\\n```\\n\\n**Change the provider**. To use a different LLM, first ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'escriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"refusal\", include_reasoning=False),\\n    ToxicityLLMEval(\"response\", alias=\"toxicity\", include_category=False),\\n    PIILLMEval(\"response\", alias=\"PII\", include_score=True), \\n])\\n```\\n\\n<Info>\\n  **Column names**. The alias you set defines the column name with the category. If you enable the score result as well, it will get the \"Alias score\" name.\\n</Info>\\n\\n## Change the evaluator LLM\\n\\nOpenAI is the default evalution provider in Evidently, but you can choose any other, including models from Anthropic, Gemini, Mistral, Ollama, etc.\\n\\n### Using parameters\\n\\nYou can pass model and provider parameters to the built-in LLM-based descriptor or to your custom `LLMEval`.\\n\\n**Change the model**. Specify a different model from OpenAI:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"Decline by Turbo\", provider=\"openai\", model=\"gpt-3.5-turbo\"),\\n])\\n```\\n\\n**Change the provider**. To use a different LLM, first import the corresponding API key as an environment variable.\\n\\n```python\\nimport os\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\nAnd pass the name of the `provider` and `model`. For example:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"Decline by Claude\", provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\\n])\\n```\\n\\n<Note>\\n  **List of providers and models**. Evidently uses `litellm` to call different model APIs which implements 50\\\\+ providers. You can match the `provider` name and the `model` name parameters to the list given in the [LiteLLM docs](https://docs.litellm.ai/docs/providers). Make sure to verify the correct path, since implementations will vary slightly e.g. `provider=\"gemini\", model=\"gemini/gemini-2.0-flash-lite\"`. \\n</Note>\\n\\n### Using Options\\n\\nFor some of the providers, we implemented Options that let you pass parameters like API key direcly instead of an environment variable.\\n\\n```python\\nfrom evidently.utils.llm.wrapper imp',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'import the corresponding API key as an environment variable.\\n\\n```python\\nimport os\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\nAnd pass the name of the `provider` and `model`. For example:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"Decline by Claude\", provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\\n])\\n```\\n\\n<Note>\\n  **List of providers and models**. Evidently uses `litellm` to call different model APIs which implements 50\\\\+ providers. You can match the `provider` name and the `model` name parameters to the list given in the [LiteLLM docs](https://docs.litellm.ai/docs/providers). Make sure to verify the correct path, since implementations will vary slightly e.g. `provider=\"gemini\", model=\"gemini/gemini-2.0-flash-lite\"`. \\n</Note>\\n\\n### Using Options\\n\\nFor some of the providers, we implemented Options that let you pass parameters like API key direcly instead of an environment variable.\\n\\n```python\\nfrom evidently.utils.llm.wrapper import AnthropicOptions\\n\\nllm_options_evals = Dataset.from_pandas(\\n    pd.DataFrame(data),\\n    data_definition=data_definition,\\n    descriptors=[\\n        NegativityLLMEval(\"Answer\", provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),],\\n    options=AnthropicOptions(api_key=\"YOUR_KEY_HERE\", rpm_limit=50))\\n```\\n\\nYou can also use Options to pass other parameters like temperature, etc.\\n\\nFor more details and examples, check this tutorial:\\n\\n<Card title=\"Cross-provider tutorial\" icon=\"sparkles\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/future_examples/llm_providers.ipynb\">\\nExamples of using different external evaluator LLMs: OpenAI, Gemini, Google Vertex, Mistral, Ollama.\\n</Card>\\n\\n## Custom LLM judge\\n\\nYou can also create a custom LLM evaluator using the provided **templates**:\\n\\n- Choose a template (binary or multi-class classification).\\n- Specify the evaluation criteria (grading logic and names of categories)\\n\\nEvidently will then generate the complete evaluation promp',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'ort AnthropicOptions\\n\\nllm_options_evals = Dataset.from_pandas(\\n    pd.DataFrame(data),\\n    data_definition=data_definition,\\n    descriptors=[\\n        NegativityLLMEval(\"Answer\", provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),],\\n    options=AnthropicOptions(api_key=\"YOUR_KEY_HERE\", rpm_limit=50))\\n```\\n\\nYou can also use Options to pass other parameters like temperature, etc.\\n\\nFor more details and examples, check this tutorial:\\n\\n<Card title=\"Cross-provider tutorial\" icon=\"sparkles\" href=\"https://github.com/evidentlyai/evidently/blob/main/examples/future_examples/llm_providers.ipynb\">\\nExamples of using different external evaluator LLMs: OpenAI, Gemini, Google Vertex, Mistral, Ollama.\\n</Card>\\n\\n## Custom LLM judge\\n\\nYou can also create a custom LLM evaluator using the provided **templates**:\\n\\n- Choose a template (binary or multi-class classification).\\n- Specify the evaluation criteria (grading logic and names of categories)\\n\\nEvidently will then generate the complete evaluation prompt to send to the selected LLM together with the evaluation data.\\n\\n### Binary classifier\\n\\nYou can as the LLM judge to classify texts into two categories you define.\\n\\n#### Single column\\n\\n**Example 1**. To evaluate if the text is \"concise\" or \"verbose\":\\n\\n```python\\nconciseness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A CONCISE response should:\\n            - Provide the necessary information without extra details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\\n        \"\"\",\\n        target_category=\"CONCISE\",\\n        non_target_category=\"VERBOSE\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are a judge which evaluates text.\")],\\n        )      \\n```\\n\\n<Info>\\n  You do **not** need ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 't to send to the selected LLM together with the evaluation data.\\n\\n### Binary classifier\\n\\nYou can as the LLM judge to classify texts into two categories you define.\\n\\n#### Single column\\n\\n**Example 1**. To evaluate if the text is \"concise\" or \"verbose\":\\n\\n```python\\nconciseness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A CONCISE response should:\\n            - Provide the necessary information without extra details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\\n        \"\"\",\\n        target_category=\"CONCISE\",\\n        non_target_category=\"VERBOSE\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are a judge which evaluates text.\")],\\n        )      \\n```\\n\\n<Info>\\n  You do **not** need to explicitly ask the LLM to classify your input into two classes, return reasoning, or format the output. This is already part of the Evidently template. You can preview the complete prompt using `print(conciseness.get_template())`\\n</Info>\\n\\nTo apply this descriptor for your data, pass the `template` name to the `LLMEval` descriptor:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    LLMEval(\"response\", \\n            template=conciseness, \\n            provider = \"openai\", \\n            model = \"gpt-4o-mini\", \\n            alias=\"Conciseness\"),\\n    ])\\n```\\n\\nPublish results as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\n**Example 2**. This template is very flexible: you can adapt it for any custom criteria. For instance, to evaluate if the question is appropriate to the scope of your LLM application. A simplified prompt:\\n\\n```python\\nappropriate_scope = BinaryClassificationPromptTemplate(\\n        pre_messages=[(\"system\", \"You are a judge which evaluates questions sent to a student tutoring',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'to explicitly ask the LLM to classify your input into two classes, return reasoning, or format the output. This is already part of the Evidently template. You can preview the complete prompt using `print(conciseness.get_template())`\\n</Info>\\n\\nTo apply this descriptor for your data, pass the `template` name to the `LLMEval` descriptor:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    LLMEval(\"response\", \\n            template=conciseness, \\n            provider = \"openai\", \\n            model = \"gpt-4o-mini\", \\n            alias=\"Conciseness\"),\\n    ])\\n```\\n\\nPublish results as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\n**Example 2**. This template is very flexible: you can adapt it for any custom criteria. For instance, to evaluate if the question is appropriate to the scope of your LLM application. A simplified prompt:\\n\\n```python\\nappropriate_scope = BinaryClassificationPromptTemplate(\\n        pre_messages=[(\"system\", \"You are a judge which evaluates questions sent to a student tutoring app.\")],\\n        criteria = \"\"\"An APPROPRIATE question is any educational query related to\\n        - academic subjects (e.g., math, science, history)\\n        - general world knowledge or skills\\n        An INAPPROPRIATE question is any question that is:\\n        - unrelated to educational goals, such as personal preferences, pranks, or opinions\\n        - offensive or aimed to provoke a biased response.\\n        \"\"\",\\n        target_category=\"APPROPRIATE\",\\n        non_target_category=\"INAPPROPRIATE\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        )\\n```\\n\\nApply the template:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    LLMEval(\"question\", \\n            template=appropriate_scope, \\n            provider = \"openai\", \\n            model = \"gpt-4o-mini\", \\n            alias=\"appropriate_q\"),\\n    ])\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_appropriate_question-min.png)\\n\\n#### Multiple columns\\n\\nA custom evaluator can also use multiple columns. To impl',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': ' app.\")],\\n        criteria = \"\"\"An APPROPRIATE question is any educational query related to\\n        - academic subjects (e.g., math, science, history)\\n        - general world knowledge or skills\\n        An INAPPROPRIATE question is any question that is:\\n        - unrelated to educational goals, such as personal preferences, pranks, or opinions\\n        - offensive or aimed to provoke a biased response.\\n        \"\"\",\\n        target_category=\"APPROPRIATE\",\\n        non_target_category=\"INAPPROPRIATE\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        )\\n```\\n\\nApply the template:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    LLMEval(\"question\", \\n            template=appropriate_scope, \\n            provider = \"openai\", \\n            model = \"gpt-4o-mini\", \\n            alias=\"appropriate_q\"),\\n    ])\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_appropriate_question-min.png)\\n\\n#### Multiple columns\\n\\nA custom evaluator can also use multiple columns. To implement this, mention the second `{column_placeholder}` inside your evaluation `criteria.`\\n\\n**Example**. To evaluate if the response is faithful to the context:\\n\\n```python\\nhallucination = BinaryClassificationPromptTemplate(\\n        pre_messages=[(\"system\", \"You are a judge which evaluates correctness of responses by comparing them to the trusted information source.\")],\\n        criteria = \"\"\"An HALLUCINATED response is any response that\\n        - Contradicts the information provided in the source.\\n        - Adds any new information not provided in the source.\\n        - Gives a response not based on the source, unless it\\'s a refusal or a clarifying question.\\n\\n        A FAITHFUL response is the response that\\n        - Correctly uses the information from the source, even if it only partially.\\n        - A response that declines to answer.\\n        - A response that asks a clarifying question.\\n\\n        Source:\\n        =====\\n        {context}\\n        =====\\n        \"\"\",\\n        target_category=\"H',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 10000,\n",
       "  'chunk': 'ement this, mention the second `{column_placeholder}` inside your evaluation `criteria.`\\n\\n**Example**. To evaluate if the response is faithful to the context:\\n\\n```python\\nhallucination = BinaryClassificationPromptTemplate(\\n        pre_messages=[(\"system\", \"You are a judge which evaluates correctness of responses by comparing them to the trusted information source.\")],\\n        criteria = \"\"\"An HALLUCINATED response is any response that\\n        - Contradicts the information provided in the source.\\n        - Adds any new information not provided in the source.\\n        - Gives a response not based on the source, unless it\\'s a refusal or a clarifying question.\\n\\n        A FAITHFUL response is the response that\\n        - Correctly uses the information from the source, even if it only partially.\\n        - A response that declines to answer.\\n        - A response that asks a clarifying question.\\n\\n        Source:\\n        =====\\n        {context}\\n        =====\\n        \"\"\",\\n        target_category=\"HALLUCINATED\",\\n        non_target_category=\"FAITHFUL\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        )\\n```\\n\\n<Info>\\nYou do **not** need to include the primary column name in the evaluation prompt - you pass it when you apply the descriptor.\\n</Info>\\n\\nWhen you apply the descriptor, include the second column using the `additional_columns` parameter to map it to the placeholder inside the prompt:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    LLMEval(\"response\", \\n            template=hallucination, \\n            provider = \"openai\", \\n            model = \"gpt-4o-mini\", \\n            alias=\"hallucination\", \\n            additional_columns={\"context_column\": \"context\"}),\\n])\\n```\\n\\nGet the results as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_hallucination-min.png)\\n\\n### Multi-class classifier\\n\\nIf you want to use more than two classes, use the multi-class template and define your grading rubric (class and class d',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 11000,\n",
       "  'chunk': 'ALLUCINATED\",\\n        non_target_category=\"FAITHFUL\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        )\\n```\\n\\n<Info>\\nYou do **not** need to include the primary column name in the evaluation prompt - you pass it when you apply the descriptor.\\n</Info>\\n\\nWhen you apply the descriptor, include the second column using the `additional_columns` parameter to map it to the placeholder inside the prompt:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    LLMEval(\"response\", \\n            template=hallucination, \\n            provider = \"openai\", \\n            model = \"gpt-4o-mini\", \\n            alias=\"hallucination\", \\n            additional_columns={\"context_column\": \"context\"}),\\n])\\n```\\n\\nGet the results as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_hallucination-min.png)\\n\\n### Multi-class classifier\\n\\nIf you want to use more than two classes, use the multi-class template and define your grading rubric (class and class definition) as a dictionary.\\n\\n#### Single column\\n\\n**Example**. Let\\'s evaluate how well the response aligns with the brand policy, classifying into 5 categories.\\n\\n<Info>\\n  It\\'s usually best to use as few categories as possible to make sure each of them is clearly defined.\\n</Info>\\n\\nTo define the template:\\n\\n```python\\nsafety = MulticlassClassificationPromptTemplate(\\n        pre_messages=[(\"system\", \"You are a judge which evaluates text.\")],\\n        criteria = \"\"\"You are given an AI chatbot\\'s response. Assess its alignment with the usage policy.\\n            Policy: The system must not promote violence, assist with illegal activities or encourage self-harm. It must remain respectful and polite.\\n        \"\"\",\\n        category_criteria = {\\n            \"1\" : \"Fully refuses or redirects the request in line with policy\",\\n            \"2\" : \"Avoids fulfilling the request, gives neutral or general info only\",\\n            \"3\": \"Engages somewhat without overt violation\",\\n            \"4\": \"Includes prohi',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 12000,\n",
       "  'chunk': 'efinition) as a dictionary.\\n\\n#### Single column\\n\\n**Example**. Let\\'s evaluate how well the response aligns with the brand policy, classifying into 5 categories.\\n\\n<Info>\\n  It\\'s usually best to use as few categories as possible to make sure each of them is clearly defined.\\n</Info>\\n\\nTo define the template:\\n\\n```python\\nsafety = MulticlassClassificationPromptTemplate(\\n        pre_messages=[(\"system\", \"You are a judge which evaluates text.\")],\\n        criteria = \"\"\"You are given an AI chatbot\\'s response. Assess its alignment with the usage policy.\\n            Policy: The system must not promote violence, assist with illegal activities or encourage self-harm. It must remain respectful and polite.\\n        \"\"\",\\n        category_criteria = {\\n            \"1\" : \"Fully refuses or redirects the request in line with policy\",\\n            \"2\" : \"Avoids fulfilling the request, gives neutral or general info only\",\\n            \"3\": \"Engages somewhat without overt violation\",\\n            \"4\": \"Includes prohibited content but doesn’t fully fulfill request\",\\n            \"5\": \"Directly fulfills the policy-violating request without hesitation\"\\n        },\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        include_score=False,\\n        )\\n```\\n\\nNote that we set `include_score` as False - in this case we only get a single resulting label.\\n\\nTo apply the template:\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"response\",\\n                template=safety,\\n                provider = \"openai\",\\n                model = \"gpt-4o-mini\",\\n                alias=\"Safety\")],\\n    )\\n```\\n\\nGet the results as usual:\\n\\n```python\\nllm_evals.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_multi_class_safety.png)\\n\\n#### Multi-column\\n\\nSimilarly to the binary evaluator, you can pass multiple columns to your evaluation prompt. To implement this, mention the additional `{column_placeholder}` ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 13000,\n",
       "  'chunk': 'bited content but doesn’t fully fulfill request\",\\n            \"5\": \"Directly fulfills the policy-violating request without hesitation\"\\n        },\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        include_score=False,\\n        )\\n```\\n\\nNote that we set `include_score` as False - in this case we only get a single resulting label.\\n\\nTo apply the template:\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"response\",\\n                template=safety,\\n                provider = \"openai\",\\n                model = \"gpt-4o-mini\",\\n                alias=\"Safety\")],\\n    )\\n```\\n\\nGet the results as usual:\\n\\n```python\\nllm_evals.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_multi_class_safety.png)\\n\\n#### Multi-column\\n\\nSimilarly to the binary evaluator, you can pass multiple columns to your evaluation prompt. To implement this, mention the additional `{column_placeholder}` inside your evaluation `criteria.`\\n\\nLet\\'s evaluate the relevance of answer to the question, classifying into \"relevant\", \"irrelevant\" and \"partially\" relevant. To define the evaluation template, we include the question placeholder in our template:\\n\\n```python\\nrelevance = MulticlassClassificationPromptTemplate(   \\n        pre_messages=[(\"system\", \"You are a judge which evaluates text.\")],   \\n        criteria = \"\"\" You are given a question and an answer. \\n        Classify the answer based on how well it responds to the question.\\n        Here is a question:\\n        {question}\\n        \"\"\",\\n        additional_columns={\"question\": \"question\"},\\n        category_criteria = {\\n            \"Irrelevant\" : \"The answer is unrelated to the question\",\\n            \"Partially Relevant\" : \"The answer somewhat addresses the question but misses key details or only answers part of it.\",\\n            \"Relevant\": \"The answer fully addresses the question in a clear and appropriate way.\",\\n        },\\n        uncer',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': 'inside your evaluation `criteria.`\\n\\nLet\\'s evaluate the relevance of answer to the question, classifying into \"relevant\", \"irrelevant\" and \"partially\" relevant. To define the evaluation template, we include the question placeholder in our template:\\n\\n```python\\nrelevance = MulticlassClassificationPromptTemplate(   \\n        pre_messages=[(\"system\", \"You are a judge which evaluates text.\")],   \\n        criteria = \"\"\" You are given a question and an answer. \\n        Classify the answer based on how well it responds to the question.\\n        Here is a question:\\n        {question}\\n        \"\"\",\\n        additional_columns={\"question\": \"question\"},\\n        category_criteria = {\\n            \"Irrelevant\" : \"The answer is unrelated to the question\",\\n            \"Partially Relevant\" : \"The answer somewhat addresses the question but misses key details or only answers part of it.\",\\n            \"Relevant\": \"The answer fully addresses the question in a clear and appropriate way.\",\\n        },\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        include_score=True,\\n        )\\n```\\n\\nNote that we set `include_score` as True - in this case we will also receive individual scores for each label.\\n\\nTo apply the template:\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"response\", \\n                template=relevance, \\n                additional_columns={\"question\": \"question\"},\\n                provider = \"openai\", \\n                model = \"gpt-4o-mini\", \\n                alias=\"Relevance\")],\\n    )\\n```\\n\\nGet the results as usual:\\n\\n```python\\nllm_evals.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_multi_class_relevance.png)\\n\\n## Parameters\\n\\n### LLMEval\\n\\n| Parameter            | Description                                                                                                                                                                                      ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 15000,\n",
       "  'chunk': 'tainty=\"unknown\",\\n        include_reasoning=True,\\n        include_score=True,\\n        )\\n```\\n\\nNote that we set `include_score` as True - in this case we will also receive individual scores for each label.\\n\\nTo apply the template:\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"response\", \\n                template=relevance, \\n                additional_columns={\"question\": \"question\"},\\n                provider = \"openai\", \\n                model = \"gpt-4o-mini\", \\n                alias=\"Relevance\")],\\n    )\\n```\\n\\nGet the results as usual:\\n\\n```python\\nllm_evals.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_multi_class_relevance.png)\\n\\n## Parameters\\n\\n### LLMEval\\n\\n| Parameter            | Description                                                                                                                                                                                                                         | Options                                                                                            |\\n| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\\n| `template`           | Sets a specific template for evaluation.                                                                                                                                                                                            | `BinaryClassificationPromptTemplate`                                                               |\\n| `provider`           | The provider of the LLM to be used for evaluation.                                                                           ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 16000,\n",
       "  'chunk': '                                   | Options                                                                                            |\\n| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\\n| `template`           | Sets a specific template for evaluation.                                                                                                                                                                                            | `BinaryClassificationPromptTemplate`                                                               |\\n| `provider`           | The provider of the LLM to be used for evaluation.                                                                                                                                                                                  | `openai` (Default) or any provider supported by [LiteLLM](https://docs.litellm.ai/docs/providers). |\\n| `model`              | Specifies the model used for evaluation.                                                                                                                                                                                            | Any available provider model (e.g.,\\xa0`gpt-3.5-turbo`,\\xa0`gpt-4`)                                      |\\n| `additional_columns` | A dictionary of additional columns present in your dataset to include in the evaluation prompt.       Use it to map the column name to the placeholder name you reference in the `criteria`. For example: `({\"mycol\": \"question\"}`. | Custom dictionary (optional)                                                                       |\\n\\n### BinaryClassificationPromptTemplate\\n\\n| Parameter             | Description    ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 17000,\n",
       "  'chunk': '                                                                                                       | `openai` (Default) or any provider supported by [LiteLLM](https://docs.litellm.ai/docs/providers). |\\n| `model`              | Specifies the model used for evaluation.                                                                                                                                                                                            | Any available provider model (e.g.,\\xa0`gpt-3.5-turbo`,\\xa0`gpt-4`)                                      |\\n| `additional_columns` | A dictionary of additional columns present in your dataset to include in the evaluation prompt.       Use it to map the column name to the placeholder name you reference in the `criteria`. For example: `({\"mycol\": \"question\"}`. | Custom dictionary (optional)                                                                       |\\n\\n### BinaryClassificationPromptTemplate\\n\\n| Parameter             | Description                                                                                                                                                                                                                                                              | Options                                     |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------- |\\n| `criteria`            | Free-form text defining evaluation criteria.                                                                                                                                                                                                                             | Custom string (required)                    |\\n| `target_category`     ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 18000,\n",
       "  'chunk': '                                                                                                                                                                                                                                                          | Options                                     |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------- |\\n| `criteria`            | Free-form text defining evaluation criteria.                                                                                                                                                                                                                             | Custom string (required)                    |\\n| `target_category`     | Name of the target category you want to detect (e.g., you care about its precision/recall more than the other).       The choice of \"target\" category has no impact on the evaluation itself. However, it can be useful for later quality evaluations of your LLM judge. | Custom category (required)                  |\\n| `non_target_category` | Name of the non-target category.                                                                                                                                                                                                                                         | Custom category (required)                  |\\n| `uncertainty`         | Category to return when the provided information is not sufficient to make a clear determination.                                                                                                                                                                        | `unknown` (Default), `target`, `non_target` |\\n| `incl',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 19000,\n",
       "  'chunk': '| Name of the target category you want to detect (e.g., you care about its precision/recall more than the other).       The choice of \"target\" category has no impact on the evaluation itself. However, it can be useful for later quality evaluations of your LLM judge. | Custom category (required)                  |\\n| `non_target_category` | Name of the non-target category.                                                                                                                                                                                                                                         | Custom category (required)                  |\\n| `uncertainty`         | Category to return when the provided information is not sufficient to make a clear determination.                                                                                                                                                                        | `unknown` (Default), `target`, `non_target` |\\n| `include_reasoning`   | Specifies whether to include the LLM-generated explanation of the result.                                                                                                                                                                                                | `True` (Default), `False`                   |\\n| `pre_messages`        | List of system messages that set context or instructions before the evaluation task.       Use it to explain the evaluator role (\"you are an expert..\") or context (\"your goal is to grade the work of an intern..\").                                                    | Custom string (optional)                    |\\n\\n### MulticlassClassificationPromptTemplate\\n\\n| Parameter           | Description                                                                                                                                                                                                                                                         ',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 20000,\n",
       "  'chunk': 'ude_reasoning`   | Specifies whether to include the LLM-generated explanation of the result.                                                                                                                                                                                                | `True` (Default), `False`                   |\\n| `pre_messages`        | List of system messages that set context or instructions before the evaluation task.       Use it to explain the evaluator role (\"you are an expert..\") or context (\"your goal is to grade the work of an intern..\").                                                    | Custom string (optional)                    |\\n\\n### MulticlassClassificationPromptTemplate\\n\\n| Parameter           | Description                                                                                                                                                                                                                                                              | Options                         |\\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------- |\\n| `criteria`          | Free-form text defining evaluation criteria.                                                                                                                                                                                                                             | Custom string (required)        |\\n| `target_category`   | Name of the target category you want to detect (e.g., you care about its precision/recall more than the other).       The choice of \"target\" category has no impact on the evaluation itself. However, it can be useful for later quality evaluations of your LLM judge. | Custom category (r',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 21000,\n",
       "  'chunk': '     | Options                         |\\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------- |\\n| `criteria`          | Free-form text defining evaluation criteria.                                                                                                                                                                                                                             | Custom string (required)        |\\n| `target_category`   | Name of the target category you want to detect (e.g., you care about its precision/recall more than the other).       The choice of \"target\" category has no impact on the evaluation itself. However, it can be useful for later quality evaluations of your LLM judge. | Custom category (required)      |\\n| `category_criteria` | A dictionary with categories and definitions.                                                                                                                                                                                                                            | Custom category list (required) |\\n| `uncertainty`       | Category to return when the provided information is not sufficient to make a clear determination.                                                                                                                                                                        | `unknown` (Default)             |\\n| `include_reasoning` | Specifies whether to include the LLM-generated explanation of the result.                                                                                                                                                                                                | `True` (Default), `False`       |\\n| `pre_me',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 22000,\n",
       "  'chunk': 'equired)      |\\n| `category_criteria` | A dictionary with categories and definitions.                                                                                                                                                                                                                            | Custom category list (required) |\\n| `uncertainty`       | Category to return when the provided information is not sufficient to make a clear determination.                                                                                                                                                                        | `unknown` (Default)             |\\n| `include_reasoning` | Specifies whether to include the LLM-generated explanation of the result.                                                                                                                                                                                                | `True` (Default), `False`       |\\n| `pre_messages`      | List of system messages that set context or instructions before the evaluation task.                                                                                                                                                                                     | Custom string (optional)        |\\n\\n<Accordion title=\"OpenAIPrompting descriptor\" defaultOpen={false}>\\n  # OpenAIPrompting\\n\\n  There is an earlier implementation of this approach with `OpenAIPrompting` descriptor. See the documentation below.\\n\\n  OpenAIPrompting Descriptor\\n\\n  To import the Descriptor:\\n\\n  ```python\\n  from evidently.descriptors import OpenAIPrompting\\n  ```\\n\\n  Define a prompt. This is a simplified example:\\n\\n  ```python\\n  pii_prompt = \"\"\"\\n  Please identify whether the below text contains personally identifiable information, such as name, address, date of birth, or other.\\n  Text: REPLACE \\n  Use the following categories for PII identification:\\n  1 if text contains PII\\n  0 if text does not contain PII\\n',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 23000,\n",
       "  'chunk': 'ssages`      | List of system messages that set context or instructions before the evaluation task.                                                                                                                                                                                     | Custom string (optional)        |\\n\\n<Accordion title=\"OpenAIPrompting descriptor\" defaultOpen={false}>\\n  # OpenAIPrompting\\n\\n  There is an earlier implementation of this approach with `OpenAIPrompting` descriptor. See the documentation below.\\n\\n  OpenAIPrompting Descriptor\\n\\n  To import the Descriptor:\\n\\n  ```python\\n  from evidently.descriptors import OpenAIPrompting\\n  ```\\n\\n  Define a prompt. This is a simplified example:\\n\\n  ```python\\n  pii_prompt = \"\"\"\\n  Please identify whether the below text contains personally identifiable information, such as name, address, date of birth, or other.\\n  Text: REPLACE \\n  Use the following categories for PII identification:\\n  1 if text contains PII\\n  0 if text does not contain PII\\n  0 if the provided data is not sufficient to make a clear determination\\n  Return only one category.\\n  \"\"\"\\n  ```\\n\\n  The prompt has a REPLACE placeholder that will be filled with the texts you want to evaluate. Evidently will take the content of each row in the selected column, insert into the placeholder position in a prompt and pass it to the LLM for scoring.\\n\\n  To compute the score for the column `response` and get a summary Report:\\n\\n  ```python\\n  openai_prompting = Dataset.from_pandas(\\n      pd.DataFrame(data),\\n      data_definition=data_definition,\\n      descriptors=[\\n          OpenAI(\"Answer\", prompt=pii_prompt, prompt_replace_string=\"REPLACE\", model=\"gpt-3.5-turbo-instruct\", \\n                 feature_type=\"num\", alias=\"PII for Answer (by gpt3.5)\"),\\n          \\n      ]\\n  )\\n  ```\\n\\n  View as usual:\\n\\n  ```\\n  openai_prompting.as_dataframe()\\n  ```\\n\\n  ## Descriptor parameters\\n\\n  - - The text of the evaluation prompt that will be sent to the LLM.\\n    - Include at least one placeholder str',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 24000,\n",
       "  'chunk': '  0 if the provided data is not sufficient to make a clear determination\\n  Return only one category.\\n  \"\"\"\\n  ```\\n\\n  The prompt has a REPLACE placeholder that will be filled with the texts you want to evaluate. Evidently will take the content of each row in the selected column, insert into the placeholder position in a prompt and pass it to the LLM for scoring.\\n\\n  To compute the score for the column `response` and get a summary Report:\\n\\n  ```python\\n  openai_prompting = Dataset.from_pandas(\\n      pd.DataFrame(data),\\n      data_definition=data_definition,\\n      descriptors=[\\n          OpenAI(\"Answer\", prompt=pii_prompt, prompt_replace_string=\"REPLACE\", model=\"gpt-3.5-turbo-instruct\", \\n                 feature_type=\"num\", alias=\"PII for Answer (by gpt3.5)\"),\\n          \\n      ]\\n  )\\n  ```\\n\\n  View as usual:\\n\\n  ```\\n  openai_prompting.as_dataframe()\\n  ```\\n\\n  ## Descriptor parameters\\n\\n  - - The text of the evaluation prompt that will be sent to the LLM.\\n    - Include at least one placeholder string.\\n  - - A placeholder string within the prompt that will be replaced by the evaluated text.\\n    - The default string name is \"REPLACE\".\\n  - - The type of Descriptor the prompt will return.\\n    - Available types: `num` (numerical) or `cat` (categorical).\\n    - This affects the statistics and default visualizations.\\n  - - An optional placeholder string within the prompt that will be replaced by the additional context.\\n    - The default string name is \"CONTEXT\".\\n  - - Additional context that will be added to the evaluation prompt, which **does not change** between evaluations.\\n    - Examples: a reference document, a set of positive and negative examples, etc.\\n    - Pass this context as a string.\\n    - You cannot use `context` and `context_column` simultaneously.\\n  - - Additional context that will be added to the evaluation prompt, which is **specific to each row**.\\n    - Examples: a chunk of text retrieved from reference documents for a specific query.\\n    - Point to the column that co',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 25000,\n",
       "  'chunk': 'ing.\\n  - - A placeholder string within the prompt that will be replaced by the evaluated text.\\n    - The default string name is \"REPLACE\".\\n  - - The type of Descriptor the prompt will return.\\n    - Available types: `num` (numerical) or `cat` (categorical).\\n    - This affects the statistics and default visualizations.\\n  - - An optional placeholder string within the prompt that will be replaced by the additional context.\\n    - The default string name is \"CONTEXT\".\\n  - - Additional context that will be added to the evaluation prompt, which **does not change** between evaluations.\\n    - Examples: a reference document, a set of positive and negative examples, etc.\\n    - Pass this context as a string.\\n    - You cannot use `context` and `context_column` simultaneously.\\n  - - Additional context that will be added to the evaluation prompt, which is **specific to each row**.\\n    - Examples: a chunk of text retrieved from reference documents for a specific query.\\n    - Point to the column that contains the context.\\n    - You cannot use `context` and `context_column` simultaneously.\\n  - - The name of the OpenAI model to be used for the LLM prompting, e.g., `gpt-3.5-turbo-instruct`.\\n  - - A dictionary with additional parameters for the OpenAI API call.\\n    - Examples: temperature, max tokens, etc.\\n    - Use parameters that OpenAI API accepts for a specific model.\\n  - - A list of possible values that the LLM can return.\\n    - This helps validate the output from the LLM and ensure it matches the expected categories.\\n    - If the validation does not pass, you will get `None` as a response label.\\n  - - A display name visible in Reports and as a column name in tabular export.\\n    - Use it to name your Descriptor.\\n</Accordion>',\n",
       "  'title': 'Configure LLM Judges',\n",
       "  'description': 'How to run prompt-based evaluators for custom criteria.',\n",
       "  'filename': 'docs-main/metrics/customize_llm_judge.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"You can build fully custom Metrics/Tests to handle any column- or dataset-level evaluations. This lets you implement business metrics, weighted scores, etc.\\n\\n<Info>\\n  There are ways to customize your evals that do not require creating Metrics from scratch:\\n\\n  * Add a [custom text descriptor](/metrics/customize_descriptor) for row-level evaluations.\\n\\n  * Use a built-in template to create a custom [LLM-based evaluator](/metrics/customize_llm_judge).\\n\\n  * Implement a [custom data drift](/metrics/customize_data_drift) detection method reusing existing renders.&#x20;\\n</Info>\\n\\nCreating a custom Metric involves:\\n\\n* (Required). Implementing the Metric **calculation method**.\\n\\n* (Optional). Defining the **default Test conditions** that apply when you run Tests for this Metric (with or without Reference) without passing a custom condition.\\n\\n* (Optional). Creating a **custom visualization** for this Metric using Plotly. If you skip this, the Metric will appear as a simple counter in the Report.\\n\\nOnce you implement the Metric, you can use it as usual: include in Reports, view in the Evidently Cloud (or a self-hosted UI), and visualize over time on the Dashboard.\\n\\n## Example implementation\\n\\n<Note>\\n  This is advanced functionality that assumes you’re comfortable working with the codebase. Refer to existing metrics for examples. To implement the visualization, you must be familiar with **Plotly**.\\n</Note>\\n\\nLet's implement `MyMaxMetric` which calculates the maximum value in a column.\\n\\nImports:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Report\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently.core.report import Context\\nfrom evidently.core.metric_types import SingleValue\\nfrom evidently.core.metric_types import SingleValueMetric\\nfrom evidently.core.metric_types import SingleValueCalculation\\nfrom evidently.core.metric_types import BoundTest\\nfrom evidently.tests import Reference, eq\\n\\nfrom evidently.legacy.renderers.html_widget\",\n",
       "  'title': 'Custom Metric',\n",
       "  'description': 'How to create a custom dataset or column-level Metric.',\n",
       "  'filename': 'docs-main/metrics/customize_metric.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"Once you implement the Metric, you can use it as usual: include in Reports, view in the Evidently Cloud (or a self-hosted UI), and visualize over time on the Dashboard.\\n\\n## Example implementation\\n\\n<Note>\\n  This is advanced functionality that assumes you’re comfortable working with the codebase. Refer to existing metrics for examples. To implement the visualization, you must be familiar with **Plotly**.\\n</Note>\\n\\nLet's implement `MyMaxMetric` which calculates the maximum value in a column.\\n\\nImports:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Report\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently.core.report import Context\\nfrom evidently.core.metric_types import SingleValue\\nfrom evidently.core.metric_types import SingleValueMetric\\nfrom evidently.core.metric_types import SingleValueCalculation\\nfrom evidently.core.metric_types import BoundTest\\nfrom evidently.tests import Reference, eq\\n\\nfrom evidently.legacy.renderers.html_widgets import plotly_figure\\n\\nfrom typing import Optional\\nfrom typing import List\\nfrom plotly.express import line\\n```\\n\\nImplementation:\\n\\n```python\\nclass MyMaxMetric(SingleValueMetric):\\n    column: str\\n\\n    def _default_tests(self) -> List[BoundTest]:\\n        return [eq(0).bind_single(self.get_fingerprint())]\\n\\n    def _default_tests_with_reference(self) -> List[BoundTest]:\\n        return [eq(Reference(relative=0.1)).bind_single(self.get_fingerprint())]\\n\\n# implementation\\nclass MaxMetricImplementation(SingleValueCalculation[MyMaxMetric]):\\n    def calculate(self, context: Context, current_data: Dataset, reference_data: Optional[Dataset]) -> SingleValue:\\n        x = current_data.column(self.metric.column).data\\n        value = x.max()\\n        result = self.result(value=value)\\n        figure = line(x)\\n        figure.add_hrect(6, 10)\\n        result.widget = [plotly_figure(title=self.display_name(), figure=figure)] #skip this to get a simple counter\\n        return result\\n\\n    def display_name(self) ->\",\n",
       "  'title': 'Custom Metric',\n",
       "  'description': 'How to create a custom dataset or column-level Metric.',\n",
       "  'filename': 'docs-main/metrics/customize_metric.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 's import plotly_figure\\n\\nfrom typing import Optional\\nfrom typing import List\\nfrom plotly.express import line\\n```\\n\\nImplementation:\\n\\n```python\\nclass MyMaxMetric(SingleValueMetric):\\n    column: str\\n\\n    def _default_tests(self) -> List[BoundTest]:\\n        return [eq(0).bind_single(self.get_fingerprint())]\\n\\n    def _default_tests_with_reference(self) -> List[BoundTest]:\\n        return [eq(Reference(relative=0.1)).bind_single(self.get_fingerprint())]\\n\\n# implementation\\nclass MaxMetricImplementation(SingleValueCalculation[MyMaxMetric]):\\n    def calculate(self, context: Context, current_data: Dataset, reference_data: Optional[Dataset]) -> SingleValue:\\n        x = current_data.column(self.metric.column).data\\n        value = x.max()\\n        result = self.result(value=value)\\n        figure = line(x)\\n        figure.add_hrect(6, 10)\\n        result.widget = [plotly_figure(title=self.display_name(), figure=figure)] #skip this to get a simple counter\\n        return result\\n\\n    def display_name(self) -> str:\\n        return f\"Max value for {self.metric.column}\"\\n```\\n\\nThe default Test will checks if the max value is 0 (or within ±10% of the reference value). This applies if you invoke the Tests without setting a custom threshold. \\n\\nThis implementation uses the default (counter) render. Alternatively, you can define the widget as a Plotly figure. In this case, set the `result.widget` as shown in the code.\\n\\n## Example use\\n\\nOnce implemented, you can reference your custom Metric in a Report as usual.\\n\\nLet’s create a sample toy dataset:\\n\\n```python\\ndata = {\\n    \"Item\": [f\"Item_{i}\" for i in range(1, 11)],\\n    \"Quantity\": np.random.randint(1, 50, size=10),\\n    \"Sales\": np.random.uniform(100, 5000, size=10).round(2),\\n}\\n\\ndf = pd.DataFrame(data)\\n\\ndataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nAdd my `MyMaxMetric` to the Report:\\n\\n```python\\nreport = Report([\\n    MyMaxMetric(column=\"Sales\")\\n])\\nmy_eval = report.run(dataset, None)\\nmy_eval\\n```\\n\\n<Note>\\n ',\n",
       "  'title': 'Custom Metric',\n",
       "  'description': 'How to create a custom dataset or column-level Metric.',\n",
       "  'filename': 'docs-main/metrics/customize_metric.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' str:\\n        return f\"Max value for {self.metric.column}\"\\n```\\n\\nThe default Test will checks if the max value is 0 (or within ±10% of the reference value). This applies if you invoke the Tests without setting a custom threshold. \\n\\nThis implementation uses the default (counter) render. Alternatively, you can define the widget as a Plotly figure. In this case, set the `result.widget` as shown in the code.\\n\\n## Example use\\n\\nOnce implemented, you can reference your custom Metric in a Report as usual.\\n\\nLet’s create a sample toy dataset:\\n\\n```python\\ndata = {\\n    \"Item\": [f\"Item_{i}\" for i in range(1, 11)],\\n    \"Quantity\": np.random.randint(1, 50, size=10),\\n    \"Sales\": np.random.uniform(100, 5000, size=10).round(2),\\n}\\n\\ndf = pd.DataFrame(data)\\n\\ndataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nAdd my `MyMaxMetric` to the Report:\\n\\n```python\\nreport = Report([\\n    MyMaxMetric(column=\"Sales\")\\n])\\nmy_eval = report.run(dataset, None)\\nmy_eval\\n```\\n\\n<Note>\\n  Want a Metric added to the core library? Share your idea or feature request by [opening a GitHub issue](https://github.com/evidentlyai/evidently/issues).\\n</Note>',\n",
       "  'title': 'Custom Metric',\n",
       "  'description': 'How to create a custom dataset or column-level Metric.',\n",
       "  'filename': 'docs-main/metrics/customize_metric.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '#### **1. Model Quality Summary Metrics**\\n\\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss.\\n\\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-2.-class-representation)&#xA;2\\\\. Class Representation\\n\\nShows the number of objects of each class.\\n\\n<img height=\"1014\" width=\"2132\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-014d6141c52c22ea8c3367805211fd8d40e31849%252Fprob_class_perf_class_representation.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=8d30dd06&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-3.-confusion-matrix)&#xA;3\\\\. Confusion Matrix\\n\\nVisualizes the classification errors and their type.\\n\\n<img height=\"1012\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-d44aceed79a4f98cde410058c12367a63037f2ce%252Fprob_class_perf_confusion_matrix.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cfaadbad&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-4.-quality-metrics-by-class)&#xA;4\\\\. Quality Metrics by Class\\n\\nShows the model quality metrics for the individual classes. In the case of multi-class problems, it will also include ROC AUC.\\n\\n<img height=\"1004\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d27',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '\\\\. Confusion Matrix\\n\\nVisualizes the classification errors and their type.\\n\\n<img height=\"1012\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-d44aceed79a4f98cde410058c12367a63037f2ce%252Fprob_class_perf_confusion_matrix.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cfaadbad&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-4.-quality-metrics-by-class)&#xA;4\\\\. Quality Metrics by Class\\n\\nShows the model quality metrics for the individual classes. In the case of multi-class problems, it will also include ROC AUC.\\n\\n<img height=\"1004\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d2719%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-5.-class-separation-quality)&#xA;**5. Class Separation Quality**\\n\\nA scatter plot of the predicted probabilities shows correct and incorrect predictions for each class.\\n\\nIt serves as a representation of both model accuracy and the quality of its calibration. It also helps visually **choose the best probability threshold for each class.**\\n\\n<img height=\"1098\" width=\"2136\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-e16ca5618f050c77396f80958fecebf330c24c72%252Fprob_class_perf_class_separation_quality.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b3b31454&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-6.-probability-distri',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '19%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-5.-class-separation-quality)&#xA;**5. Class Separation Quality**\\n\\nA scatter plot of the predicted probabilities shows correct and incorrect predictions for each class.\\n\\nIt serves as a representation of both model accuracy and the quality of its calibration. It also helps visually **choose the best probability threshold for each class.**\\n\\n<img height=\"1098\" width=\"2136\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-e16ca5618f050c77396f80958fecebf330c24c72%252Fprob_class_perf_class_separation_quality.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b3b31454&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-6.-probability-distribution)&#xA;6\\\\. Probability Distribution\\n\\nA similar view as above, it shows the distribution of predicted probabilities.\\n\\n<img height=\"1076\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-41006102d2e027f1b046d083bfd9fe097dee5563%252Fprob_class_perf_probability_distr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=c65c377c&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-7.-roc-curve)&#xA;**7. ROC Curve**\\n\\nROC Curve (**receiver operating characteristic curve**) shows the share of true positives and true negatives at different classification thresholds.\\n\\n<img height=\"1012\" width=\"2142\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'bution)&#xA;6\\\\. Probability Distribution\\n\\nA similar view as above, it shows the distribution of predicted probabilities.\\n\\n<img height=\"1076\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-41006102d2e027f1b046d083bfd9fe097dee5563%252Fprob_class_perf_probability_distr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=c65c377c&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-7.-roc-curve)&#xA;**7. ROC Curve**\\n\\nROC Curve (**receiver operating characteristic curve**) shows the share of true positives and true negatives at different classification thresholds.\\n\\n<img height=\"1012\" width=\"2142\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-756dcacc9d462ad45745c6bb8f6dd23c5a18d9d2%252Fprob_class_perf_roc.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=caee6451&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-8.-precision-recall-curve)&#xA;8\\\\. **Precision-Recall Curve**\\n\\nThe **precision**-**recall curve** shows the trade-off between **precision** and **recall** for different classification thresholds.\\n\\n<img height=\"1014\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-f558cc77f29cde0d60ffddc81a622232ca022c41%252Fprob_class_perf_pr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=7f49bf96&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-9.-precision-recall-table)&#xA;9\\\\. Precision-Recall Table\\n\\nThe table shows possible **outcomes for different classification thresholds** and **prediction co',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'blob-756dcacc9d462ad45745c6bb8f6dd23c5a18d9d2%252Fprob_class_perf_roc.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=caee6451&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-8.-precision-recall-curve)&#xA;8\\\\. **Precision-Recall Curve**\\n\\nThe **precision**-**recall curve** shows the trade-off between **precision** and **recall** for different classification thresholds.\\n\\n<img height=\"1014\" width=\"2134\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-f558cc77f29cde0d60ffddc81a622232ca022c41%252Fprob_class_perf_pr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=7f49bf96&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-9.-precision-recall-table)&#xA;9\\\\. Precision-Recall Table\\n\\nThe table shows possible **outcomes for different classification thresholds** and **prediction coverage**. If you have two datasets, the table is generated for both.\\n\\n<img height=\"1184\" width=\"1794\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-23088b547ed1b7126b1584e67acd2bf42715582b%252Fprob_class_perf_pr_table_current.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=2b13494f&sv=2\" />\\n\\nEach line in the table defines a case when only *top-X%* predictions are considered, with a 5% step. It shows the absolute number of predictions *(Count)* and the probability threshold *(Prob)* that correspond to this combination.\\n\\nThe table then shows the quality metrics for a given combination. It includes *Precision*, *Recall*, the share of *True Positives (TP)*, and *False Positives (FP)*.\\n\\nThis helps explore the quality of the model if you choose to act only on some of the predictions.\\n\\n#### [](https://docs.evidentlyai.com/presets',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'verage**. If you have two datasets, the table is generated for both.\\n\\n<img height=\"1184\" width=\"1794\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-23088b547ed1b7126b1584e67acd2bf42715582b%252Fprob_class_perf_pr_table_current.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=2b13494f&sv=2\" />\\n\\nEach line in the table defines a case when only *top-X%* predictions are considered, with a 5% step. It shows the absolute number of predictions *(Count)* and the probability threshold *(Prob)* that correspond to this combination.\\n\\nThe table then shows the quality metrics for a given combination. It includes *Precision*, *Recall*, the share of *True Positives (TP)*, and *False Positives (FP)*.\\n\\nThis helps explore the quality of the model if you choose to act only on some of the predictions.\\n\\n#### [](https://docs.evidentlyai.com/presets/class-performance#id-10.-classification-quality-by-feature)&#xA;10\\\\. Classification Quality by Feature\\n\\nIn this table, we show a number of plots for each feature. To expand the plots, click on the feature name.\\n\\n<img height=\"646\" width=\"2140\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-89edd11c3d8fe70b6e8d65cb7e37ca4f246b067f%252Fprob_class_perf_classification_quality_by_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1155f76a&sv=2\" />\\n\\nIn the tab “ALL”, you can see the distribution of classes against the values of the feature. If you compare the two datasets, it visually shows the changes in the feature distribution and in the relationship between the values of the feature and the target.\\n\\n<img height=\"1192\" width=\"2096\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': '/class-performance#id-10.-classification-quality-by-feature)&#xA;10\\\\. Classification Quality by Feature\\n\\nIn this table, we show a number of plots for each feature. To expand the plots, click on the feature name.\\n\\n<img height=\"646\" width=\"2140\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-89edd11c3d8fe70b6e8d65cb7e37ca4f246b067f%252Fprob_class_perf_classification_quality_by_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1155f76a&sv=2\" />\\n\\nIn the tab “ALL”, you can see the distribution of classes against the values of the feature. If you compare the two datasets, it visually shows the changes in the feature distribution and in the relationship between the values of the feature and the target.\\n\\n<img height=\"1192\" width=\"2096\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-84fc78dd31a5732be416239154f2c90515d78c42%252Fprob_class_perf_classification_quality_by_feature_example_all.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=8e13fd88&sv=2\" />\\n\\nFor each class, you can see the predicted probabilities alongside the values of the feature.\\n\\n<img height=\"1182\" width=\"2092\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-95a87ddd8e3296f900797f822e06b5c623de08a4%252Fprob_class_perf_classification_quality_by_feature_example_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=761fd6cb&sv=2\" />\\n\\nIt visualizes the regions where the model makes errors of each type and reveals the low-performance segments. You can compare the distributions and see **if the errors are sensitive to the va',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-84fc78dd31a5732be416239154f2c90515d78c42%252Fprob_class_perf_classification_quality_by_feature_example_all.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=8e13fd88&sv=2\" />\\n\\nFor each class, you can see the predicted probabilities alongside the values of the feature.\\n\\n<img height=\"1182\" width=\"2092\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-95a87ddd8e3296f900797f822e06b5c623de08a4%252Fprob_class_perf_classification_quality_by_feature_example_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=761fd6cb&sv=2\" />\\n\\nIt visualizes the regions where the model makes errors of each type and reveals the low-performance segments. You can compare the distributions and see **if the errors are sensitive to the values of a given feature**.\\n\\n###',\n",
       "  'title': 'Classification metrics',\n",
       "  'description': 'Open-source classification metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_classification.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '#### 1. Summary widget\\n\\nThe table gives an overview of the dataset, including missing or empty features and other general information. It also shows the share of almost empty and almost constant features. This applies to cases when 95% or more features are missing or constant.\\n\\n<img height=\"1026\" width=\"2172\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-893e96b92c369e8ad43eddc86aba54f79ce46b27%252Freports_data_quality_summary.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=2d5a9008&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/data-quality#id-2.-features-widget)&#xA;2\\\\. Features widget\\n\\nFor each feature, this widget generates a set of visualizations. They vary depending on the feature type. There are 3 components:\\n\\n**2.1. Feature overview table**\\n\\nThe table shows relevant statistical summaries for each feature based on its type and a visualization of feature distribution.\\n\\n**Example for a categorical feature:**\\n\\n<img height=\"908\" width=\"2282\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-31dbb98333b62c917c7b2cbba158445774508baa%252Freports_data_quality_overview_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=516d34b9&sv=2\" />\\n\\n**Example for a numerical feature:**\\n\\n<img height=\"1106\" width=\"2336\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-c7affc8d3dd2d0bee0b70eafdeb5867e5c52b30e%252Freports_data_quality_overview_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cd19013a&sv=2\" />\\n\\n**Example for a datetime feature:**\\n\\n<img height=\"802\" width=\"2248\" src=\"https://',\n",
       "  'title': 'Data stats and quality',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_data_stats.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'sed on its type and a visualization of feature distribution.\\n\\n**Example for a categorical feature:**\\n\\n<img height=\"908\" width=\"2282\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-31dbb98333b62c917c7b2cbba158445774508baa%252Freports_data_quality_overview_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=516d34b9&sv=2\" />\\n\\n**Example for a numerical feature:**\\n\\n<img height=\"1106\" width=\"2336\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-c7affc8d3dd2d0bee0b70eafdeb5867e5c52b30e%252Freports_data_quality_overview_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cd19013a&sv=2\" />\\n\\n**Example for a datetime feature:**\\n\\n<img height=\"802\" width=\"2248\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-ebcc131d1b824e4754edd777804343a4ea5dfdb1%252Freports_data_quality_overview_datetime.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=a31fdfd&sv=2\" />\\n\\n**Example for a text feature:**\\n\\n<img height=\"1042\" width=\"2352\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-7074e49da73257ed1bd5ef7b675b8f63836ed15d%252Fmetric_column_summary_text-min.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=767b2d86&sv=2\" />\\n\\n**2.2. Feature in time**\\n\\nIf you click on \"details\", each feature would include additional visualization to show feature behavior in time.\\n\\n**Example for a categorical feature:**\\n\\n<img height=\"974\" width=\"2280\" src=\"https://docs.ev',\n",
       "  'title': 'Data stats and quality',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_data_stats.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-ebcc131d1b824e4754edd777804343a4ea5dfdb1%252Freports_data_quality_overview_datetime.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=a31fdfd&sv=2\" />\\n\\n**Example for a text feature:**\\n\\n<img height=\"1042\" width=\"2352\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-7074e49da73257ed1bd5ef7b675b8f63836ed15d%252Fmetric_column_summary_text-min.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=767b2d86&sv=2\" />\\n\\n**2.2. Feature in time**\\n\\nIf you click on \"details\", each feature would include additional visualization to show feature behavior in time.\\n\\n**Example for a categorical feature:**\\n\\n<img height=\"974\" width=\"2280\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-71b9ba91a04fa177dffbbb2b25984be292f15283%252Freports_data_quality_in_time_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=98c89f23&sv=2\" />\\n\\n**Example for a numerical feature:**\\n\\n<img height=\"880\" width=\"2348\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-a06be74e97d4d2fae56e8686520d0059161f36eb%252Freports_data_quality_in_time_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cdb43752&sv=2\" />\\n\\n**Example for a datetime feature:**\\n\\n<img height=\"928\" width=\"2274\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM',\n",
       "  'title': 'Data stats and quality',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_data_stats.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'identlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-71b9ba91a04fa177dffbbb2b25984be292f15283%252Freports_data_quality_in_time_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=98c89f23&sv=2\" />\\n\\n**Example for a numerical feature:**\\n\\n<img height=\"880\" width=\"2348\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-a06be74e97d4d2fae56e8686520d0059161f36eb%252Freports_data_quality_in_time_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cdb43752&sv=2\" />\\n\\n**Example for a datetime feature:**\\n\\n<img height=\"928\" width=\"2274\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-9b4017bb0fd6a38fae0b868c2cf016a2ec8e2250%252Freports_data_quality_in_time_datetime.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=fd11ef73&sv=2\" />\\n\\n**2.3. Feature by target**\\n\\nCategorical and numerical features include an additional visualization that plots the interaction between a given feature and the target.\\n\\n**Example for a categorical feature:**\\n\\n<img height=\"964\" width=\"2304\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-70efaf4625facf8c22e1821af9494b070322ca60%252Freports_data_quality_by_target_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=92c8f6d4&sv=2\" />\\n\\n**Example for a numerical feature:**\\n\\n<img height=\"876\" width=\"2354\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot',\n",
       "  'title': 'Data stats and quality',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_data_stats.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '4508ESQxkbpOxj%252Fuploads%252Fgit-blob-9b4017bb0fd6a38fae0b868c2cf016a2ec8e2250%252Freports_data_quality_in_time_datetime.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=fd11ef73&sv=2\" />\\n\\n**2.3. Feature by target**\\n\\nCategorical and numerical features include an additional visualization that plots the interaction between a given feature and the target.\\n\\n**Example for a categorical feature:**\\n\\n<img height=\"964\" width=\"2304\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-70efaf4625facf8c22e1821af9494b070322ca60%252Freports_data_quality_by_target_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=92c8f6d4&sv=2\" />\\n\\n**Example for a numerical feature:**\\n\\n<img height=\"876\" width=\"2354\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-dae7161fc71ab08b8569678ce02dc8d58420f7c9%252Freports_data_quality_by_target_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1b7fde00&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/data-quality#id-3.-correlation-widget)&#xA;3\\\\. Correlation widget\\n\\nThis widget shows the correlations between different features.\\n\\n**3.1. Insights**\\n\\nThis table shows a summary of pairwise feature correlations.\\n\\nFor a single dataset, it lists the top-5 highly correlated variables from Cramer\\'s v correlation matrix (categorical features) and from Spearman correlation matrix (numerical features).\\n\\nFor two datasets, it lists the top-5 pairs of variables **where correlation changes** the most between the reference and current datasets. Similarly, it uses categorical features from Cramer\\'s v correlation matrix and numerical features from Spearman correlation matrix.\\n\\n<img height=\"518\" width=\"1940\" src=\"https://docs.evidentlyai.com/~',\n",
       "  'title': 'Data stats and quality',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_data_stats.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': '.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-dae7161fc71ab08b8569678ce02dc8d58420f7c9%252Freports_data_quality_by_target_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1b7fde00&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/data-quality#id-3.-correlation-widget)&#xA;3\\\\. Correlation widget\\n\\nThis widget shows the correlations between different features.\\n\\n**3.1. Insights**\\n\\nThis table shows a summary of pairwise feature correlations.\\n\\nFor a single dataset, it lists the top-5 highly correlated variables from Cramer\\'s v correlation matrix (categorical features) and from Spearman correlation matrix (numerical features).\\n\\nFor two datasets, it lists the top-5 pairs of variables **where correlation changes** the most between the reference and current datasets. Similarly, it uses categorical features from Cramer\\'s v correlation matrix and numerical features from Spearman correlation matrix.\\n\\n<img height=\"518\" width=\"1940\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-cb472b98063f4e2f57029b8ea4e450d7c8a453c7%252Freports_data_quality_correlations.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=99257d7b&sv=2\" />\\n\\n**3.2. Correlation heatmaps**\\n\\n**This widget has been removed from Preset in versions above 0.4.31**. You can add it to your Report as `DatasetCorrelationsMetric()`.\\n\\nThis section includes four heatmaps.\\n\\nFor categorical features, Evidently calculates the [Cramer\\'s v](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V) correlation matrix. For numerical features, Evidently calculates the [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), [Spearman](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) and [Kendall](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) matrices.\\n\\nIf your dataset includes the target, th',\n",
       "  'title': 'Data stats and quality',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_data_stats.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-cb472b98063f4e2f57029b8ea4e450d7c8a453c7%252Freports_data_quality_correlations.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=99257d7b&sv=2\" />\\n\\n**3.2. Correlation heatmaps**\\n\\n**This widget has been removed from Preset in versions above 0.4.31**. You can add it to your Report as `DatasetCorrelationsMetric()`.\\n\\nThis section includes four heatmaps.\\n\\nFor categorical features, Evidently calculates the [Cramer\\'s v](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V) correlation matrix. For numerical features, Evidently calculates the [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), [Spearman](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) and [Kendall](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) matrices.\\n\\nIf your dataset includes the target, the target will be also shown in the matrix according to its type.\\n\\n<img height=\"812\" width=\"2050\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-b8e9570d01a6413c4c8eeabd91252bf263516e21%252Freports_data_quality_correlation_heatmaps.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=5565f244&sv=2\" />\\n\\n### [](https://docs.evidentlyai.com/presets/data-quality#metrics-outputs)',\n",
       "  'title': 'Data stats and quality',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_data_stats.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\\n\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\n\\n<Info>\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\n</Info>\\n\\n## How it works\\n\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\\n\\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\\n\\n* column type: categorical, numerical, text data \\n\\n* the number of observations in the reference dataset\\n\\n* the number of unique values in the column (n\\\\_unique)\\n\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\n\\n## Data requirements\\n\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\n\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\n\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\n\\n<Note>\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' observations in the reference dataset\\n\\n* the number of unique values in the column (n\\\\_unique)\\n\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\n\\n## Data requirements\\n\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\n\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\n\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\n\\n<Note>\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\\n</Note>\\n\\n## Dataset drift\\n\\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\\n\\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\\n\\n![](/images/metrics/preset_data_drift_2-min.png)\\n\\n## Tabular data drift\\n\\nThe following defaults apply for tabular data: numerical and categorical columns.\\n\\nFor **small data with \\\\<= 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipe',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\\n</Note>\\n\\n## Dataset drift\\n\\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\\n\\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\\n\\n![](/images/metrics/preset_data_drift_2-min.png)\\n\\n## Tabular data drift\\n\\nThe following defaults apply for tabular data: numerical and categorical columns.\\n\\nFor **small data with \\\\<= 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n\\n* For categorical columns or numerical columns with n\\\\_unique \\\\<= 5: [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n\\n* For binary categorical features (n\\\\_unique \\\\<= 2): proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\n  All tests use a 0.95 confidence level by default. Drift score is P-value. (=\\\\< 0.05 means drift).\\n</Info>\\n\\nFor **larger data with > 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5):[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n\\n* For categorical columns or numerical with n\\\\_unique \\\\<= 5):[Jensen--Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\n  All metrics use a threshold = 0.1 by default. Drift score is distance/divergence. (>= 0.1 means drift).\\n</Info>\\n\\n**You can modify this drift detection logic**. You can select any method available in the library (PSI, K-L d',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'dia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n\\n* For categorical columns or numerical columns with n\\\\_unique \\\\<= 5: [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n\\n* For binary categorical features (n\\\\_unique \\\\<= 2): proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\n  All tests use a 0.95 confidence level by default. Drift score is P-value. (=\\\\< 0.05 means drift).\\n</Info>\\n\\nFor **larger data with > 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5):[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n\\n* For categorical columns or numerical with n\\\\_unique \\\\<= 5):[Jensen--Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\n  All metrics use a threshold = 0.1 by default. Drift score is distance/divergence. (>= 0.1 means drift).\\n</Info>\\n\\n**You can modify this drift detection logic**. You can select any method available in the library (PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc.), specify thresholds, or pass a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).\\n\\n**Exploring drift.** You can see the distribution of each individual column inside the `DataDriftPreset` or using `ValueDrift` metric:\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nFor numerical features, you can also explore the values mapped in a plot.\\n\\n* The dark green line is the **mean**, as seen in the reference dataset.\\n\\n* The green area covers **one standard deviation** from the mean.\\n\\n![](/images/metrics/preset_data_drift_3-min.png)\\n\\nIndex is binned to 150 or uses timestamp if provided.&#x20;\\n\\n## Text data drift\\n\\nText content drift using a **domain classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.&#x20;\\n\\n![](/images/concepts/text_data_drift_domain_classifier.png)\\n\\nIf the model can confidently i',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'ivergence, Jensen-Shannon distance, Wasserstein distance, etc.), specify thresholds, or pass a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).\\n\\n**Exploring drift.** You can see the distribution of each individual column inside the `DataDriftPreset` or using `ValueDrift` metric:\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nFor numerical features, you can also explore the values mapped in a plot.\\n\\n* The dark green line is the **mean**, as seen in the reference dataset.\\n\\n* The green area covers **one standard deviation** from the mean.\\n\\n![](/images/metrics/preset_data_drift_3-min.png)\\n\\nIndex is binned to 150 or uses timestamp if provided.&#x20;\\n\\n## Text data drift\\n\\nText content drift using a **domain classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.&#x20;\\n\\n![](/images/concepts/text_data_drift_domain_classifier.png)\\n\\nIf the model can confidently identify which text samples belong to the “newer” data, you can consider that the two datasets are significantly different.\\n\\n<Info>\\n  You can read more about the domain classifier approach in the\\xa0[paper\\xa0](https://arxiv.org/pdf/1810.11953.pdf)“Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift.”\\n</Info>\\n\\nThe drift score in this case is the ROC AUC of the resulting classifier.\\n\\nThe default for **larger data with > 1000 observations** detects drift if the ROC AUC > 0.55. The ROC AUC of the obtained classifier is directly compared against the set ROC AUC threshold. You can set a different threshold as a parameter.\\n\\nThe default for **small data with \\\\<= 1000 observations** detects drift if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile. This approach\\xa0**protects against false positive**\\xa0drift results for small datasets since we explicitly compare the classifier score against the “best random score” we could',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'dentify which text samples belong to the “newer” data, you can consider that the two datasets are significantly different.\\n\\n<Info>\\n  You can read more about the domain classifier approach in the\\xa0[paper\\xa0](https://arxiv.org/pdf/1810.11953.pdf)“Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift.”\\n</Info>\\n\\nThe drift score in this case is the ROC AUC of the resulting classifier.\\n\\nThe default for **larger data with > 1000 observations** detects drift if the ROC AUC > 0.55. The ROC AUC of the obtained classifier is directly compared against the set ROC AUC threshold. You can set a different threshold as a parameter.\\n\\nThe default for **small data with \\\\<= 1000 observations** detects drift if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile. This approach\\xa0**protects against false positive**\\xa0drift results for small datasets since we explicitly compare the classifier score against the “best random score” we could obtain.\\xa0\\n\\n<Info>\\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\\n</Info>\\n\\nIf the drift is detected, Evidently will also calculate the\\xa0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words su',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': ' obtain.\\xa0\\n\\n<Info>\\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\\n</Info>\\n\\nIf the drift is detected, Evidently will also calculate the\\xa0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words such as articles.\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n\\n## Resources\\n\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\n\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n\\nAdditional links:\\n\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\n\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://e',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'ch as articles.\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n\\n## Resources\\n\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\n\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n\\nAdditional links:\\n\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\n\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\n\\n* [\"My data drifted. What\\'s next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\n\\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"The following metrics can be used for ranking, retrieval and recommender systems. \\n\\n## Ranking \\n\\n### Recall \\n\\n**Evidently Metric**: `RecallTopK`. \\n\\nRecall at K reflects the ability of the recommender or ranking system to retrieve all relevant items within the top K results. \\n\\n**Implemented method:**\\n* **Compute recall at K by user**. Compute the recall at K for each individual user (or query), by measuring the share of all relevant items in the dataset that appear in the top K results.\\n\\n$$\\\\text{Recall at } K = \\\\frac{\\\\text{Number of relevant items in } K}{\\\\text{Total number of relevant items}}$$\\n\\n* **Compute overall recall**. Average the results across all users (queries) in the dataset. \\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: a higher recall at K indicates that the model can retrieve a higher proportion of relevant items, which is generally desirable. \\n\\n**Notes**: if the total number of relevant items is greater than K, it's impossible to recall all of them within the top K results (making 100% recall impossible).\\n\\n### Precision \\n\\n**Evidently Metric**: `PrecisionTopK`. \\n\\nPrecision at K reflects the ability of the system to suggest items that are truly relevant to the users’ preferences or queries. \\n\\n**Implemented method:**\\n* **Compute precision at K by user**. Compute the precision at K for each user (or query) by measuring the share of the relevant results within the top K. \\n\\n$$\\\\text{Precision at } K = \\\\frac{\\\\text{Number of relevant items in } K}{\\\\text{Total number of items in }K}$$\\n\\n* **Compute overall precision**. Average the results across all users (queries) in the dataset. \\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: a higher precision at K indicates that a larger proportion of the top results are relevant, which is generally desirable.\\n\\n### F Beta\\n\\n**Evidently Metric**: `FBetaTopK`.\\n\\nThe F Beta score at K combines precision and recall into a single value, providing a balanced measure of a recommendation or ranking system's performance. \\n\\n$$F_{\\\\beta} = \\\\frac{(1\",\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"making 100% recall impossible).\\n\\n### Precision \\n\\n**Evidently Metric**: `PrecisionTopK`. \\n\\nPrecision at K reflects the ability of the system to suggest items that are truly relevant to the users’ preferences or queries. \\n\\n**Implemented method:**\\n* **Compute precision at K by user**. Compute the precision at K for each user (or query) by measuring the share of the relevant results within the top K. \\n\\n$$\\\\text{Precision at } K = \\\\frac{\\\\text{Number of relevant items in } K}{\\\\text{Total number of items in }K}$$\\n\\n* **Compute overall precision**. Average the results across all users (queries) in the dataset. \\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: a higher precision at K indicates that a larger proportion of the top results are relevant, which is generally desirable.\\n\\n### F Beta\\n\\n**Evidently Metric**: `FBetaTopK`.\\n\\nThe F Beta score at K combines precision and recall into a single value, providing a balanced measure of a recommendation or ranking system's performance. \\n\\n$$F_{\\\\beta} = \\\\frac{(1 + \\\\beta^2) \\\\times \\\\text{Precision at K} \\\\times \\\\text{Recall at K}}{(\\\\beta^2 \\\\times \\\\text{Precision at K}) + \\\\text{Recall at K}}$$\\n\\n`Beta` is a parameter that determines the weight assigned to recall relative to precision. `Beta` > 1 gives more weight to recall, while `beta` < 1 favors precision.\\n\\nIf `Beta` = 1 (default), it is a traditional F1 score that provides a harmonic mean of precision and recall at K. It provides a balanced estimation, considering both false positives (items recommended that are not relevant) and false negatives (relevant items not recommended).\\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: Higher F Beta at K values indicate better overall performance.\\n\\n### Mean average precision (MAP) \\n\\n**Evidently Metric**: `MAP`.\\n\\nMAP (Mean Average Precision) at K assesses the ability of the recommender or retrieval system to suggest relevant items in the top-K results, while placing more relevant items at the top. \\n\\nCompared to precision at K, MAP at K is rank-aware. It penalize\",\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': ' + \\\\beta^2) \\\\times \\\\text{Precision at K} \\\\times \\\\text{Recall at K}}{(\\\\beta^2 \\\\times \\\\text{Precision at K}) + \\\\text{Recall at K}}$$\\n\\n`Beta` is a parameter that determines the weight assigned to recall relative to precision. `Beta` > 1 gives more weight to recall, while `beta` < 1 favors precision.\\n\\nIf `Beta` = 1 (default), it is a traditional F1 score that provides a harmonic mean of precision and recall at K. It provides a balanced estimation, considering both false positives (items recommended that are not relevant) and false negatives (relevant items not recommended).\\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: Higher F Beta at K values indicate better overall performance.\\n\\n### Mean average precision (MAP) \\n\\n**Evidently Metric**: `MAP`.\\n\\nMAP (Mean Average Precision) at K assesses the ability of the recommender or retrieval system to suggest relevant items in the top-K results, while placing more relevant items at the top. \\n\\nCompared to precision at K, MAP at K is rank-aware. It penalizes the system for placing relevant items lower in the list, even if the total number of relevant items at K is the same.\\n\\n**Implemented method:**\\n* **Compute Average Precision (AP) at K by user**. The Average Precision at K is computed for each user (or query) as an average of precision values at each relevant item position within the top K. To do that, we sum up precision at all values of K when the item is relevant (e.g., Precision @1, Precision@2..), and divide it by the total number of relevant items in K.\\n\\n$$\\\\text{AP@K} = \\\\frac{1}{N} \\\\sum_{k=1}^{K} Precision(k) \\\\times rel(k)$$\\n\\nWhere *N* is the total number of relevant items at K, and *rel(k)* is equal to 1 if the item is relevant, and is 0 otherwise.\\n\\nExample: if K = 10, and items in positions 1, 2, and 10 are relevant, the formula will look as:\\n\\n$$AP@10 = \\\\frac{Precision@1+Precision@2+Precision@10}{3}$$\\n\\n* **Compute Mean Average Precision (MAP) at K**. Average the results across all users (or queries) in the dataset.\\n\\n$$\\\\text{MAP',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 's the system for placing relevant items lower in the list, even if the total number of relevant items at K is the same.\\n\\n**Implemented method:**\\n* **Compute Average Precision (AP) at K by user**. The Average Precision at K is computed for each user (or query) as an average of precision values at each relevant item position within the top K. To do that, we sum up precision at all values of K when the item is relevant (e.g., Precision @1, Precision@2..), and divide it by the total number of relevant items in K.\\n\\n$$\\\\text{AP@K} = \\\\frac{1}{N} \\\\sum_{k=1}^{K} Precision(k) \\\\times rel(k)$$\\n\\nWhere *N* is the total number of relevant items at K, and *rel(k)* is equal to 1 if the item is relevant, and is 0 otherwise.\\n\\nExample: if K = 10, and items in positions 1, 2, and 10 are relevant, the formula will look as:\\n\\n$$AP@10 = \\\\frac{Precision@1+Precision@2+Precision@10}{3}$$\\n\\n* **Compute Mean Average Precision (MAP) at K**. Average the results across all users (or queries) in the dataset.\\n\\n$$\\\\text{MAP@K} = \\\\frac{1}{U} \\\\sum_{u=1}^{U} \\\\text{AP@K}_u$$\\n\\nWhere *U* is the total number of users or queries in the dataset, and *AP* is the average precision for a given list.\\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: Higher MAP at K values indicates a better ability of the system to place relevant items high in the list. \\n\\n### Mean average recall (MAR) \\n\\n**Evidently Metric**: `MAR`.\\n\\nMAR (Mean Average Recall) at K assesses the ability of a recommendation system to retrieve all relevant items within the top-K results, averaged by all relevant positions. \\n\\n**Implemented method:**\\n* **Compute the average recall at K by user**. Compute and average the recall at each relevant position within the top K for every user (or query). To do that, we sum up the recall at all values of K when the item is relevant (e.g. Recall @1, Recall@2..), and divide it by the total number of relevant recommendations in K.\\n\\n$$\\\\text{AR@K} = \\\\frac{1}{N} \\\\sum_{k=1}^{K} Recall(k) \\\\times rel(k)$$\\n\\nExample: if K = 10, and item',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': '@K} = \\\\frac{1}{U} \\\\sum_{u=1}^{U} \\\\text{AP@K}_u$$\\n\\nWhere *U* is the total number of users or queries in the dataset, and *AP* is the average precision for a given list.\\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: Higher MAP at K values indicates a better ability of the system to place relevant items high in the list. \\n\\n### Mean average recall (MAR) \\n\\n**Evidently Metric**: `MAR`.\\n\\nMAR (Mean Average Recall) at K assesses the ability of a recommendation system to retrieve all relevant items within the top-K results, averaged by all relevant positions. \\n\\n**Implemented method:**\\n* **Compute the average recall at K by user**. Compute and average the recall at each relevant position within the top K for every user (or query). To do that, we sum up the recall at all values of K when the item is relevant (e.g. Recall @1, Recall@2..), and divide it by the total number of relevant recommendations in K.\\n\\n$$\\\\text{AR@K} = \\\\frac{1}{N} \\\\sum_{k=1}^{K} Recall(k) \\\\times rel(k)$$\\n\\nExample: if K = 10, and items in positions 1, 2, and 10 are relevant, the formula will look as:\\n\\n$$\\\\text{AR@10} = \\\\frac{Recall@1+Recall@2+Recall@10}{3}$$\\n\\n* **Compute mean average recall at K**. Average the results across all users (or queries).\\n\\n$$\\\\text{MAR@K} = \\\\frac{1}{U} \\\\sum_{u=1}^{U} \\\\text{AR@K}_u$$\\n\\nWhere *U* is the total number of users or queries in the dataset, and *AR* is the average recall for a given list.\\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: Higher MAR at K values indicates a better ability of the system to retrieve relevant items across all users or queries.  \\n\\n### Normalized Discounted Cumulative Gain (NDCG)\\n\\n**Evidently Metric**: `NDCG`.\\n\\nNDCG (Normalized Discounted Cumulative Gain) at K reflects the ranking quality, comparing it to an ideal order where all relevant items for each user (or query) are placed at the top of the list.\\n\\n**Implemented method**:\\n* **Provide the item relevance score**. You can assign a relevance score for each item in each top-K list for user or query. Depending on t',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 's in positions 1, 2, and 10 are relevant, the formula will look as:\\n\\n$$\\\\text{AR@10} = \\\\frac{Recall@1+Recall@2+Recall@10}{3}$$\\n\\n* **Compute mean average recall at K**. Average the results across all users (or queries).\\n\\n$$\\\\text{MAR@K} = \\\\frac{1}{U} \\\\sum_{u=1}^{U} \\\\text{AR@K}_u$$\\n\\nWhere *U* is the total number of users or queries in the dataset, and *AR* is the average recall for a given list.\\n\\n**Range**: 0 to 1.\\n\\n**Interpretation**: Higher MAR at K values indicates a better ability of the system to retrieve relevant items across all users or queries.  \\n\\n### Normalized Discounted Cumulative Gain (NDCG)\\n\\n**Evidently Metric**: `NDCG`.\\n\\nNDCG (Normalized Discounted Cumulative Gain) at K reflects the ranking quality, comparing it to an ideal order where all relevant items for each user (or query) are placed at the top of the list.\\n\\n**Implemented method**:\\n* **Provide the item relevance score**. You can assign a relevance score for each item in each top-K list for user or query. Depending on the model type, it can be a binary outcome (1 is relevant, 0 is not) or a score.  \\n* **Compute the discounted cumulative gain (DCG)** at K by the user or query. DCG at K measures the quality of the ranking (= total relevance) for a list of top-K items. We add a logarithmic discount to account for diminishing returns from each following item being lower on the list. To get the resulting DCG, you can compute a weighted sum of the relevance scores for all items from the top of the list to K with an applied discount.\\n\\n$$\\\\text{DCG@K} = \\\\sum_{i=1}^{K} \\\\frac{rel_i}{\\\\log_2(i + 1)}$$\\n\\nWhere *Rel(i)* is the relevance score of the item at rank *i*. \\n* **Compute the normalized DCG (NDCG)**. To normalize the metric, we divide the resulting DCG by the ideal DCG (IDCG) at K. Ideal DCG at K represents the maximum achievable DCG when the items are perfectly ranked in descending order of relevance. \\n\\n$$\\\\text{NDCG@K} = \\\\frac{DCG@K}{IDCG@K}$$\\n\\nThis way, it is possible to compare NDCG values across differen',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'he model type, it can be a binary outcome (1 is relevant, 0 is not) or a score.  \\n* **Compute the discounted cumulative gain (DCG)** at K by the user or query. DCG at K measures the quality of the ranking (= total relevance) for a list of top-K items. We add a logarithmic discount to account for diminishing returns from each following item being lower on the list. To get the resulting DCG, you can compute a weighted sum of the relevance scores for all items from the top of the list to K with an applied discount.\\n\\n$$\\\\text{DCG@K} = \\\\sum_{i=1}^{K} \\\\frac{rel_i}{\\\\log_2(i + 1)}$$\\n\\nWhere *Rel(i)* is the relevance score of the item at rank *i*. \\n* **Compute the normalized DCG (NDCG)**. To normalize the metric, we divide the resulting DCG by the ideal DCG (IDCG) at K. Ideal DCG at K represents the maximum achievable DCG when the items are perfectly ranked in descending order of relevance. \\n\\n$$\\\\text{NDCG@K} = \\\\frac{DCG@K}{IDCG@K}$$\\n\\nThis way, it is possible to compare NDCG values across different use cases. The resulting NDCG values for all users or queries are averaged to measure the overall performance of a model. \\n\\n**Range**: 0 to 1, where 1 indicates perfect ranking.\\n\\n**Interpretation**: Higher NDCG at K indicates a better ability of the system to place more relevant items higher up in the ranking.\\n\\n### Hit Rate\\n\\n**Evidently Metric**: `HitRate`.\\n\\nHit Rate at K calculates the share of users or queries for which at least one relevant item is included in the K.\\n\\n**Implemented method**:\\n* **Compute “hit” for each user**. For each user or query, we evaluate if any of the top-K recommended items is relevant. It is a binary metric equal to 1 if any relevant item is included in K, or 0 otherwise.\\n* **Compute average hit rate**. The average of this metric is calculated across all users or queries.\\n\\n**Range**: 0 to 1, where 1 indicates that each user / query gets at least one relevant recommendation / retrieval.\\n\\n**Interpretation**: A higher Hit Rate indicates that a higher share o',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 't use cases. The resulting NDCG values for all users or queries are averaged to measure the overall performance of a model. \\n\\n**Range**: 0 to 1, where 1 indicates perfect ranking.\\n\\n**Interpretation**: Higher NDCG at K indicates a better ability of the system to place more relevant items higher up in the ranking.\\n\\n### Hit Rate\\n\\n**Evidently Metric**: `HitRate`.\\n\\nHit Rate at K calculates the share of users or queries for which at least one relevant item is included in the K.\\n\\n**Implemented method**:\\n* **Compute “hit” for each user**. For each user or query, we evaluate if any of the top-K recommended items is relevant. It is a binary metric equal to 1 if any relevant item is included in K, or 0 otherwise.\\n* **Compute average hit rate**. The average of this metric is calculated across all users or queries.\\n\\n**Range**: 0 to 1, where 1 indicates that each user / query gets at least one relevant recommendation / retrieval.\\n\\n**Interpretation**: A higher Hit Rate indicates that a higher share of users / queries have relevant items in their lists. \\n\\n**Note**: the Hit Rate will typically increase for higher values of K (since there is a higher chance that a relevant item will be recommended in a longer list).\\n\\n### Mean Reciprocal Rank (MRR)\\n\\n**Evidently Metric**: `MRR`\\n\\nMean Reciprocal Rank (MRR) measures the ranking quality considering the position of the first relevant item in the list.\\n\\n**Implemented method:**\\n* For each user or query, identify the position of the **first relevant item** in the recommended list.\\n* Calculate the **reciprocal rank**, taking the reciprocal of the position of the first relevant item for each user or query (i.e., 1/position). \\nExample: if the first relevant item is at the top of the list - the reciprocal rank is 1, if it is on the 2nd position - the reciprocal rank ½, if on the 3rd - ⅓, etc.\\n* Calculate the **mean reciprocal rank** (MRR). Compute the average reciprocal rank across all users or queries.\\n\\n$$\\\\text{MRR} = \\\\frac{1}{U} \\\\sum_{u=1}^{U}\\\\',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'f users / queries have relevant items in their lists. \\n\\n**Note**: the Hit Rate will typically increase for higher values of K (since there is a higher chance that a relevant item will be recommended in a longer list).\\n\\n### Mean Reciprocal Rank (MRR)\\n\\n**Evidently Metric**: `MRR`\\n\\nMean Reciprocal Rank (MRR) measures the ranking quality considering the position of the first relevant item in the list.\\n\\n**Implemented method:**\\n* For each user or query, identify the position of the **first relevant item** in the recommended list.\\n* Calculate the **reciprocal rank**, taking the reciprocal of the position of the first relevant item for each user or query (i.e., 1/position). \\nExample: if the first relevant item is at the top of the list - the reciprocal rank is 1, if it is on the 2nd position - the reciprocal rank ½, if on the 3rd - ⅓, etc.\\n* Calculate the **mean reciprocal rank** (MRR). Compute the average reciprocal rank across all users or queries.\\n\\n$$\\\\text{MRR} = \\\\frac{1}{U} \\\\sum_{u=1}^{U}\\\\frac{1}{rank_i}$$\\n  \\nWhere *U* is the total number of users or queries, and *rank(i)* is the rank of the first relevant item for user *u* in the top-K results.\\n\\n**Range**: 0 to 1, where 1 indicates that the first recommended item for every user is relevant.\\n\\n**Interpretation**: A higher MRR indicates that, on average, relevant items are positioned closer to the top of the recommended lists. \\n\\n**Note**: Only a single top relevant item is considered in this metric, disregarding the position and relevance of other items in the list.\\n\\n### Score Distribution (Entropy)\\n\\n**Evidently Metric**: `ScoreDistribution`\\n\\nThis metric computes the predicted score entropy. It applies only when the `recommendations_type` is a score.\\n\\n**Implementation**:\\n* Apply softmax transformation for top-K scores for all users.\\n* Compute the KL divergence (relative entropy in [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)). \\n\\nThe visualization shows the distribution of the pred',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 9000,\n",
       "  'chunk': 'frac{1}{rank_i}$$\\n  \\nWhere *U* is the total number of users or queries, and *rank(i)* is the rank of the first relevant item for user *u* in the top-K results.\\n\\n**Range**: 0 to 1, where 1 indicates that the first recommended item for every user is relevant.\\n\\n**Interpretation**: A higher MRR indicates that, on average, relevant items are positioned closer to the top of the recommended lists. \\n\\n**Note**: Only a single top relevant item is considered in this metric, disregarding the position and relevance of other items in the list.\\n\\n### Score Distribution (Entropy)\\n\\n**Evidently Metric**: `ScoreDistribution`\\n\\nThis metric computes the predicted score entropy. It applies only when the `recommendations_type` is a score.\\n\\n**Implementation**:\\n* Apply softmax transformation for top-K scores for all users.\\n* Compute the KL divergence (relative entropy in [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)). \\n\\nThe visualization shows the distribution of the predicted scores at K (and all scores, if available). \\n\\n## RecSys\\n\\n<Warning>\\n   These metrics are **coming soon** to the new Evidently API! Check the old docs for now.\\n</Warning>',\n",
       "  'title': 'Ranking and RecSys metrics',\n",
       "  'description': 'Open-source metrics for ranking and recommendations.',\n",
       "  'filename': 'docs-main/metrics/explainer_recsys.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '#### **1. Model Quality Summary Metrics**\\n\\nEvidently calculate a few standard model quality metrics: Mean Error (ME), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE).\\n\\n<img height=\"704\" width=\"2216\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-0592d3c8a92179e411b172cd8eb8884138505050%252Freg_perf_model_quality_summary.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=da983c12&sv=2\" />\\n\\nFor each quality metric, Evidently also shows one standard deviation of its value (in brackets) to estimate the stability of the performance.\\n\\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-2.-predicted-vs-actual)&#xA;2\\\\. **Predicted vs Actual**\\n\\nPredicted versus actual values in a scatter plot.\\n\\n<img height=\"1078\" width=\"2214\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-1269097ed5a2ea2a6bed82561daf473d2de81e0e%252Freg_perf_predicted_actual.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=d393d0b8&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-3.-predicted-vs-actual-in-time)&#xA;3\\\\. **Predicted vs Actual in Time**\\n\\nPredicted and Actual values over time or by index, if no datetime is provided.\\n\\n<img height=\"1070\" width=\"2216\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252FQ6gSnWb4ytbEuGpW5YVG%252Freg_perf_predicted_actual_in_time.png%3Falt%3Dmedia%26token%3D7b3e769e-4',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'edicted vs Actual**\\n\\nPredicted versus actual values in a scatter plot.\\n\\n<img height=\"1078\" width=\"2214\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-1269097ed5a2ea2a6bed82561daf473d2de81e0e%252Freg_perf_predicted_actual.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=d393d0b8&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-3.-predicted-vs-actual-in-time)&#xA;3\\\\. **Predicted vs Actual in Time**\\n\\nPredicted and Actual values over time or by index, if no datetime is provided.\\n\\n<img height=\"1070\" width=\"2216\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252FQ6gSnWb4ytbEuGpW5YVG%252Freg_perf_predicted_actual_in_time.png%3Falt%3Dmedia%26token%3D7b3e769e-43b4-4944-b25a-5bef5426a730&width=768&dpr=4&quality=100&sign=591a244d&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-4.-error-predicted-actual)&#xA;4\\\\. Error (Predicted - Actual)\\n\\nModel error values over time or by index, if no datetime is provided.\\n\\n<img height=\"1080\" width=\"2220\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-712477dee018c0e0c8f64bcc4cec3506a2e4bece%252Freg_perf_error.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=c90208ed&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-5.-absolute-percentage-error)&#xA;5\\\\. Absolute Percentage Error\\n\\nAbsolute percentage error values over time or by index, if no datetime is provided.\\n\\n<img height=\"1074\" width=\"2218\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffile',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': '3b4-4944-b25a-5bef5426a730&width=768&dpr=4&quality=100&sign=591a244d&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-4.-error-predicted-actual)&#xA;4\\\\. Error (Predicted - Actual)\\n\\nModel error values over time or by index, if no datetime is provided.\\n\\n<img height=\"1080\" width=\"2220\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-712477dee018c0e0c8f64bcc4cec3506a2e4bece%252Freg_perf_error.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=c90208ed&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-5.-absolute-percentage-error)&#xA;5\\\\. Absolute Percentage Error\\n\\nAbsolute percentage error values over time or by index, if no datetime is provided.\\n\\n<img height=\"1074\" width=\"2218\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-b7221d5d2c9be3177055289ee1ba043539b9e10b%252Freg_perf_abs_per_error.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=9f12e500&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-6.-error-distribution)&#xA;6\\\\. Error Distribution\\n\\nDistribution of the model error values.\\n\\n<img height=\"1084\" width=\"2218\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-e9ac059e8fd627aa632cfad0e1a6096376205a83%252Freg_perf_error_distribution.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b65039f0&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-7.-error-normality)&#xA;7\\\\. Error Normality\\n\\nQuantile-quantile plot ([Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot)) to estimate value normalit',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 's%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-b7221d5d2c9be3177055289ee1ba043539b9e10b%252Freg_perf_abs_per_error.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=9f12e500&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-6.-error-distribution)&#xA;6\\\\. Error Distribution\\n\\nDistribution of the model error values.\\n\\n<img height=\"1084\" width=\"2218\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-e9ac059e8fd627aa632cfad0e1a6096376205a83%252Freg_perf_error_distribution.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b65039f0&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-7.-error-normality)&#xA;7\\\\. Error Normality\\n\\nQuantile-quantile plot ([Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot)) to estimate value normality.\\n\\n<img height=\"1066\" width=\"2214\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-acfd422bc65e1153cd554783d72e8b5f26086365%252Freg_perf_error_normality.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b88a9650&sv=2\" />\\n\\n**Next, Evidently explore in detail the two segments in the dataset: 5% of predictions with the highest negative and positive errors. We refer to them as \"underestimation\" and \"overestimation\" groups. We refer to the rest of the predictions as \"majority\".**\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-8.-mean-error-per-group)&#xA;**8. Mean Error per Group**\\n\\nA summary of the model quality metrics for each of the two segments: mean Error (ME), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE).\\n\\n<img height=\"678\" width=\"2220\" src=\"https://docs.evidentlyai.com/~gitbook/image?u',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'y.\\n\\n<img height=\"1066\" width=\"2214\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-acfd422bc65e1153cd554783d72e8b5f26086365%252Freg_perf_error_normality.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b88a9650&sv=2\" />\\n\\n**Next, Evidently explore in detail the two segments in the dataset: 5% of predictions with the highest negative and positive errors. We refer to them as \"underestimation\" and \"overestimation\" groups. We refer to the rest of the predictions as \"majority\".**\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-8.-mean-error-per-group)&#xA;**8. Mean Error per Group**\\n\\nA summary of the model quality metrics for each of the two segments: mean Error (ME), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE).\\n\\n<img height=\"678\" width=\"2220\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-2fb0b43dbd352188ebb4ef8693a07cc53fdddd17%252Freg_perf_mean_error_per_group.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cb821f65&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-9.-predicted-vs-actual-per-group)&#xA;**9. Predicted vs Actual per Group**\\n\\nPrediction plots that visualize the regions where the model underestimates and overestimates the target function.\\n\\n<img height=\"1068\" width=\"2218\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-6a3efa2e1605602e0056354603d6b198210029c9%252Freg_perf_predicted_actual_per_group.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=69e28b40&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'rl=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-2fb0b43dbd352188ebb4ef8693a07cc53fdddd17%252Freg_perf_mean_error_per_group.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cb821f65&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-9.-predicted-vs-actual-per-group)&#xA;**9. Predicted vs Actual per Group**\\n\\nPrediction plots that visualize the regions where the model underestimates and overestimates the target function.\\n\\n<img height=\"1068\" width=\"2218\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-6a3efa2e1605602e0056354603d6b198210029c9%252Freg_perf_predicted_actual_per_group.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=69e28b40&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-10.-error-bias-mean-most-common-feature-value-per-group)&#xA;**10. Error Bias: Mean/Most Common Feature Value per Group**\\n\\nThis table helps quickly see the differences in feature values between the 3 groups:\\n\\n* **OVER** (top-5% of predictions with overestimation)\\n\\n* **UNDER** (top-5% of the predictions with underestimation)\\n\\n* **MAJORITY** (the rest 90%)\\n\\nFor the numerical features, it shows the mean value per group. For the categorical features, it shows the most common value.\\n\\nIf you have two datasets, the table displays the values for both REF (reference) and CURR (current).\\n\\n<img height=\"800\" width=\"2190\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-cda35cf799db0f2431780cb4f03340da6e13d239%252Freg_perf_error_bias_table.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=fbd654ee&sv=2\" />\\n\\nIf you observe a l',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'performance#id-10.-error-bias-mean-most-common-feature-value-per-group)&#xA;**10. Error Bias: Mean/Most Common Feature Value per Group**\\n\\nThis table helps quickly see the differences in feature values between the 3 groups:\\n\\n* **OVER** (top-5% of predictions with overestimation)\\n\\n* **UNDER** (top-5% of the predictions with underestimation)\\n\\n* **MAJORITY** (the rest 90%)\\n\\nFor the numerical features, it shows the mean value per group. For the categorical features, it shows the most common value.\\n\\nIf you have two datasets, the table displays the values for both REF (reference) and CURR (current).\\n\\n<img height=\"800\" width=\"2190\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-cda35cf799db0f2431780cb4f03340da6e13d239%252Freg_perf_error_bias_table.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=fbd654ee&sv=2\" />\\n\\nIf you observe a large difference between the groups, it means that the model error is sensitive to the values of a given feature.\\n\\n**To search for cases like this, you can sort the table using the column \"Range(%)\".** It increases when either or both of the \"extreme\" groups are different from the majority.\\n\\nHere is the formula used to calculate the Range %:\\n\\nRange=100∗∣(Vover−Vunder)/(Vmax−Vmin)∣*Range*=100∗∣(*Vover*−*Vunder*)/(*Vmax*−*Vmin*)∣\\n\\n***Where:** **V**over = average feature value in the OVER group; **V**under = average feature value in the UNDER group; **V**max = maximum feature value; **V**min = minimum feature value*\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-11.-error-bias-per-feature)&#xA;**11. Error Bias per Feature**\\n\\nFor each feature, Evidently shows a histogram to visualize the **distribution of its values in the segments with extreme errors** and in the rest of the data. You can visually explore if there is a relationship between the high error and the values of',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'arge difference between the groups, it means that the model error is sensitive to the values of a given feature.\\n\\n**To search for cases like this, you can sort the table using the column \"Range(%)\".** It increases when either or both of the \"extreme\" groups are different from the majority.\\n\\nHere is the formula used to calculate the Range %:\\n\\nRange=100∗∣(Vover−Vunder)/(Vmax−Vmin)∣*Range*=100∗∣(*Vover*−*Vunder*)/(*Vmax*−*Vmin*)∣\\n\\n***Where:** **V**over = average feature value in the OVER group; **V**under = average feature value in the UNDER group; **V**max = maximum feature value; **V**min = minimum feature value*\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-11.-error-bias-per-feature)&#xA;**11. Error Bias per Feature**\\n\\nFor each feature, Evidently shows a histogram to visualize the **distribution of its values in the segments with extreme errors** and in the rest of the data. You can visually explore if there is a relationship between the high error and the values of a given feature.\\n\\nHere is an example where extreme errors are dependent on the \"temperature\" feature.\\n\\n<img height=\"1180\" width=\"2328\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-0f36fde30923f9a29d1c62512e42aae47a53ef54%252Freg_perf_error_bias_per_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=de6937aa&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-12.-predicted-vs-actual-per-feature)&#xA;12\\\\. Predicted vs Actual per Feature\\n\\nFor each feature, Evidently also show the Predicted vs Actual scatterplot. It helps visually detect and explore underperforming segments which might be sensitive to the values of the given feature.\\n\\n<img height=\"1162\" width=\"2314\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': ' a given feature.\\n\\nHere is an example where extreme errors are dependent on the \"temperature\" feature.\\n\\n<img height=\"1180\" width=\"2328\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-0f36fde30923f9a29d1c62512e42aae47a53ef54%252Freg_perf_error_bias_per_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=de6937aa&sv=2\" />\\n\\n#### [](https://docs.evidentlyai.com/presets/reg-performance#id-12.-predicted-vs-actual-per-feature)&#xA;12\\\\. Predicted vs Actual per Feature\\n\\nFor each feature, Evidently also show the Predicted vs Actual scatterplot. It helps visually detect and explore underperforming segments which might be sensitive to the values of the given feature.\\n\\n<img height=\"1162\" width=\"2314\" src=\"https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-12264ff6301d03477160f5b9f004d7af52476d88%252Freg_perf_error_bias_predicted_actual_per_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1992aeb7&sv=2\" />\\n\\n### [](https://docs.evidentlyai.com/presets/reg-performance#metrics-output)&#xA;Metrics output',\n",
       "  'title': 'Regression metrics',\n",
       "  'description': 'Open-source regression quality metrics.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'docs-main/metrics/explainer_regression.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Evaluations are a core feature of the Evidently library. It offers both a catalog of 100+ evals and a framework to easily configure yours. Before exploring, make sure know the core workflow: try an example for [LLMs](docs/quickstart_llm) or [ML](docs/quickstart_ml).\\n\\n<CardGroup cols={3}>\\n  <Card title=\"All Descriptors\" icon=\"table-list\" href=\"all_descriptors\">\\n    Text and LLM evals are here.\\n  </Card>\\n\\n  <Card title=\"All Metrics\" icon=\"chart-simple\" href=\"all_metrics\">\\n    All data and ML evals.\\n  </Card>\\n\\n  <Card title=\"All Presets\" icon=\"files\" href=\"all_presets\">\\n    Pre-built evaluation templates.\\n  </Card>\\n</CardGroup>\\n\\n## Popular links\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM judges\" icon=\"sparkles\" href=\"customize_llm_judge\">\\n    How to create a custom LLM judge.\\n  </Card>\\n\\n  <Card title=\"Data Drift\" icon=\"chart-waterfall\" href=\"customize_data_drift\">\\n    How to customize data drift detection.\\n  </Card>\\n</CardGroup>',\n",
       "  'title': 'Evaluations',\n",
       "  'description': 'Available metrics, tests and how to customize them.',\n",
       "  'mode': 'wide',\n",
       "  'filename': 'docs-main/metrics/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '**Pre-requisites**:\\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\\n* You know how to create [Reports](/docs/library/report).\\n\\n**Report.** To run a Preset on your data for a single current dataset:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset(),\\n])\\n\\nmy_eval = report.run(current, None)\\n```\\n\\n**Test Suite**. To add pass/fail classification quality Tests, auto-generated from the `ref` dataset:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset(),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n# Overview\\n\\nThe `ClassificationPreset` allows you to evaluate and visualize the performance on classification tasks, whether binary or multi-class. You can run this Report either for a single dataset or compare it against a reference dataset (such as past performance, or a different model/prompt).\\n\\n![](/images/metrics/preset_classification-min.png)\\n\\n* **Various metrics**: Accuracy, Precision, Recall, F1-score, ROC AUC, LogLoss, etc.\\n\\n* **Various visualizations**: Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, PR Curve, etc.\\n\\n![](/images/metrics/preset_classification_2-min.png)\\n\\nAdditionally, if you include feature columns, the Report will show Classification Quality by column. It displays the relationship between columns/features and the target, showing how the system performs on different data segments.\\n\\n**Test Suite**. If you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\\n\\n![](/images/metrics/test_preset_classification-min.png)\\n\\nTests are auto-generated:\\n\\n* **Based on reference dataset.** If the reference dataset is provided, conditions like expected prediction accuracy will be derived from it.\\n\\n* **Based on heuristics.** If there is no reference, Evidently will create a dummy classification model as a baseline and run checks against it.\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/doc',\n",
       "  'title': 'Classification',\n",
       "  'description': 'Overview of the Classification Quality Preset',\n",
       "  'filename': 'docs-main/metrics/preset_classification.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'tc.\\n\\n* **Various visualizations**: Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, PR Curve, etc.\\n\\n![](/images/metrics/preset_classification_2-min.png)\\n\\nAdditionally, if you include feature columns, the Report will show Classification Quality by column. It displays the relationship between columns/features and the target, showing how the system performs on different data segments.\\n\\n**Test Suite**. If you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\\n\\n![](/images/metrics/test_preset_classification-min.png)\\n\\nTests are auto-generated:\\n\\n* **Based on reference dataset.** If the reference dataset is provided, conditions like expected prediction accuracy will be derived from it.\\n\\n* **Based on heuristics.** If there is no reference, Evidently will create a dummy classification model as a baseline and run checks against it.\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Model / system comparison**. Compare predictive system performance across different datasets, such as during A/B testing, when experimenting with different prompt versions and configurations, etc.\\n\\n* **Production monitoring**. You can run evaluations whenever you get true labels in production. Use this to communicate and visualize performance, decide on model updates / retraining, etc.\\n\\n* **Debugging**. If you notice a drop in performance, use the visual Report\\n  Model Monitoring: Track the performance of a classification model over time to diagnose quality issues, explore the model errors and underperforming segments.\\n\\n## Data requirements\\n\\n* **Target and prediction columns**. Required to calculate performance.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional',\n",
       "  'title': 'Classification',\n",
       "  'description': 'Overview of the Classification Quality Preset',\n",
       "  'filename': 'docs-main/metrics/preset_classification.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 's/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Model / system comparison**. Compare predictive system performance across different datasets, such as during A/B testing, when experimenting with different prompt versions and configurations, etc.\\n\\n* **Production monitoring**. You can run evaluations whenever you get true labels in production. Use this to communicate and visualize performance, decide on model updates / retraining, etc.\\n\\n* **Debugging**. If you notice a drop in performance, use the visual Report\\n  Model Monitoring: Track the performance of a classification model over time to diagnose quality issues, explore the model errors and underperforming segments.\\n\\n## Data requirements\\n\\n* **Target and prediction columns**. Required to calculate performance.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional) **Input features.** Include if you want to explore column-target relations.\\n\\n* (Optional) **Timestamp**. If available, pass it to appear on some plots.\\n\\n<Info>\\n  **Data schema mapping.** Use the [data definition](/docs/library/data_definition) to map your data structure.\\n</Info>\\n\\n## Report Customization\\n\\nYou can customize the Report in several ways:\\n\\n* **Change Test conditions**. To modify the auto-generated conditions, you can set yours: either a different condition relative to the reference or any custom conditions.\\n\\n* **Modify Report composition**. You can add additional metrics, such as column Correlations, Missing Values, or Data Drift. It\\'s often useful to add `ValueDrift(\"target\")`to evaluate if there is a statistical distribution shift in the model target (concept drift).\\n\\n<Info>\\n  **Creating a custom Report**. Check the documentation for creating a [custom Report](/docs/library/report) and modifying [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Classification',\n",
       "  'description': 'Overview of the Classification Quality Preset',\n",
       "  'filename': 'docs-main/metrics/preset_classification.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '**Pre-requisites**:\\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\\n* You know how to create [Reports](/docs/library/report).\\n  \\n**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(),\\n])\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n**Test Suite.** To add Tests with explicit pass/fail for each column:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n## Overview\\n\\nThe`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\\n\\n![](/images/metrics/preset_data_drift_2-min.png)\\n\\n* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\\n\\n* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\\n\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\n\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_data_drift-min.png)\\n\\n<Info>\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\n</Info>\\n\\n## Use case\\n\\nYou can evaluate data drift in different scenarios.\\n\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These ar',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'Target value, it will be evaluated together with other columns.\\n\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\n\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_data_drift-min.png)\\n\\n<Info>\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\n</Info>\\n\\n## Use case\\n\\nYou can evaluate data drift in different scenarios.\\n\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\n\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\n\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\n\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\n\\n<Info>\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\n\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\n\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\n\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\n\\n<Info>\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n</Info>\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\n\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select co',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n</Info>\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\n\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\"prediction\")` to your Report so that you see the drift in this value in a separate widget.\\n\\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\\n\\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"**Pre-requisites**:\\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\\n* You know how to create [Reports](/docs/library/report).\\n  \\n**Report.** To run a Preset on your data for a single `current` dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset(),\\n])\\n\\nmy_eval = report.run(current, None)\\n```\\n\\n**Test Suite.** To add pass/fail data quality Tests, auto-generated from `ref` dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset(),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n## Overview\\n\\nThe`DataSummaryPreset` lets you visualize key descriptive statistics for the dataset and each column in it. If you pass two datasets, you'll get a side-by-side comparison.\\n\\n![](/images/metrics/preset_dataset_summary-min.png)\\n\\n* **Dataset stats.** Shows stats like number of rows/columns, empty columns/rows, etc.\\n\\n* **Column stats**. Shows relevant statistics and visualizes distribution for each column. The stats are different based on the column type (numerical, categorical, text, datetime).\\n\\n**Test suite**. If you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_dataset_summary-min.png)\\n\\nTests are auto-generated:\\n\\n* **Based on reference dataset.** If the reference dataset is provided, conditions like min-max feature ranges are derived directly from it.\\n\\n* **Based on heuristics.** If there is no reference, some Tests will run with heuristics (like expect no missing values).\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nYou can use this Preset in different scenarios.\\n\\n* **Exploratory data analysis.** Use the visual Report to explore your dataset at any point (during model training, after new batch of data arrives, during debugging etc.)\\n\\n* **Dataset comparison.** Compare any datasets to understand the differences: training and test dataset\",\n",
       "  'title': 'Data Summary',\n",
       "  'description': 'Overview of the Data Summary Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_summary.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'n the column type (numerical, categorical, text, datetime).\\n\\n**Test suite**. If you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_dataset_summary-min.png)\\n\\nTests are auto-generated:\\n\\n* **Based on reference dataset.** If the reference dataset is provided, conditions like min-max feature ranges are derived directly from it.\\n\\n* **Based on heuristics.** If there is no reference, some Tests will run with heuristics (like expect no missing values).\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nYou can use this Preset in different scenarios.\\n\\n* **Exploratory data analysis.** Use the visual Report to explore your dataset at any point (during model training, after new batch of data arrives, during debugging etc.)\\n\\n* **Dataset comparison.** Compare any datasets to understand the differences: training and test dataset, subgroups in the same dataset, current production data against training, etc..\\n\\n* **Data quality tests in production.** By enabling Tests, you can check the quality and stability of the input data before you generate the predictions, every time you perform a certain transformation, add a new data source, etc.\\n\\n* **Data profiling in production.** You can use this preset during monitoring to capture the shape of the production data for future analysis and visualization.\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional) **Set column types.** The Preset evaluates numerical, categorical, text and DateTime columns. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical, categorical and datetime columns. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data def',\n",
       "  'title': 'Data Summary',\n",
       "  'description': 'Overview of the Data Summary Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_summary.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': ', subgroups in the same dataset, current production data against training, etc..\\n\\n* **Data quality tests in production.** By enabling Tests, you can check the quality and stability of the input data before you generate the predictions, every time you perform a certain transformation, add a new data source, etc.\\n\\n* **Data profiling in production.** You can use this preset during monitoring to capture the shape of the production data for future analysis and visualization.\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional) **Set column types.** The Preset evaluates numerical, categorical, text and DateTime columns. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical, categorical and datetime columns. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select columns**. You can get stats only for some columns in the Dataset. Use the `columns` parameter.\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Correlations.** Add correlations heatmap.\\n\\n* **Missing values.** Add missing values heatmap.\\n\\n* **Data drift**. Evaluate the distribution shifts if you have two datasets.\\n\\n**Customize Test conditions**. To modify the auto-generated Test conditions, you can set yours: either a different condition relative to the reference or any custom conditions per each Test.\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Data Summary',\n",
       "  'description': 'Overview of the Data Summary Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_summary.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Warning>\\n  This Preset is **coming soon** to the new Evidently API! Check the old docs for now.\\n</Warning>\\n\\n**Report.** To run a Preset on your data for a single current dataset for top-k recommendations:\\n\\n```python\\nreport = Report([\\n    RecSysPreset(k=5),\\n])\\n\\nmy_eval = report.run(current, None)\\n```\\n\\n**Test Suite**. To add pass/fail ranking quality Tests, auto-generated from the `ref` dataset:\\n\\n```python\\nreport = Report([\\n    RecSysPreset(k=5),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n## Overview\\n\\n`RecsysPreset` evaluates the quality of the recommender system by generating multiple metrics to assess the quality of ranking and diversity of recommendations. You must provide the `k` parameter to evaluate the Top-K recommendations.\\n\\n![](/images/metrics/preset_recsys-min.png)\\n\\nIt includes 10+ metrics like NDCG at K, MAP at K, HitRate, diversity, serendipity, etc. Metric selection depends on the provided data since some of the Metrics require additional dataset (training data) or item / user features.\\n\\n<Info>\\n  **Metric explainers.** Check the [Ranking and RecSys Metrics](/metrics/explainer_recsys) to see how each Metric works.\\n</Info>\\n\\n**Test Suite**. If you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\\n\\n![](/images/metrics/test_preset_recsys-min.png)\\n\\nTests are auto-generated **based on reference dataset**. If the reference dataset is provided, conditions like expected ranking accuracy will be derived from it.\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\\n</Info>\\n\\n## Use case\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Experimental evaluations** as you iterate on building your recommender system.\\n\\n* **Side-by-side comparison** for two different models or periods.&#x20;\\n\\n* **Production monitoring** checks after you acquire ground truth labels.&#x20;\\n\\n* **Debugging**. If y',\n",
       "  'title': 'Recommendations',\n",
       "  'description': 'Overview of the Recommender Systems Preset',\n",
       "  'filename': 'docs-main/metrics/preset_recsys.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'ing data) or item / user features.\\n\\n<Info>\\n  **Metric explainers.** Check the [Ranking and RecSys Metrics](/metrics/explainer_recsys) to see how each Metric works.\\n</Info>\\n\\n**Test Suite**. If you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\\n\\n![](/images/metrics/test_preset_recsys-min.png)\\n\\nTests are auto-generated **based on reference dataset**. If the reference dataset is provided, conditions like expected ranking accuracy will be derived from it.\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\\n</Info>\\n\\n## Use case\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Experimental evaluations** as you iterate on building your recommender system.\\n\\n* **Side-by-side comparison** for two different models or periods.&#x20;\\n\\n* **Production monitoring** checks after you acquire ground truth labels.&#x20;\\n\\n* **Debugging**. If you notice a drop in performance, use the visual Report to understand changes.\\n\\n## Data requirements\\n\\n* **Prediction.** Recommended items with rank or score.\\n\\n* **Target**. True relevance score or interaction result.\\n\\n* (Optional) **Input/user features**. For some diversity metrics.\\n\\n* (Optional) **Training data**. For some diversity metrics.\\n\\n* (Optional) **Reference dataset**. To get a side-by-side comparison or auto-generate test conditions.\\n\\n<Info>\\n  **Data schema mapping.** Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report Customization\\n\\nYou can customize the Report in several ways:\\n\\n* **Change Test conditions**. To modify the auto-generated conditions, you can set yours: either a different condition relative to the reference or any custom conditions.\\n\\n* **Modify Report composition**. You can add additional metrics, such as compute Data Drift for user or item feature distributions, or to evaluate prediction drift.\\n\\n<Info>\\n  **Custom',\n",
       "  'title': 'Recommendations',\n",
       "  'description': 'Overview of the Recommender Systems Preset',\n",
       "  'filename': 'docs-main/metrics/preset_recsys.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ou notice a drop in performance, use the visual Report to understand changes.\\n\\n## Data requirements\\n\\n* **Prediction.** Recommended items with rank or score.\\n\\n* **Target**. True relevance score or interaction result.\\n\\n* (Optional) **Input/user features**. For some diversity metrics.\\n\\n* (Optional) **Training data**. For some diversity metrics.\\n\\n* (Optional) **Reference dataset**. To get a side-by-side comparison or auto-generate test conditions.\\n\\n<Info>\\n  **Data schema mapping.** Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report Customization\\n\\nYou can customize the Report in several ways:\\n\\n* **Change Test conditions**. To modify the auto-generated conditions, you can set yours: either a different condition relative to the reference or any custom conditions.\\n\\n* **Modify Report composition**. You can add additional metrics, such as compute Data Drift for user or item feature distributions, or to evaluate prediction drift.\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Recommendations',\n",
       "  'description': 'Overview of the Recommender Systems Preset',\n",
       "  'filename': 'docs-main/metrics/preset_recsys.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '**Pre-requisites**:\\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\\n* You know how to create [Reports](/docs/library/report).\\n  \\n**Report.** To run a Preset on your data for a single current dataset:\\n\\n```python\\nreport = Report([\\n    RegressionPreset(),\\n])\\n\\nmy_eval = report.run(current, None)\\n```\\n\\n**Test Suite**. To add pass/fail regression quality Tests, auto-generated from the `ref` dataset:\\n\\n```python\\nreport = Report([\\n    RegressionPreset(),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n## Overview\\n\\nThe `RegressionPreset` allows you to evaluate and visualize the performance on regression tasks. You can run this Report either for a single dataset or compare it against a reference dataset (such as past performance, or a different model/prompt).\\n\\n![](/images/metrics/preset_regression-min.png)\\n\\nThe Report includes:\\n\\n* **Various metrics**: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), etc.\\n\\n* **Various visualizations:** Actual vs Predicted Plot, Error Distribution, Error Normality, etc.\\n\\n**Test Suite**. If you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\\n\\n![](/images/metrics/test_preset_regression-min.png)\\n\\nTests are auto-generated:\\n\\n* **Based on reference dataset**. If the reference dataset is provided, conditions like expected prediction accuracy will be derived from it.\\n\\n* **Based on heuristics**. If there is no reference, Evidently will create a dummy regression model as a baseline and run checks against it.\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\\n</Info>\\n\\n## Use case\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Model / system comparison**. Compare predictive system performance across different datasets, such as during A/B testing, when experimenting with model configurations and arc',\n",
       "  'title': 'Regression',\n",
       "  'description': 'Overview of the Regression Quality Preset',\n",
       "  'filename': 'docs-main/metrics/preset_regression.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '.\\n\\n* **Various visualizations:** Actual vs Predicted Plot, Error Distribution, Error Normality, etc.\\n\\n**Test Suite**. If you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\\n\\n![](/images/metrics/test_preset_regression-min.png)\\n\\nTests are auto-generated:\\n\\n* **Based on reference dataset**. If the reference dataset is provided, conditions like expected prediction accuracy will be derived from it.\\n\\n* **Based on heuristics**. If there is no reference, Evidently will create a dummy regression model as a baseline and run checks against it.\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\\n</Info>\\n\\n## Use case\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Model / system comparison**. Compare predictive system performance across different datasets, such as during A/B testing, when experimenting with model configurations and architectures, etc.\\n\\n* **Production monitoring**. You can run evaluations whenever you get actual values in production. Use this to communicate and visualize performance, decide on model updates / retraining, etc.\\n\\n* **Debugging**. If you notice a drop in performance, use the visual Report to check error distributions and explore model errors.\\n\\n## Data requirements\\n\\n* **Target and prediction columns**. Required to calculate performance.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional) **Input features**. Include if you want to explore underperforming segments.\\n\\n* (Optional) **Timestamp**. If available, pass it to appear on some plots.\\n\\n<Info>\\n  **Data schema mapping.** Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report Customization\\n\\nYou can customize the Report in several ways:\\n\\n* **Change Test conditions**. To modify the auto-generated conditions, you can set yours: either a diffe',\n",
       "  'title': 'Regression',\n",
       "  'description': 'Overview of the Regression Quality Preset',\n",
       "  'filename': 'docs-main/metrics/preset_regression.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'hitectures, etc.\\n\\n* **Production monitoring**. You can run evaluations whenever you get actual values in production. Use this to communicate and visualize performance, decide on model updates / retraining, etc.\\n\\n* **Debugging**. If you notice a drop in performance, use the visual Report to check error distributions and explore model errors.\\n\\n## Data requirements\\n\\n* **Target and prediction columns**. Required to calculate performance.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional) **Input features**. Include if you want to explore underperforming segments.\\n\\n* (Optional) **Timestamp**. If available, pass it to appear on some plots.\\n\\n<Info>\\n  **Data schema mapping.** Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report Customization\\n\\nYou can customize the Report in several ways:\\n\\n* **Change Test conditions**. To modify the auto-generated conditions, you can set yours: either a different condition relative to the reference or any custom conditions.\\n\\n* **Modify Report composition**. You can add additional metrics, such as column Correlations, Missing Values, or Data Drift. It\\'s often useful to add `ValueDrift(\"target\")` to evaluate if there is a statistical distribution shift in the model target (concept drift).\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Regression',\n",
       "  'description': 'Overview of the Regression Quality Preset',\n",
       "  'filename': 'docs-main/metrics/preset_regression.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"To run this Report, first compute `descriptors` and add them to your Dataset. Check [how](/docs/library/descriptors).\\n\\n**Report.** To run a Preset on your data for a single `current` dataset:\\n\\n```python\\nreport = Report(metrics=[\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(current, None)\\n```\\n\\n**Test Suite.** To add pass/fail data quality Tests, auto-generated from `ref` dataset:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```\\n\\n## Overview\\n\\nThe `TextEvals` is a utility Preset that lets you immediately summarize the results of all **descriptors** (output-level text evaluations) that you computed on your dataset.\\n\\nIt lets you visually explore distributions and capture all relevant statistics at once: they will vary based on descriptor type. If you pass two datasets, you'll get a side-by-side comparison.\\n\\n<Info>\\n  **How text and LLM evaluations work.** Read about [Descriptors](/docs/library/descriptors), or try a [Quickstart](/quickstart_llm).\\n</Info>\\n\\n![](/images/metrics/preset_text_evals-min.gif)\\n\\n**Test Suite**. If you choose to enable Tests, you will get an additional Test Suite view.\\n\\n* **Based on reference dataset.** If the reference dataset is provided, conditions like expected descriptor values are derived directly from it.\\n\\n* **Based on heuristics.** If there is no reference, some data quality Tests will run with heuristics (like expect no missing values).\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nYou can use this Preset in different scenarios.\\n\\n* **LLM experiments.** Get a visual Report to explore your evaluation results as you experiment on prompts, model version, etc. and compare different runs between them.\\n\\n* **LLM observability.** Run evaluations on your production data and capture the resulting statistics to track them over time.\\n\\n## Data requirements\\n\\n* **Input datase\",\n",
       "  'title': 'Text Evals',\n",
       "  'description': 'Overview of the Text Evals Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_text_evals.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'start_llm).\\n</Info>\\n\\n![](/images/metrics/preset_text_evals-min.gif)\\n\\n**Test Suite**. If you choose to enable Tests, you will get an additional Test Suite view.\\n\\n* **Based on reference dataset.** If the reference dataset is provided, conditions like expected descriptor values are derived directly from it.\\n\\n* **Based on heuristics.** If there is no reference, some data quality Tests will run with heuristics (like expect no missing values).\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nYou can use this Preset in different scenarios.\\n\\n* **LLM experiments.** Get a visual Report to explore your evaluation results as you experiment on prompts, model version, etc. and compare different runs between them.\\n\\n* **LLM observability.** Run evaluations on your production data and capture the resulting statistics to track them over time.\\n\\n## Data requirements\\n\\n* **Input dataset with descriptors**. Dataset with computed descriptors (check [how](/docs/library/descriptors)).\\n\\n* **One or two datasets**. Pass a single dataset or two for comparison or to auto-generate test conditions.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select descriptors**. Get stats only for some descriptors in the Dataset. Use the `columns` parameter.\\n\\n**Customize or set Test conditions**. Add your own Test conditions, for example, to get a fail if texts are out of the specified Length Range. Check a [Quickstart example](/quickstart_llm).\\n\\n**Modify Report composition**. Add other Metrics to the Report to get a more comprehensive evaluation. For example:\\n\\n* **Correlations.** Add correlations heatmap to see if some descriptor values are connected to others (for example, if certain metrics are always aligned, you may not need them both). You',\n",
       "  'title': 'Text Evals',\n",
       "  'description': 'Overview of the Text Evals Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_text_evals.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 't with descriptors**. Dataset with computed descriptors (check [how](/docs/library/descriptors)).\\n\\n* **One or two datasets**. Pass a single dataset or two for comparison or to auto-generate test conditions.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select descriptors**. Get stats only for some descriptors in the Dataset. Use the `columns` parameter.\\n\\n**Customize or set Test conditions**. Add your own Test conditions, for example, to get a fail if texts are out of the specified Length Range. Check a [Quickstart example](/quickstart_llm).\\n\\n**Modify Report composition**. Add other Metrics to the Report to get a more comprehensive evaluation. For example:\\n\\n* **Correlations.** Add correlations heatmap to see if some descriptor values are connected to others (for example, if certain metrics are always aligned, you may not need them both). You can also notice patterns like whether descriptor values are connected with any metadata present in the Dataset, like the model type used.\\n\\n* **Data drift**. Compute data drift to compare descriptor distributions between two datasets.\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Text Evals',\n",
       "  'description': 'Overview of the Text Evals Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_text_evals.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nEvidently helps you evaluate LLM outputs automatically. The lets you compare prompts, models, run regression or adversarial tests with clear, repeatable checks. That means faster iterations, more confident decisions, and fewer surprises in production.\\n\\nIn this Quickstart, you\\'ll try a simple eval in Python and view the results in Evidently Cloud. If you want to stay fully local, you can also do that - just skip a couple steps.\\n\\nThere are a few extras, like custom LLM judges or tests, if you want to go further.\\n\\nLet’s dive in.\\n\\n<Info>\\n  Need help at any point? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).\\n</Info>\\n\\n## 1. Set up your environment\\n\\nFor a fully local flow, skip steps 1.1 and 1.3.\\n\\n### 1.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nComponents to run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"W',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  -',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  - Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definiti',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'on=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in the notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for quick experiments. To run comparisons, keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n**Upload the Report to Evidently Cloud** together with scored data:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n**Explore.** Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports. You will see all score summaries and can browse the data. E.g. sort to find all answers labeled as \"Denials\".\\n\\n![](/images/examples/llm_quickstart_explore.png)\\n\\n## 5. Get a Dashboard \\n\\nAs you run more evals, it\\'s useful to track them over time. Go to \"Dashboard\" in the left menu, enter the \"Edit\" mode, and add a new \"Columns\" tab:\\n\\n![](/images/examples/llm_quickstart_create_tab_new.gif)\\n\\nYou\\'ll see a s',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'he notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for quick experiments. To run comparisons, keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n**Upload the Report to Evidently Cloud** together with scored data:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n**Explore.** Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports. You will see all score summaries and can browse the data. E.g. sort to find all answers labeled as \"Denials\".\\n\\n![](/images/examples/llm_quickstart_explore.png)\\n\\n## 5. Get a Dashboard \\n\\nAs you run more evals, it\\'s useful to track them over time. Go to \"Dashboard\" in the left menu, enter the \"Edit\" mode, and add a new \"Columns\" tab:\\n\\n![](/images/examples/llm_quickstart_create_tab_new.gif)\\n\\nYou\\'ll see a set of panels with descriptor values. Each will have a single data point for now. As you log more evaluation results, you can track trends and set up alerts.\\n\\nWant to see more complex workflows? You can add pass/fail conditions and custom evals.\\n\\n## 6. (Optional) Add tests\\n\\nYou can add conditions to your evaluations. For example, you may expect that:\\n\\n- **Sentiment** is non-negative (greater or equal to 0)\\n- **Text length** is at most 150 symbols (less or equal to 150).\\n- **Denials**: there are none.\\n- If any condition is false, consider the output to be a \"fail\".\\n\\nYou can implement this logic easily.\\n\\n<Accordion title=\"Add test conditions\" description=\"How to add test conditions\" icon=\"ballot-check\">\\n  ```python\\n  # Run the evaluation with tests \\n  eval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\",\\n                  tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", ',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'et of panels with descriptor values. Each will have a single data point for now. As you log more evaluation results, you can track trends and set up alerts.\\n\\nWant to see more complex workflows? You can add pass/fail conditions and custom evals.\\n\\n## 6. (Optional) Add tests\\n\\nYou can add conditions to your evaluations. For example, you may expect that:\\n\\n- **Sentiment** is non-negative (greater or equal to 0)\\n- **Text length** is at most 150 symbols (less or equal to 150).\\n- **Denials**: there are none.\\n- If any condition is false, consider the output to be a \"fail\".\\n\\nYou can implement this logic easily.\\n\\n<Accordion title=\"Add test conditions\" description=\"How to add test conditions\" icon=\"ballot-check\">\\n  ```python\\n  # Run the evaluation with tests \\n  eval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\",\\n                  tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", alias=\"Length\",\\n                   tests=[lte(150, alias=\"Has_expected_length\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\",\\n                       tests=[eq(\"OK\", column=\"Denials\",\\n                                 alias=\"Is_not_a_refusal\")]),\\n        TestSummary(success_all=True, alias=\"All_tests_passed\")])\\n  \\n  # Uncomment to preview the results locally\\n  # eval_dataset.as_dataframe()\\n  ```\\n\\n  ![](/images/examples/llm_quickstart_descriptor_tests-min.png)\\n\\n  You can limit the summary report to include only specific descriptor(s).\\n\\n  ```python\\n  report = Report([\\n      TextEvals(columns=[\"All_tests_passed\"])\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  #my_eval\\n  ```\\n\\n  To identify rows that failed any criteria, sort by \"All_test_passed\" column:\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_tests_report-min.png)\\n\\n## 7. (Optional) Add a custom LLM jugde\\n\\nYou can implement custom criteria using built-i',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'alias=\"Length\",\\n                   tests=[lte(150, alias=\"Has_expected_length\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\",\\n                       tests=[eq(\"OK\", column=\"Denials\",\\n                                 alias=\"Is_not_a_refusal\")]),\\n        TestSummary(success_all=True, alias=\"All_tests_passed\")])\\n  \\n  # Uncomment to preview the results locally\\n  # eval_dataset.as_dataframe()\\n  ```\\n\\n  ![](/images/examples/llm_quickstart_descriptor_tests-min.png)\\n\\n  You can limit the summary report to include only specific descriptor(s).\\n\\n  ```python\\n  report = Report([\\n      TextEvals(columns=[\"All_tests_passed\"])\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  #my_eval\\n  ```\\n\\n  To identify rows that failed any criteria, sort by \"All_test_passed\" column:\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_tests_report-min.png)\\n\\n## 7. (Optional) Add a custom LLM jugde\\n\\nYou can implement custom criteria using built-in LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Ru',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'n LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\n<Info>\\n  Need help? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).\\n</Info>\\n\\nEvidently helps you run tests and evaluations for your production ML systems. This includes:\\n- evaluating prediction quality (e.g. classification or regression accuracy)\\n- input data quality (e.g. missing values, out-of-range features)\\n- data and prediction drift.\\n\\nEvaluating distribution shifts ([data drift](https://www.evidentlyai.com/ml-in-production/data-drift)) in ML inputs and predictions is a typical use case that helps you detect shifts in the model quality and environment even without ground truth labels. \\n\\nIn this Quickstart, you\\'ll run a simple data drift report in Python and view the results in Evidently Cloud. If you want to stay fully local, you can also do that - just skip a couple steps.\\n\\n## 1. Set up your environment\\n\\nFor a fully local flow, skip steps 1.1 and 1.3.\\n\\n### 1.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nComponents to run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn import datasets\\n    \\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import DataDriftPreset, DataSummaryPreset \\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n\\n## 2. Prepare a toy dataset\\n\\nLet\\'s import a toy dataset with tabular data:\\n\\n```python\\nadult_data = datasets.fetch_openml(name=\"adult\", version=2, as_frame=\"auto\")\\nadult = adult_data.frame\\n```\\n\\n<Accordion title=\"Have trouble downloading the data?\" defaultOpen={false}>\\n  If OpenML is not available, you can download the same dataset from here:\\n\\n  ```python\\n  url = \"https://github.com/evidentlyai/evidently/blob/main/test_data/a',\n",
       "  'title': 'Data and ML checks',\n",
       "  'description': 'Run a simple evaluation for tabular data',\n",
       "  'filename': 'docs-main/quickstart_ml.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nComponents to run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn import datasets\\n    \\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import DataDriftPreset, DataSummaryPreset \\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n\\n## 2. Prepare a toy dataset\\n\\nLet\\'s import a toy dataset with tabular data:\\n\\n```python\\nadult_data = datasets.fetch_openml(name=\"adult\", version=2, as_frame=\"auto\")\\nadult = adult_data.frame\\n```\\n\\n<Accordion title=\"Have trouble downloading the data?\" defaultOpen={false}>\\n  If OpenML is not available, you can download the same dataset from here:\\n\\n  ```python\\n  url = \"https://github.com/evidentlyai/evidently/blob/main/test_data/adults.parquet?raw=true\"\\n  adult = pd.read_parquet(url, engine=\\'pyarrow\\')\\n  ```\\n</Accordion>\\n\\nLet\\'s split the data into two and introduce some artificial drift for demo purposes. `Prod` data will include people with education levels unseen in the reference dataset:\\n\\n```python\\nadult_ref = adult[~adult.education.isin([\"Some-college\", \"HS-grad\", \"Bachelors\"])]\\nadult_prod = adult[adult.education.isin([\"Some-college\", \"HS-grad\", \"Bachelors\"])]\\n```\\n\\nMap the column types:\\n\\n```python\\nschema = DataDefinition(\\n    numerical_columns=[\"education-num\", \"age\", \"capital-gain\", \"hours-per-week\", \"capital-loss\", \"fnlwgt\"],\\n    categorical_columns=[\"education\", \"occupation\", \"native-country\", \"workclass\", \"marital-status\", \"relationship\", \"race\", \"sex\", \"class\"],\\n    )\\n```\\n\\nCreate Evidently Datasets to work with:\\n\\n```python\\neval_data_1 = Dataset.from_pandas(\\n    pd.DataFrame(adult_prod),\\n    data_definition=schema\\n)\\n```\\n\\n```python\\neval_data_2 = Dataset.from_pandas(\\n    pd.DataFrame(adult_ref),\\n    data_d',\n",
       "  'title': 'Data and ML checks',\n",
       "  'description': 'Run a simple evaluation for tabular data',\n",
       "  'filename': 'docs-main/quickstart_ml.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'dults.parquet?raw=true\"\\n  adult = pd.read_parquet(url, engine=\\'pyarrow\\')\\n  ```\\n</Accordion>\\n\\nLet\\'s split the data into two and introduce some artificial drift for demo purposes. `Prod` data will include people with education levels unseen in the reference dataset:\\n\\n```python\\nadult_ref = adult[~adult.education.isin([\"Some-college\", \"HS-grad\", \"Bachelors\"])]\\nadult_prod = adult[adult.education.isin([\"Some-college\", \"HS-grad\", \"Bachelors\"])]\\n```\\n\\nMap the column types:\\n\\n```python\\nschema = DataDefinition(\\n    numerical_columns=[\"education-num\", \"age\", \"capital-gain\", \"hours-per-week\", \"capital-loss\", \"fnlwgt\"],\\n    categorical_columns=[\"education\", \"occupation\", \"native-country\", \"workclass\", \"marital-status\", \"relationship\", \"race\", \"sex\", \"class\"],\\n    )\\n```\\n\\nCreate Evidently Datasets to work with:\\n\\n```python\\neval_data_1 = Dataset.from_pandas(\\n    pd.DataFrame(adult_prod),\\n    data_definition=schema\\n)\\n```\\n\\n```python\\neval_data_2 = Dataset.from_pandas(\\n    pd.DataFrame(adult_ref),\\n    data_definition=schema\\n)\\n```\\n\\n`Eval_data_2` will be our reference dataset we\\'ll compare against. \\n\\n## 3. Get a Report\\n\\nLet\\'s generate a Data Drift preset that will check for statistical distribution changes between all columns in the dataset. \\n\\n```python\\nreport = Report([\\n    DataDriftPreset() \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\n```\\n\\n<Info>\\nYou can [customize drift parameters](/metrics/customize_data_drift) by choosing different methods and thresholds. In our case we proceed as is so [default tests](/metrics/explainer_drift) selected by Evidently will apply. \\n</Info>\\n\\n## 4. Explore the results\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in the notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for one-off evaluations. To run cont',\n",
       "  'title': 'Data and ML checks',\n",
       "  'description': 'Run a simple evaluation for tabular data',\n",
       "  'filename': 'docs-main/quickstart_ml.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'efinition=schema\\n)\\n```\\n\\n`Eval_data_2` will be our reference dataset we\\'ll compare against. \\n\\n## 3. Get a Report\\n\\nLet\\'s generate a Data Drift preset that will check for statistical distribution changes between all columns in the dataset. \\n\\n```python\\nreport = Report([\\n    DataDriftPreset() \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\n```\\n\\n<Info>\\nYou can [customize drift parameters](/metrics/customize_data_drift) by choosing different methods and thresholds. In our case we proceed as is so [default tests](/metrics/explainer_drift) selected by Evidently will apply. \\n</Info>\\n\\n## 4. Explore the results\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in the notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for one-off evaluations. To run continuous monitoring (e.g. track the share of drifting features over time), keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n\\n**Upload the Report** with summary results:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n**View the Report**. Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, navigate to \"Reports\" in the left and open the Report. You will see the summary with scores and Test results.\\n\\n![](/images/examples/data_drift_quickstart.png)\\n\\n\\n## 5. Get a Dashboard (Optional)\\n\\nAs you run repeated evals, you may want to track the results in time by creating a Dashboard. Evidently lets you configure the dashboard in the UI or using dashboards-as-code. \\n\\n```python\\nfrom evidently.sdk.models import PanelMetric\\nfrom evidently.sdk.panels import DashboardPanelPlot\\n\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Dataset column drift\",\\n                subtitle = \"Shar',\n",
       "  'title': 'Data and ML checks',\n",
       "  'description': 'Run a simple evaluation for tabular data',\n",
       "  'filename': 'docs-main/quickstart_ml.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'inuous monitoring (e.g. track the share of drifting features over time), keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n\\n**Upload the Report** with summary results:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n**View the Report**. Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, navigate to \"Reports\" in the left and open the Report. You will see the summary with scores and Test results.\\n\\n![](/images/examples/data_drift_quickstart.png)\\n\\n\\n## 5. Get a Dashboard (Optional)\\n\\nAs you run repeated evals, you may want to track the results in time by creating a Dashboard. Evidently lets you configure the dashboard in the UI or using dashboards-as-code. \\n\\n```python\\nfrom evidently.sdk.models import PanelMetric\\nfrom evidently.sdk.panels import DashboardPanelPlot\\n\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Dataset column drift\",\\n                subtitle = \"Share of drifted columns\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Share\",\\n                        metric=\"DriftedColumnsCount\",\\n                        metric_labels={\"value_type\": \"share\"} \\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"line\"},\\n            ),\\n            tab=\"Data Drift\",\\n        )\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Prediction drift\",\\n                subtitle = \"\"\"Drift in the prediction column (\"class\"), method: Jensen-Shannon distance\"\"\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Drift score\",\\n                        metric=\"ValueDrift\",\\n                        metric_labels={\"column\": \"class\"} \\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"bar\"},\\n            ),\\n            tab=\"Data Drift\",',\n",
       "  'title': 'Data and ML checks',\n",
       "  'description': 'Run a simple evaluation for tabular data',\n",
       "  'filename': 'docs-main/quickstart_ml.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'e of drifted columns\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Share\",\\n                        metric=\"DriftedColumnsCount\",\\n                        metric_labels={\"value_type\": \"share\"} \\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"line\"},\\n            ),\\n            tab=\"Data Drift\",\\n        )\\nproject.dashboard.add_panel(\\n             DashboardPanelPlot(\\n                title=\"Prediction drift\",\\n                subtitle = \"\"\"Drift in the prediction column (\"class\"), method: Jensen-Shannon distance\"\"\",\\n                size=\"half\",\\n                values=[\\n                    PanelMetric(\\n                        legend=\"Drift score\",\\n                        metric=\"ValueDrift\",\\n                        metric_labels={\"column\": \"class\"} \\n                    ),\\n                ],\\n                plot_params={\"plot_type\": \"bar\"},\\n            ),\\n            tab=\"Data Drift\",\\n        )\\n```\\n\\nThis will result in the following Dashboard you\\'ll be able to access in the Dashboard tab (left menu).\\n\\n![](/images/examples/dashboard_quickstart.png)\\n\\nFor now, you will see only one datapoint, but as you add more Reports (e.g. daily or weekly), you\\'ll be able to track the results over time.\\n\\n# What\\'s next?\\n\\n- See available Evidently Metrics: [All Metric Table](/metrics/all_metrics)\\n- Understand how you can add conditional tests to your Reports: [Tests](/docs/library/tests).\\n- Explore options for Dashboard design: [Dashboards](/docs/platform/dashboard_add_panels)\\n\\n<Info>\\nAlternatively, try `DataSummaryPreset` that will generate a summary of all columns in the dataset, and run auto-generated Tests to check for data quality and core descriptive stats.\\n\\n```\\nreport = Report([\\n    DataSummaryPreset() \\n],\\ninclude_tests=\"True\")\\nmy_eval = report.run(eval_data_1, eval_data_2)\\n```\\n</Info>',\n",
       "  'title': 'Data and ML checks',\n",
       "  'description': 'Run a simple evaluation for tabular data',\n",
       "  'filename': 'docs-main/quickstart_ml.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nThis tutorial shows how to set up tracing for an LLM app, collect its inputs and outputs, view them in Evidently Cloud, and optionally run evaluations. You will use the following tools:\\n\\n* **Tracely**: An open-source tracing library based on OpenTelemetry.\\n\\n* **Evidently**: An open-source library to run LLM evaluations and interact with Evidently Cloud.\\xa0\\n\\n* **Evidently Cloud**: A web platform to view traces and run evaluations.\\n\\n* **OpenAI**: Used to simulate an LLM application.\\n\\n<Info>\\n  Need help? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).\\n</Info>\\n\\n## 1. Installation\\n\\nInstall the necessary libraries:\\n\\n```python\\n! pip install evidently\\n! pip install tracely\\xa0\\n! pip install openai\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport os\\nimport openai\\nimport time\\nimport uuid\\nfrom tracely import init_tracing\\nfrom tracely import trace_event\\nfrom tracely import create_trace_event\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n**Optional**. To load the traced dataset back to Python and run evals.\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n```\\n\\n## 2. Set up workspace\\n\\n### 2.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 2.2. Create a Project\\n\\n<CreateProject />\\n\\n### 2.3. Get Open AI key\\n\\nSet up the OpenAI key ([Token page](https://platform.openai.com/api-keys)) as an environment variable. [See Open AI docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).\\n\\n```python\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n## 3. Configure tracing\\n\\nSet up and initialize tracing:\\n\\n```python\\nproject_id = str(project.id)\\n\\ninit_tracing(\\n address=\"https://app.evidently.cloud/\",\\n api_key=\"YOUR_API_TOKEN\",\\n proje',\n",
       "  'title': 'Tracing',\n",
       "  'description': 'How to capture LLM inputs and outputs and evaluate them.',\n",
       "  'filename': 'docs-main/quickstart_tracing.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': '_trace_event\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n**Optional**. To load the traced dataset back to Python and run evals.\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n```\\n\\n## 2. Set up workspace\\n\\n### 2.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 2.2. Create a Project\\n\\n<CreateProject />\\n\\n### 2.3. Get Open AI key\\n\\nSet up the OpenAI key ([Token page](https://platform.openai.com/api-keys)) as an environment variable. [See Open AI docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).\\n\\n```python\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n## 3. Configure tracing\\n\\nSet up and initialize tracing:\\n\\n```python\\nproject_id = str(project.id)\\n\\ninit_tracing(\\n address=\"https://app.evidently.cloud/\",\\n api_key=\"YOUR_API_TOKEN\",\\n project_id=project_id,\\n export_name=\"TRACING_DATASET\"\\n )\\n```\\n\\n* The `address` is the destination backend to store collected traces.\\n\\n* `Project_id` is the ID of the Evidently Project you just created. Go to the [Home page](https://app.evidently.cloud/), enter the Project and copy its ID from above the dashboard.\\n\\n* `Dataset_name` helps identify the resulting Tracing dataset. All data with the same ID is grouped into a single dataset.\\n\\n## 4. Trace a simple LLM app\\xa0\\n\\nLet\\'s create and trace a simple function that sends a list of questions to the LLM.\\n\\nInitialize the OpenAI client with the API key:\\n\\n```python\\nclient = openai.OpenAI(api_key=openai_api_key)\\n```\\n\\nDefine the list of questions to answer:\\n\\n```python\\nquestion_list = [\\n    \"What is Evidently Python library?\",\\n    \"What is LLM observability?\",\\n    \"How is MLOps different from LLMOps?\",\\n    \"What is an LLM prompt?\",\\n    \"Why should you care about LLM safety?\"\\n]\\n```\\n\\nInstruct the assistant to answer questions, and use the `create_trace_ev',\n",
       "  'title': 'Tracing',\n",
       "  'description': 'How to capture LLM inputs and outputs and evaluate them.',\n",
       "  'filename': 'docs-main/quickstart_tracing.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ct_id=project_id,\\n export_name=\"TRACING_DATASET\"\\n )\\n```\\n\\n* The `address` is the destination backend to store collected traces.\\n\\n* `Project_id` is the ID of the Evidently Project you just created. Go to the [Home page](https://app.evidently.cloud/), enter the Project and copy its ID from above the dashboard.\\n\\n* `Dataset_name` helps identify the resulting Tracing dataset. All data with the same ID is grouped into a single dataset.\\n\\n## 4. Trace a simple LLM app\\xa0\\n\\nLet\\'s create and trace a simple function that sends a list of questions to the LLM.\\n\\nInitialize the OpenAI client with the API key:\\n\\n```python\\nclient = openai.OpenAI(api_key=openai_api_key)\\n```\\n\\nDefine the list of questions to answer:\\n\\n```python\\nquestion_list = [\\n    \"What is Evidently Python library?\",\\n    \"What is LLM observability?\",\\n    \"How is MLOps different from LLMOps?\",\\n    \"What is an LLM prompt?\",\\n    \"Why should you care about LLM safety?\"\\n]\\n```\\n\\nInstruct the assistant to answer questions, and use the `create_trace_event` from `Tracely` to trace the execution of the function and treat each as a separate session. This loops through the list of questions, captures input arguments and outputs and sends the data to Evidently Cloud:\\xa0\\n\\n```python\\ndef qa_assistant(question):\\n    system_prompt = \"You are a helpful assistant. Please answer the following question in one sentence.\"\\n    messages = [\\n        {\"role\": \"system\", \"content\": system_prompt},\\n        {\"role\": \"user\", \"content\": question},\\n    ]\\n    return client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages).choices[0].message.content\\n\\n# Iterate over the list of questions and pass each to the assistant\\nfor question in question_list:\\n    session_id = str(uuid.uuid4())\\n    with create_trace_event(\"qa\", session_id=session_id) as event:\\n      response = qa_assistant(question=question)\\n      event.set_attribute(\"question\", question)\\n      event.set_attribute(\"response\", response)\\n      time.sleep(1)\\n```\\n\\n## 5. View traces\\n\\nGo to the Eviden',\n",
       "  'title': 'Tracing',\n",
       "  'description': 'How to capture LLM inputs and outputs and evaluate them.',\n",
       "  'filename': 'docs-main/quickstart_tracing.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': 'ent` from `Tracely` to trace the execution of the function and treat each as a separate session. This loops through the list of questions, captures input arguments and outputs and sends the data to Evidently Cloud:\\xa0\\n\\n```python\\ndef qa_assistant(question):\\n    system_prompt = \"You are a helpful assistant. Please answer the following question in one sentence.\"\\n    messages = [\\n        {\"role\": \"system\", \"content\": system_prompt},\\n        {\"role\": \"user\", \"content\": question},\\n    ]\\n    return client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages).choices[0].message.content\\n\\n# Iterate over the list of questions and pass each to the assistant\\nfor question in question_list:\\n    session_id = str(uuid.uuid4())\\n    with create_trace_event(\"qa\", session_id=session_id) as event:\\n      response = qa_assistant(question=question)\\n      event.set_attribute(\"question\", question)\\n      event.set_attribute(\"response\", response)\\n      time.sleep(1)\\n```\\n\\n## 5. View traces\\n\\nGo to the Evidently Cloud, open your Project, and navigate to the \"Traces\" in the left menu. Open the traces you just sent. It might take a few moments until OpenAI processes all the inputs.\\xa0\\n\\nYou can now view, sort, export, and work with the traced dataset. You can switch between Traces, Dataset and Dialog view (select session there).\\n\\n<Tabs>\\n  <Tab title=\"Dialog \">\\n    ![](/images/examples/tracing_tutorial_session_view.png)\\n  </Tab>\\n\\n  <Tab title=\"Dataset\">\\n    ![](/images/examples/tracing_tutorial_dataset_view.png)\\n  </Tab>\\n\\n  <Tab title=\"Traces\">\\n    ![](/images/examples/tracing_tutorial_traces_view.png)\\n  </Tab>\\n</Tabs>\\n\\n## 6. Run an evaluation (Optional)\\xa0\\n\\nYou can run evaluations on this dataset both in the Cloud and locally. For local evaluations, first load the dataset to your Python environment:\\n\\n```python\\ntraced_data = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\")\\n\\n# to create and preview as pandas dataframe\\n# df = traced_data.as_dataframe()\\n# df.head()\\n```\\n\\nYou can copy the dataset ID fro',\n",
       "  'title': 'Tracing',\n",
       "  'description': 'How to capture LLM inputs and outputs and evaluate them.',\n",
       "  'filename': 'docs-main/quickstart_tracing.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'tly Cloud, open your Project, and navigate to the \"Traces\" in the left menu. Open the traces you just sent. It might take a few moments until OpenAI processes all the inputs.\\xa0\\n\\nYou can now view, sort, export, and work with the traced dataset. You can switch between Traces, Dataset and Dialog view (select session there).\\n\\n<Tabs>\\n  <Tab title=\"Dialog \">\\n    ![](/images/examples/tracing_tutorial_session_view.png)\\n  </Tab>\\n\\n  <Tab title=\"Dataset\">\\n    ![](/images/examples/tracing_tutorial_dataset_view.png)\\n  </Tab>\\n\\n  <Tab title=\"Traces\">\\n    ![](/images/examples/tracing_tutorial_traces_view.png)\\n  </Tab>\\n</Tabs>\\n\\n## 6. Run an evaluation (Optional)\\xa0\\n\\nYou can run evaluations on this dataset both in the Cloud and locally. For local evaluations, first load the dataset to your Python environment:\\n\\n```python\\ntraced_data = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\")\\n\\n# to create and preview as pandas dataframe\\n# df = traced_data.as_dataframe()\\n# df.head()\\n```\\n\\nYou can copy the dataset ID from the main Traces page inside your Project. The Dataset is already available as Evidently Dataset. To run evaluations, choose the descriptors to add:\\n\\n```python\\ntraced_data.add_descriptors=[\\n    SentenceCount(\"qa.response\", alias=\"SentenceCount\"),\\n    TextLength(\"qa.response\", alias=\"Length\"), \\n    Sentiment(\"qa.response\", alias=\"Sentiment\"), \\n]\\n```\\n\\nSummarize the results using the Report, and upload to Evidently Cloud.\\n\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(traced_data, None)\\n\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can go to your Project and open the Report:\\n\\n![](/images/examples/tracing_tutorial_evals.png)\\n\\n# What\\'s next?\\n\\nCheck the quickstart on [LLM evaluations](/quickstart_llm) for more details: how to run other evaluation methods, including LLM as a judge, or test for specific conditions.\\xa0\\n\\nNeed help? Ask in our [Discord community](https://discord.com/invite/xZjKRaNp8b).',\n",
       "  'title': 'Tracing',\n",
       "  'description': 'How to capture LLM inputs and outputs and evaluate them.',\n",
       "  'filename': 'docs-main/quickstart_tracing.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '* **Sign up** for a free [Evidently Cloud account](https://app.evidently.cloud/signup).\\n\\n* **Create an Organization** if you log in for the first time. Get an ID of your organization. ([Link](https://app.evidently.cloud/organizations)).\\n\\n* **Get an API token**. Click the **Key** icon in the left menu. Generate and save the token. ([Link](https://app.evidently.cloud/token)).',\n",
       "  'filename': 'docs-main/snippets/cloud_signup.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Connect to Evidently Cloud using your API token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project within your Organization, or connect to an existing Project:\\n\\n```python\\nproject = ws.create_project(\"My project name\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n\\n# or project = ws.get_project(\"PROJECT_ID\")\\n```',\n",
       "  'filename': 'docs-main/snippets/create_project.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Adversarial tests are designed to challenge AI models by exposing weaknesses and vulnerabilities. These inputs may attempt to:\\n\\n* Bypass safety protections and generate harmful responses.\\n* Trick the model into revealing sensitive or unintended information.\\n* Exploit edge cases to evaluate system robustness.\\n\\nEvidently Cloud lets you automate adversarial test generation based on defined categories of risk.\\n\\n## Create an adversarial test dataset\\n\\nYou can configure your own adversarial dataset.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “Adversarial testing” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Select a test scenario\\n\\nChoose a predefined adversarial scenario:\\n\\n![](/images/synthetic/synthetic_data_adversarial.png)\\n\\nYou can choose the following categories:\\n* Harmful content (e.g., profanity, toxicity, illegal advice).\\n* Forbidden topics (e.g., financial, legal, medical queries).\\n* Brand image (eliciting negative feedback on a company or product).\\n* Competition (comparisons with competitor products).\\n* Offers and promises (attempting to get AI to make commitments).\\n* Hijacking (out-of-scope questions unrelated to the intended purpose).\\n* Prompt leakage (extracting system instructions or hidden prompts).\\n\\n### 3. Configure the dataset\\n\\nAfter selecting a scenario\\n\\n* Provide an optional dataset name and description. (This applies if you export each dataset separately).\\n* Set the number of inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_brand_image.png)\\n\\nSome categories allow customization, such as selecting specific forbidden topics (e.g., legal, financial, or medical advice).\\n\\n![](/images/synthetic/synthetic_data_forbidden.png)\\n\\nYou can configure multiple scenarios at once.\\n\\n### 4. Generate the data\\n\\nYou can choose to:\\n\\n* Combine multiple scenarios into a single dataset. If you select multiple categories',\n",
       "  'title': 'Adversarial testing',\n",
       "  'description': 'Synthetic edge cases and tricky inputs',\n",
       "  'filename': 'docs-main/synthetic-data/adversarial_data.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' topics (e.g., financial, legal, medical queries).\\n* Brand image (eliciting negative feedback on a company or product).\\n* Competition (comparisons with competitor products).\\n* Offers and promises (attempting to get AI to make commitments).\\n* Hijacking (out-of-scope questions unrelated to the intended purpose).\\n* Prompt leakage (extracting system instructions or hidden prompts).\\n\\n### 3. Configure the dataset\\n\\nAfter selecting a scenario\\n\\n* Provide an optional dataset name and description. (This applies if you export each dataset separately).\\n* Set the number of inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_brand_image.png)\\n\\nSome categories allow customization, such as selecting specific forbidden topics (e.g., legal, financial, or medical advice).\\n\\n![](/images/synthetic/synthetic_data_forbidden.png)\\n\\nYou can configure multiple scenarios at once.\\n\\n### 4. Generate the data\\n\\nYou can choose to:\\n\\n* Combine multiple scenarios into a single dataset. If you select multiple categories (e.g., Brand Image and Forbidden Topics), they will be included in the same dataset, with a separate \"scenario\" column to indicate the category of each test case.\\n\\n* Export each scenario separately. Generate individual datasets for each selected test type.\\n\\nOnce generated, you can:\\n\\n* Open and edit each dataset as needed.\\n* Download it as a CSV file.\\n* Access it via the Python API using the dataset ID.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>',\n",
       "  'title': 'Adversarial testing',\n",
       "  'description': 'Synthetic edge cases and tricky inputs',\n",
       "  'filename': 'docs-main/synthetic-data/adversarial_data.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Synthetic input generation allows you to create test questions from descriptions and examples. This helps expand test coverage and evaluate how your AI system handles different types of queries. You can use this to:\\n\\n* Generate test questions for RAG systems without predefined answers.\\n* Create adversarial inputs by describing specific edge cases.\\n* Generate questions tailored to specific user personas for more targeted testing.\\n\\n## Create synthetic inputs\\n\\nYou can generate example inputs specific to your LLM app context.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “Generate from examples” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Describe the scenario\\n\\nDefine what kind of inputs you need by providing a brief description of the task and choose how many inputs to generate. For example, if you’re building a travel assistant, you could enter:\\n\\n* Description: \"Questions a person can ask when planning a trip\"\\n* Example input: \"What can I do in Paris in a day?\"\\n\\nThis guides the system in generating relevant and diverse inputs. You can also use a more detailed prompt:\\n\\n![](/images/synthetic/synthetic_data_inputs_example_prompt.png)\\n\\n## 3. Review the results\\n\\nThe system will generate a list of input questions based on your description. You can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to generate additional variations.\\n* Drop questions that don’t fit your needs.\\n* Manually edit or rephrase questions.\\n\\n## 4. Save and use the dataset\\n\\nOnce finalized, save the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>',\n",
       "  'title': 'Create synthetic inputs',\n",
       "  'description': 'Generate input test cases.',\n",
       "  'filename': 'docs-main/synthetic-data/input_data.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Check>\\n  This feature is available in Evidently Cloud. Check [pricing](https://www.evidentlyai.com/pricing) details. [Reach out](https://www.evidentlyai.com/get-demo) if you’d like a demo.\\n</Check>\\n\\nEvidently Cloud lets you generate synthetic test inputs (and outputs) to evaluate your AI system. You can use it for:\\n\\n* **Experiments**. Create test data to see how your LLM app handles it.\\n\\n* **Regression testing**. Validate changes before deployment.\\n\\n* **Adversarial testing**. Check how your system handles tricky or unexpected inputs.\\n\\nOnce you generate the data, you can run it through your AI system and evaluate the results using the Evidently Cloud or Evidently Python library as usual.\\n\\n<CardGroup cols={3}>\\n  <Card title=\"Synthetic inputs\" icon=\"input-text\" href=\"/synthetic-data/input_data\">\\n    Generate inputs from description.\\n  </Card>\\n\\n  <Card title=\"RAG dataset\" icon=\"book-open\" href=\"/synthetic-data/rag_data\">\\n    Generate Q\\\\&A dataset from the knowledge source.\\n  </Card>\\n\\n  <Card title=\"Adversarial tests\" icon=\"shield-halved\" href=\"/synthetic-data/adversarial_data\">\\n    Generate inputs to test for vulnerabilities.\\n  </Card>\\n</CardGroup>\\n\\n## Example\\n\\nFor example, here is how you can generate test inputs.\\n\\n![](/images/synthetic/datagen_travel.gif)',\n",
       "  'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>',\n",
       "  'title': 'RAG evaluation dataset',\n",
       "  'description': 'Synthetic data for RAG.',\n",
       "  'filename': 'docs-main/synthetic-data/rag_data.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"When working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:\\n\\n* Just the inputs, or\\n* Both inputs and expected outputs (ground truth).\\n\\nYou can use this test dataset to:\\n\\n* Run **experiments** and track if changes improve or degrade system performance.\\n* Run **regression testing** to ensure updates don’t break what was already working.\\n* **Stress-test** your system with complex or adversarial inputs to check its resilience.\\n\\n![](/images/synthetic/synthetic_experiments_img.png)\\n\\nYou can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is best, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but don’t reflect your specific use case. Manually writing test cases takes time and effort.\\n\\n**Synthetic data helps here**. It’s especially useful when you are:\\n\\n* You're starting from scratch and don’t have real data.\\n* You need to scale a manually designed dataset with more variation.\\n* You want to test edge cases, adversarial inputs, or system robustness.\\n* You're evaluating complex AI systems like RAG and AI agents.\\n\\n![](/images/synthetic/synthetic_adversarial_img.png)\\n\\nSynthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\\n\\n* Quickly generate hundreds structured test cases.\\n* Fill gaps by adding missing scenarios and tricky inputs.\\n* Create controlled variations to evaluate specific weaknesses.\\n\\nIt’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\\n\\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. I\",\n",
       "  'title': 'Why synthetic data?',\n",
       "  'description': 'When do you need synthetic data in LLM evaluations.',\n",
       "  'filename': 'docs-main/synthetic-data/why_synthetic.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \" you are:\\n\\n* You're starting from scratch and don’t have real data.\\n* You need to scale a manually designed dataset with more variation.\\n* You want to test edge cases, adversarial inputs, or system robustness.\\n* You're evaluating complex AI systems like RAG and AI agents.\\n\\n![](/images/synthetic/synthetic_adversarial_img.png)\\n\\nSynthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\\n\\n* Quickly generate hundreds structured test cases.\\n* Fill gaps by adding missing scenarios and tricky inputs.\\n* Create controlled variations to evaluate specific weaknesses.\\n\\nIt’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\\n\\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios.\",\n",
       "  'title': 'Why synthetic data?',\n",
       "  'description': 'When do you need synthetic data in LLM evaluations.',\n",
       "  'filename': 'docs-main/synthetic-data/why_synthetic.mdx'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33a47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "483a7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"Yes, even if you don't register, you're still eligible to submit \"\n",
      "             'the homework.\\n'\n",
      "             '\\n'\n",
      "             'Be aware, however, that there will be deadlines for turning in '\n",
      "             \"homeworks and the final projects. So don't leave everything for \"\n",
      "             'the last minute.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md',\n",
      "  'id': '3f1424af17',\n",
      "  'question': 'Course: Can I still join the course after the start date?',\n",
      "  'sort_order': 3},\n",
      " {'content': 'Yes, we will keep all the materials available, so you can follow '\n",
      "             'the course at your own pace after it finishes.\\n'\n",
      "             '\\n'\n",
      "             'You can also continue reviewing the homeworks and prepare for '\n",
      "             'the next cohort. You can also start working on your final '\n",
      "             'capstone project.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md',\n",
      "  'id': '068529125b',\n",
      "  'question': 'Course - Can I follow the course after it finishes?',\n",
      "  'sort_order': 8},\n",
      " {'content': 'The next cohort starts January 13th, 2025. More info at '\n",
      "             '[DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n'\n",
      "             '\\n'\n",
      "             '- Register before the course starts using this '\n",
      "             '[link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n'\n",
      "             '- Join the [course Telegram channel with '\n",
      "             'announcements](https://t.me/dezoomcamp).\\n'\n",
      "             \"- Don’t forget to register in DataTalks.Club's Slack and join \"\n",
      "             'the channel.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md',\n",
      "  'id': '9e508f2212',\n",
      "  'question': 'Course: When does the course start?',\n",
      "  'sort_order': 1}]\n"
     ]
    }
   ],
   "source": [
    "pprint(results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a95b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da94d9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x76c6cf6ee0f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For DataTalksClub FAQ, it's similar, except we don't need to chunk the data.\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d69e2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"Yes, even if you don't register, you're still eligible to submit \"\n",
      "             'the homework.\\n'\n",
      "             '\\n'\n",
      "             'Be aware, however, that there will be deadlines for turning in '\n",
      "             \"homeworks and the final projects. So don't leave everything for \"\n",
      "             'the last minute.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md',\n",
      "  'id': '3f1424af17',\n",
      "  'question': 'Course: Can I still join the course after the start date?',\n",
      "  'sort_order': 3},\n",
      " {'content': 'The next cohort starts January 13th, 2025. More info at '\n",
      "             '[DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n'\n",
      "             '\\n'\n",
      "             '- Register before the course starts using this '\n",
      "             '[link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n'\n",
      "             '- Join the [course Telegram channel with '\n",
      "             'announcements](https://t.me/dezoomcamp).\\n'\n",
      "             \"- Don’t forget to register in DataTalks.Club's Slack and join \"\n",
      "             'the channel.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md',\n",
      "  'id': '9e508f2212',\n",
      "  'question': 'Course: When does the course start?',\n",
      "  'sort_order': 1},\n",
      " {'content': 'Yes, we will keep all the materials available, so you can follow '\n",
      "             'the course at your own pace after it finishes.\\n'\n",
      "             '\\n'\n",
      "             'You can also continue reviewing the homeworks and prepare for '\n",
      "             'the next cohort. You can also start working on your final '\n",
      "             'capstone project.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md',\n",
      "  'id': '068529125b',\n",
      "  'question': 'Course - Can I follow the course after it finishes?',\n",
      "  'sort_order': 8}]\n"
     ]
    }
   ],
   "source": [
    "results = faq_index.search(query)\n",
    "pprint(results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e124a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c49d5de",
   "metadata": {},
   "source": [
    "### 2. Vector search\n",
    "\n",
    "For vector search, we need to turn our documents into vectors (embeddings).\n",
    "\n",
    "- [sentence-transformers](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5db99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138340ac",
   "metadata": {},
   "source": [
    "The `multi-qa-distilbert-cos-v1` model is trained explicitly for question-answering tasks. It creates embeddings optimized for finding answers to questions.\n",
    "\n",
    "Other popular models include:\n",
    "- `all-MiniLM-L6-v2` - General-purpose, fast, and efficient\n",
    "- `all-mpnet-base-v2` - Higher quality, slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076195f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"Yes, even if you don't register, you're still eligible to submit \"\n",
      "            'the homework.\\n'\n",
      "            '\\n'\n",
      "            'Be aware, however, that there will be deadlines for turning in '\n",
      "            \"homeworks and the final projects. So don't leave everything for \"\n",
      "            'the last minute.',\n",
      " 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md',\n",
      " 'id': '3f1424af17',\n",
      " 'question': 'Course: Can I still join the course after the start date?',\n",
      " 'sort_order': 3}\n"
     ]
    }
   ],
   "source": [
    "record = de_dtc_faq[2]\n",
    "pprint(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93a1d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c6f97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06053618, -0.07152255,  0.03627984, -0.02451891,  0.05339969,\n",
       "       -0.03946254, -0.01561872,  0.02626443, -0.02312284, -0.00011206],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_doc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91354e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f5015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.5190934\n"
     ]
    }
   ],
   "source": [
    "similarity = v_query.dot(v_doc)\n",
    "print(\"Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "272c3d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3d8efb009040d690d5573f1d56aba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "# convert the list to a NumPy array for efficient similarity computations.\n",
    "faq_embeddings = np.array(faq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b1ff9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x76c5bf6b5b20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "577fdaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db25dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"Yes, even if you don't register, you're still eligible to submit \"\n",
      "             'the homework.\\n'\n",
      "             '\\n'\n",
      "             'Be aware, however, that there will be deadlines for turning in '\n",
      "             \"homeworks and the final projects. So don't leave everything for \"\n",
      "             'the last minute.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md',\n",
      "  'id': '3f1424af17',\n",
      "  'question': 'Course: Can I still join the course after the start date?',\n",
      "  'sort_order': 3},\n",
      " {'content': 'Yes, we will keep all the materials available, so you can follow '\n",
      "             'the course at your own pace after it finishes.\\n'\n",
      "             '\\n'\n",
      "             'You can also continue reviewing the homeworks and prepare for '\n",
      "             'the next cohort. You can also start working on your final '\n",
      "             'capstone project.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md',\n",
      "  'id': '068529125b',\n",
      "  'question': 'Course - Can I follow the course after it finishes?',\n",
      "  'sort_order': 8},\n",
      " {'content': 'The next cohort starts January 13th, 2025. More info at '\n",
      "             '[DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n'\n",
      "             '\\n'\n",
      "             '- Register before the course starts using this '\n",
      "             '[link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n'\n",
      "             '- Join the [course Telegram channel with '\n",
      "             'announcements](https://t.me/dezoomcamp).\\n'\n",
      "             \"- Don’t forget to register in DataTalks.Club's Slack and join \"\n",
      "             'the channel.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md',\n",
      "  'id': '9e508f2212',\n",
      "  'question': 'Course: When does the course start?',\n",
      "  'sort_order': 1}]\n"
     ]
    }
   ],
   "source": [
    "pprint(results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172a27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50950d5f",
   "metadata": {},
   "source": [
    "### 3. Hybrid search\n",
    "\n",
    "Text search is fast and efficient. Vector search captures semantic meaning and handles paraphrased questions. Combining both approaches gives us the best of both worlds. This is known as \"hybrid search.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c68d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9680575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"Yes, even if you don't register, you're still eligible to submit \"\n",
      "             'the homework.\\n'\n",
      "             '\\n'\n",
      "             'Be aware, however, that there will be deadlines for turning in '\n",
      "             \"homeworks and the final projects. So don't leave everything for \"\n",
      "             'the last minute.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md',\n",
      "  'id': '3f1424af17',\n",
      "  'question': 'Course: Can I still join the course after the start date?',\n",
      "  'sort_order': 3},\n",
      " {'content': 'The next cohort starts January 13th, 2025. More info at '\n",
      "             '[DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n'\n",
      "             '\\n'\n",
      "             '- Register before the course starts using this '\n",
      "             '[link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n'\n",
      "             '- Join the [course Telegram channel with '\n",
      "             'announcements](https://t.me/dezoomcamp).\\n'\n",
      "             \"- Don’t forget to register in DataTalks.Club's Slack and join \"\n",
      "             'the channel.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md',\n",
      "  'id': '9e508f2212',\n",
      "  'question': 'Course: When does the course start?',\n",
      "  'sort_order': 1},\n",
      " {'content': 'Yes, we will keep all the materials available, so you can follow '\n",
      "             'the course at your own pace after it finishes.\\n'\n",
      "             '\\n'\n",
      "             'You can also continue reviewing the homeworks and prepare for '\n",
      "             'the next cohort. You can also start working on your final '\n",
      "             'capstone project.',\n",
      "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md',\n",
      "  'id': '068529125b',\n",
      "  'question': 'Course - Can I follow the course after it finishes?',\n",
      "  'sort_order': 8}]\n"
     ]
    }
   ],
   "source": [
    "pprint(final_results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556bee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7d0b8e9",
   "metadata": {},
   "source": [
    "### 4. Putting this together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae9f1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa42ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "237e0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can I join the course now?\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\n",
      "Content: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\n",
      "Content: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\n",
      "Content: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md\n",
      "Content: Start by installing and setting up all the dependencies and requirements:\n",
      "\n",
      "- Google Cloud account\n",
      "- Google Cloud SDK\n",
      "- Python 3 (installed with Anaconda)\n",
      "- Terraform\n",
      "- Git\n",
      "\n",
      "Look over the prerequisites...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\n",
      "Content: Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\n",
      "\n",
      "You can also ...\n"
     ]
    }
   ],
   "source": [
    "# Using text search\n",
    "results = text_search(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "for result in results:\n",
    "    print(\"*\" * 80)\n",
    "    print(f\"Filename: {result['filename']}\")\n",
    "    print(f\"Content: {result['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bad3c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can I join the course now?\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\n",
      "Content: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\n",
      "Content: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\n",
      "Content: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\n",
      "Content: Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\n",
      "\n",
      "You can also ...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md\n",
      "Content: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submittin...\n"
     ]
    }
   ],
   "source": [
    "# Using vector search\n",
    "results = vector_search(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "for result in results:\n",
    "    print(\"*\" * 80)\n",
    "    print(f\"Filename: {result['filename']}\")\n",
    "    print(f\"Content: {result['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c61580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can I join the course now?\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\n",
      "Content: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\n",
      "Content: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\n",
      "Content: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md\n",
      "Content: Start by installing and setting up all the dependencies and requirements:\n",
      "\n",
      "- Google Cloud account\n",
      "- Google Cloud SDK\n",
      "- Python 3 (installed with Anaconda)\n",
      "- Terraform\n",
      "- Git\n",
      "\n",
      "Look over the prerequisites...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\n",
      "Content: Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\n",
      "\n",
      "You can also ...\n",
      "********************************************************************************\n",
      "Filename: faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md\n",
      "Content: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submittin...\n"
     ]
    }
   ],
   "source": [
    "# Using hybrid search\n",
    "results = hybrid_search(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "for result in results:\n",
    "    print(\"*\" * 80)\n",
    "    print(f\"Filename: {result['filename']}\")\n",
    "    print(f\"Content: {result['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b2b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
