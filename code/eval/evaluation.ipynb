{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d0e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LOGS_DIRECTORY'] = '../eval_logs'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99af05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "from logs import LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7199d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/oliver/Desktop/Agents/ai-agent-crash-course/code/eval/../eval_logs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIR.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1017eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG> for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user’s instructions (in <INSRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user’s question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: The agent invoked search tool (in <LOG>)\n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n",
    "    \n",
    "\n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='groq:llama-3.3-70b-versatile', # Use the model that can handle structured output\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7037448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n",
    "\n",
    "\n",
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "            \n",
    "            if kind == 'thinking':\n",
    "                part['content'] = 'THINKING_REDACTED'\n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-return':\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed3c8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "    \n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][-1]['content']\n",
    "    \n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b7fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'gh_agent' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record.get('source') != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1403fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b783b784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3fb1e5fd204a308bbda9058200e7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0298deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cc0d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    100.0\n",
       "instructions_avoid     100.0\n",
       "answer_relevant        100.0\n",
       "answer_clear           100.0\n",
       "answer_citations       100.0\n",
       "completeness           100.0\n",
       "tool_call_search       100.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals.mean(numeric_only=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4dc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
